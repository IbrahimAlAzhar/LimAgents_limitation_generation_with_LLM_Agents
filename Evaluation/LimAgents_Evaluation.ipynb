{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Ground Truth Coverage"
      ],
      "metadata": {
        "id": "n0wcbptK8l5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df.csv\")"
      ],
      "metadata": {
        "id": "Jmk3Jeh6-dij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: Lim_and_OR_ground_truth_final ||\n",
        "LLM Generated Limitations: master_agent_ext_analy_rev_cit"
      ],
      "metadata": {
        "id": "bSxlNYDC-mor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making lists of list 'master_agent' text\n",
        "import re\n",
        "\n",
        "# make sure the output column exists\n",
        "df['master_agent_ext_analy_rev_cit_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"master_agent_ext_analy_rev_cit\"]\n",
        "    # skip if missing or not a string\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'master_agent_ext_analy_rev_cit_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'master_agent_ext_analy_rev_cit_list'] = lim_list\n"
      ],
      "metadata": {
        "id": "i-YZE2sB8ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5_JG1h8Zg6"
      },
      "outputs": [],
      "source": [
        "# making lists of list 'ground truth' text\n",
        "\n",
        "import re\n",
        "\n",
        "# ensure the output column exists\n",
        "df['Lim_and_OR_ground_truth_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"Lim_and_OR_ground_truth_final\"]\n",
        "    # skip non-strings\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, flags=re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making combinations from 'ground truth' and llm generated text'\n",
        "\n",
        "df['combined'] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Generate combinations for each row\n",
        "for i in range(len(df)):\n",
        "    combined_list = []\n",
        "    list1 = df[\"Lim_and_OR_ground_truth_list\"][i]\n",
        "    list2 = df[\"master_agent_ext_analy_rev_cit_list\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df.at[i, 'combined'] = combined_list  # Truncate if needed"
      ],
      "metadata": {
        "id": "Q4BL139r8w3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "# from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "import json\n",
        "\n",
        "llm_results = []\n",
        "df['LLM_eval_master_agent_ext_analy_rev_cit_list'] = ''\n",
        "for idx in range(len(df)): # len(df)\n",
        "    print(\"idx is\",idx)\n",
        "    pairs = df.at[idx, 'combined']   # assume this is List[Tuple[list, list]]\n",
        "    if not isinstance(pairs, list) or not pairs:\n",
        "        llm_results.append(None)\n",
        "        continue\n",
        "\n",
        "    # build the named-pairs block in one go\n",
        "    formatted = \"\\n\".join(\n",
        "        f\"Pair {i+1}:\\n  List1: {first}\\n  List2: {second}\"\n",
        "        for i, (first, second) in enumerate(pairs)\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"For each of the following pairs, answer “Yes” if List1 contains a topic or limitation\\n\"\n",
        "        \"from List2, or List2 contains a topic or limitation from from List1; otherwise answer “No”.\\n\"\n",
        "        \"Respond *only* with a JSON object mapping each Pair name to “Yes” or “No”.\\n\\n\"\n",
        "        \"Pairs:\\n\"\n",
        "        f\"{formatted}\"\n",
        "    )\n",
        "\n",
        "    # single call per row\n",
        "    resp_text = run_critic_openai(prompt)\n",
        "    llm_results.append(resp_text)\n",
        "\n",
        "    df.at[idx, 'LLM_eval_master_agent_ext_analy_rev_cit_list'] = resp_text\n"
      ],
      "metadata": {
        "id": "vSA2C5Q58wxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# extract all 'Yes', 'No'\n",
        "pattern = r'\"Pair\\s*\\d+\"\\s*:\\s*\"(Yes|No)\"'\n",
        "\n",
        "all_matches = []\n",
        "for idx in range(len(df)):\n",
        "    raw = df.at[idx, 'LLM_eval_master_agent_ext_analy_rev_cit_list']\n",
        "    if not isinstance(raw, str):\n",
        "        all_matches.append([])\n",
        "        continue\n",
        "    matches = re.findall(pattern, raw)\n",
        "    all_matches.append(matches)\n"
      ],
      "metadata": {
        "id": "-QPgh-ML8wrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for idx, tuples in df['combined'].items():\n",
        "    if not isinstance(tuples, list):\n",
        "        continue\n",
        "    # get the matching list for this row\n",
        "    matches = all_matches[idx] if idx < len(all_matches) else []\n",
        "\n",
        "    for j, (list1, list2) in enumerate(tuples):\n",
        "        # grab the j-th match or None if out of range\n",
        "        is_match = matches[j] if j < len(matches) else None\n",
        "\n",
        "        rows.append({\n",
        "            'source_row': idx,\n",
        "            'List1':      list1,\n",
        "            'List2':      list2,\n",
        "            'is_match':   is_match\n",
        "        })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "\n",
        "result_df.rename(\n",
        "    columns={\n",
        "        'List1': 'Ground_Truth',\n",
        "        'List2': 'LLM_generated'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "coYkymR69RH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_leading_number(x):\n",
        "    \"\"\"\n",
        "    If x is a list, grab its first element; then:\n",
        "    • If it’s an int, return it.\n",
        "    • If it’s a string starting with digits (with or without a dot), return those digits.\n",
        "    Otherwise return None.\n",
        "    \"\"\"\n",
        "    # step 1: if it’s a list, pull out the first item\n",
        "    val = x[0] if isinstance(x, list) and x else x\n",
        "\n",
        "    # step 2: if it’s already an int, just return it\n",
        "    if isinstance(val, int):\n",
        "        return val\n",
        "\n",
        "    # step 3: if it’s a string, regex for leading digits\n",
        "    if isinstance(val, str):\n",
        "        # match “123.” or just “123”\n",
        "        m = re.match(r'^\\s*(\\d+)(?:\\.)?', val)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "# extract into new columns\n",
        "result_df['gt_number']        = result_df['Ground_Truth'].apply(extract_leading_number)\n",
        "result_df['llm_gen_number']   = result_df['LLM_generated'].apply(extract_leading_number)\n"
      ],
      "metadata": {
        "id": "Qzxu5do09RCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth coverage\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "match = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in result_df.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['gt_number'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            match += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['gt_number']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    match += 1\n",
        "print(match)\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = result_df['Ground_Truth'].ne(result_df['Ground_Truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "ck = result_df.groupby(unique_blocks)['gt_number'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(ck)\n"
      ],
      "metadata": {
        "id": "sTscp5vO9Q7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Quality bewtween matched pairs (NLP based metrics)"
      ],
      "metadata": {
        "id": "jCYma4Kk-9fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: ground_truth || LLM_Generated limitation: llm_generated"
      ],
      "metadata": {
        "id": "3oeLB_NN_lpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# say you want to rename 'oldA'→'newA' and 'oldB'→'newB'\n",
        "df.rename(columns={\n",
        "    'Ground_Truth': 'ground_truth',\n",
        "    'LLM_generated': 'llm_generated',\n",
        "    # 'Is_same': 'is_match',\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "iBF9CAs3Tn-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the column 'is_match' is 'no'\n",
        "df_filtered = df[df['is_match'].str.lower() != 'no']"
      ],
      "metadata": {
        "id": "3kCqlOU0Tn-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WQOUxnfuTn-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore (all)\n",
        "!pip3 -q install bert-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d89e97-82cc-4988-8a2f-0aa4714969ab",
        "id": "oH3IU2CYTn-P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTScor for whole texts"
      ],
      "metadata": {
        "id": "hF9cjY0TTn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row using one loop\n",
        "def calculate_bertscore(row):\n",
        "    # Calculate BERT Scores directly for the ground_truth and llm_generated of the row\n",
        "    _, _, F1 = scorer.score([row['ground_truth']], [row['llm_generated']])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df_filtered['bert_score'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "3c71198f0e664124afc47473567b5c7a",
            "4a2b155dd49a4665bb6c17a9f5e2c897",
            "46431161f79d4133b0e9626f15d16d1b",
            "709e4d670c8d4be1aadd41a3707db2bd",
            "216d57cf081f413ea5d73b930a5ce928",
            "9a89c927429240aba5bbc46c244f0ca9",
            "cad2d872c4334e1fb3e2f5743d01b2d0",
            "1f1072c5671e4e4c80af4697626fc2ce",
            "cfbafbdc20e645fa98ce3c756b83264a",
            "1c5cedb4617d415eb809d260a5faeb1f",
            "1c392a13dd7b46e0a62787c17e38f6f6",
            "1e315aeefc844d129827e305f35b62ea",
            "5810d6dc0d5f4c0cbc05b8333383ee4e",
            "128271b8f3594010813411e064d5664f",
            "ae3660d5c79e4bda8d65778eb6df33c1",
            "b8506a22de2c4f8387946838f89f1123",
            "33bf8a8277a94066b0ef57b346ba0d1b",
            "fe25938c285a477a9202247f53e5237a",
            "82370adfac8c46b3aee660d53989c276",
            "793de62a29d74245b0af7c2e1cfbb2af",
            "54acb0f9329d4ed5b165919d61c6890f",
            "b090beec7ce94d0485d107c2b2bc05dd",
            "7e016666a1de4333aa74fc7584a3d13c",
            "5a8dfff12b1d440a9a3d6d118f5da1be",
            "b94fbeaa201f41fe9ff5c72233bec98b",
            "09f1c2b4f9d042fd8702d6798e23440b",
            "c883587700ce4c5abdc80f11b7c534a2",
            "220aff6f3714437a8b4b75b7b4eb32da",
            "3af05e812a7249c8932df09b0b53306c",
            "fc2cb511d6a2451d9955f81cf7ee5257",
            "c4b2149e029d4241b3f0b12b88351cf3",
            "8da5dcf9d2774cb59ef13f7ed8514591",
            "8c127dbd91d64b7da16e839e386b687b",
            "2917cf2db36b4b61a87072624477fe0d",
            "62ad8f05e06847a1a2e12e6946705690",
            "6104e7963a7545da974364de4070a3e1",
            "fa9d0f0d04654132b18afbabec187fba",
            "85d2034a97bd46d7938dbc65d992004a",
            "d5586a6ca4bd4bd0a5f65aed18f3ee10",
            "49c4c1f5ee0f4a89a2ff0e21d8d8433a",
            "d091c71f3bec4e03946613cf82643417",
            "05a5fa84b2aa4d7699d6796a575cd9dc",
            "b6ff5229f29441c987deb9a9817c5062",
            "4b283526b0b54602913fc94cbb40fd6d",
            "0ca6e64c03854952a65f14dc74dafa14",
            "b9f0c45a54da452ca5f0acb93eadb483",
            "9d3a1439dacb4f7a8433437d0742ef81",
            "710b38cefec74705b660dc8e9d740717",
            "226ffb072e73472f94f5fd91f4be059b",
            "fd64cd9ef63145fbb78ed8d905badf32",
            "3df1bfcc51694873abd31ca55afa9b8c",
            "f48ea351ebfe403d87f245979441daf3",
            "5f4ac0df859d4dadb4d967a2dd625463",
            "61837e63136b4226affe1ce10e5e8d89",
            "384a6bcd98394b2fa356b6e5f121b40c",
            "81ca3b6ef8f64c37b5455e491284d849",
            "40ce7b223d0e4ba29a256cf17728b9d7",
            "b1dfc1aa16dc40f79d3f325c35400a61",
            "0bd1d66ffdfc43ef9c4c912c33901984",
            "cac14d9a288e477184e5df449fae4429",
            "1591b816fe744f1ca1097f13b8edc702",
            "5c1a00e56d3d4442a97bc1766c85c7de",
            "57087f649e9a443887468848973c3a92",
            "f1aa9f1cad4748e9898dc58905f3b3cf",
            "13d95bc774f84438b4eaf16cd6ef07dc",
            "e5287bed16cc4c21b30d91a1969829ce"
          ],
          "height": 357
        },
        "outputId": "aa3ad15c-492f-43d9-bae8-cfb62dfbccdd",
        "id": "8dNlr6IbTn-R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c71198f0e664124afc47473567b5c7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e315aeefc844d129827e305f35b62ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e016666a1de4333aa74fc7584a3d13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2917cf2db36b4b61a87072624477fe0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ca6e64c03854952a65f14dc74dafa14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81ca3b6ef8f64c37b5455e491284d849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the 'bert_score' column in df_highest_score\n",
        "average_bert_score = df_filtered['bert_score'].mean()\n",
        "\n",
        "# Display the average\n",
        "average_bert_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee158757-c545-4b03-ee05-c9a608e322b7",
        "id": "9hG55egrTn-S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.8640587552331024)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a58773-93b1-46bb-d899-d2cd0ae30095",
        "id": "_4-fkhcGTn-T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate similarity metrics for each row\n",
        "def calculate_metrics(row):\n",
        "    metrics = {}\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(row['ground_truth'], row['llm_generated'])\n",
        "    metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
        "    metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
        "    metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Cosine Similarity\n",
        "    vectorizer = CountVectorizer().fit_transform([row['ground_truth'], row['llm_generated']])\n",
        "    vectors = vectorizer.toarray()\n",
        "    metrics['cosine_similarity'] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set1 = set(row['ground_truth'].split())\n",
        "    set2 = set(row['llm_generated'].split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    metrics['jaccard_similarity'] = intersection / union if union > 0 else 0\n",
        "\n",
        "    # BLEU Score\n",
        "    metrics['bleu_score'] = sentence_bleu([row['ground_truth'].split()], row['llm_generated'].split())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Apply the function to each row in the DataFrame and store results in new columns\n",
        "metric_results = df_filtered.apply(calculate_metrics, axis=1)\n",
        "\n",
        "# Expand the dictionary into separate columns\n",
        "metric_results_df = pd.DataFrame(metric_results.tolist())\n",
        "df_filtered = pd.concat([df_filtered, metric_results_df], axis=1)\n"
      ],
      "metadata": {
        "id": "_y2fbOzaA1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of each metric\n",
        "average_metrics = {\n",
        "    'Average ROUGE-1': df_filtered['rouge1'].mean(),\n",
        "    'Average ROUGE-2': df_filtered['rouge2'].mean(),\n",
        "    'Average ROUGE-L': df_filtered['rougeL'].mean(),\n",
        "    'Average Cosine Similarity': df_filtered['cosine_similarity'].mean(),\n",
        "    'Average Jaccard Similarity': df_filtered['jaccard_similarity'].mean(),\n",
        "    'Average BLEU Score': df_filtered['bleu_score'].mean()\n",
        "}\n",
        "\n",
        "# Print the average metrics\n",
        "average_metrics\n"
      ],
      "metadata": {
        "id": "x_JC28oCA3-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16d4d33-2042-4912-dcee-fdaab3a5fb55",
        "id": "omZk27ZkTn-U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic similarity"
      ],
      "metadata": {
        "id": "qY1r5p35Tn-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Ensure all entries are strings (even if NaN)\n",
        "df_filtered['ground_truth'] = df_filtered['ground_truth'].fillna(\"\").astype(str)\n",
        "df_filtered['llm_generated'] = df_filtered['llm_generated'].fillna(\"\").astype(str)\n",
        "\n",
        "# Now apply KeyBERT safely\n",
        "# df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "# df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n",
        "\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    return [kw[0] for kw in keywords]  # Extract just the keywords\n",
        "\n",
        "# Apply KeyBERT to 'ground_truth' and 'LLM_generated' columns\n",
        "df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "id": "D_un-kmDBCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard Similarity\n",
        "def jaccard_similarity(row):\n",
        "    set1 = set(row['ground_truth_words'])\n",
        "    set2 = set(row['LLM_generated_words'])\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Apply Jaccard Similarity to each row\n",
        "df_filtered['jaccard_similarity_topic'] = df_filtered.apply(jaccard_similarity, axis=1)\n",
        "df_filtered['jaccard_similarity_topic'].mean()"
      ],
      "metadata": {
        "id": "FZ9I8AWzTn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute Cosine Similarity with empty check\n",
        "def cosine_sim(row):\n",
        "    gt = ' '.join(row['ground_truth_words'])\n",
        "    llm = ' '.join(row['LLM_generated_words'])\n",
        "\n",
        "    # If either is empty, return 0 similarity\n",
        "    if not gt.strip() or not llm.strip():\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer().fit_transform([gt, llm])\n",
        "        vectors = vectorizer.toarray()\n",
        "        return cosine_similarity(vectors)[0, 1]\n",
        "    except ValueError:\n",
        "        return 0.0  # fallback if vocabulary is still empty\n",
        "\n",
        "df_filtered['cosine_similarity_topic'] = df_filtered.apply(cosine_sim, axis=1)\n",
        "mean_sim = df_filtered['cosine_similarity_topic'].mean()\n",
        "print(mean_sim)\n"
      ],
      "metadata": {
        "id": "AC4rW-7VA-Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract text between double asterisks\n",
        "def extract_text_between_asterisks(text):\n",
        "    matches = re.findall(r'\\*\\*(.*?)\\*\\*', text)\n",
        "    return matches\n",
        "\n",
        "# Apply the function to both columns and store results in new columns\n",
        "df_filtered['ground_truth_extracted'] = df_filtered['ground_truth'].apply(extract_text_between_asterisks)\n",
        "df_filtered['llm_generated_extracted'] = df_filtered['llm_generated'].apply(extract_text_between_asterisks)\n"
      ],
      "metadata": {
        "id": "cq4Ekh7-Tn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: Faithfulness, Soundness, and Importance"
      ],
      "metadata": {
        "id": "lbNbk5Ayfhzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_prompt_template = '''\n",
        "You are an expert reviewer. Evaluate the quality of the generated limitations based on the following three criteria: Faithfulness,\n",
        "Soundness, and Importance. For each criterion, assign a score between 1 and 5 and provide a short justification.\n",
        "\n",
        "Faithfulness = The generated limitations should accurately represent the paper’s content and findings, avoiding any introduction\n",
        "of misinformation or contradictions to the original concepts, methodologies or results presented.\n",
        "– 5 points: Perfect alignment with the original content and findings, with no misinformation or contradictions. Fully reflects the\n",
        "paper’s concepts, methodologies, and results\n",
        "accurately.\n",
        "– 4 points: Mostly aligns with the original content but contains minor inaccuracies or slight\n",
        "misinterpretations. These do not significantly\n",
        "affect the overall understanding of the paper’s\n",
        "concepts or results.\n",
        "– 3 points: Generally aligns with the original\n",
        "content but includes several minor inaccuracies or contradictions. Some elements may\n",
        "not fully reflect the paper’s concepts or results,\n",
        "though the overall understanding is mostly intact.\n",
        "– 2 points: Noticeable misalignment with the\n",
        "original content, with multiple inaccuracies\n",
        "or contradictions that could mislead readers.\n",
        "Some key aspects of the paper’s concepts or\n",
        "results are misrepresented.\n",
        "– 1 point: Introduces significant misalignment\n",
        "by misrepresenting issues that do not exist in\n",
        "the paper. Creates considerable misinformation and contradictions that distort the original\n",
        "content, concepts, or results.\n",
        "\n",
        "Soundness = The generated limitations should be detailed and specific, with suggestions or critiques that are practical, logically\n",
        "coherent, and purposeful. It should clearly address relevant aspects of the paper and offer insights that can genuinely improve the\n",
        "research.\n",
        "– 5 points: Highly detailed and specific, with\n",
        "practical, logically coherent, and purposeful\n",
        "suggestions. Clearly addresses relevant aspects and offers insights that substantially improve the research.\n",
        "– 4 points: Detailed and mostly specific, with\n",
        "generally practical and logically sound suggestions. Addresses relevant aspects well but may\n",
        "lack depth or novelty in some areas.\n",
        "– 3 points: Detailed and specific but with some\n",
        "issues in practicality or logical coherence. Suggestions are somewhat relevant and offer partial improvements.\n",
        "– 2 points: Somewhat vague or lacking in specificity, with suggestions that have limited practicality or logical coherence. Addresses\n",
        "relevant aspects only partially and provides minimal improvement.\n",
        "– 1 point: Lacks detail and specificity, with impractical or incoherent suggestions. Fails to\n",
        "effectively address relevant aspects or offer\n",
        "constructive insights for improvement.\n",
        "\n",
        "Importance =  The generated limitations should\n",
        "address the most significant issues that impact the\n",
        "paper’s main findings and contributions. They\n",
        "should highlight key areas where improvements\n",
        "or further research are needed, emphasizing their\n",
        "potential to enhance the research’s relevance and\n",
        "overall impact.\n",
        "– 5 points: Addresses critical issues that substantially impact the paper’s findings and contributions. Clearly identifies major areas for\n",
        "significant improvement or further research,\n",
        "enhancing the research’s relevance and overall\n",
        "impact.\n",
        "– 4 points: Identifies meaningful issues that contribute to refining the paper’s findings and\n",
        "methodology. While the impact is notable,\n",
        "it does not reach the level of fundamentally\n",
        "shaping future research directions.\n",
        "– 3 points: Highlights important issues that offer some improvement to the current work but\n",
        "do not significantly impact future research directions. Provides useful insights for refining\n",
        "the paper but lacks broader implications for\n",
        "further study.\n",
        "– 2 points: Points out limitations with limited\n",
        "relevance to the paper’s overall findings and\n",
        "contributions. Suggestions offer marginal improvements but fail to address more substantial\n",
        "gaps in the research.\n",
        "– 1 point: Focuses on trivial issues, such as minor errors or overly detailed aspects. Does not\n",
        "address substantive issues affecting the paper’s\n",
        "findings or contributions, limiting its overall\n",
        "relevance and impact.\n",
        "\n",
        "Input:\n",
        "Input Paper: [Input Paper]\n",
        "LLM Generated Limitations: [LLM Generated Limitations]\n",
        "\n",
        "Please evaluate the **Generated Limitations** based on the **Input Paper Content** and return your response strictly in the following JSON format:\n",
        "\n",
        "Faithfulness: rating: , explanation:,\n",
        "Soundness:    rating: explanation: ,\n",
        "Importance:   rating: , explanation:\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "s3CF3JRvfhnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n"
      ],
      "metadata": {
        "id": "-uapq3btfpur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Faithfulness, Sundness, and Importance between Input text (response_string_neurips) and LLM Generated limitations (master_agent_ext_analy_rev_cit_with_rel)"
      ],
      "metadata": {
        "id": "8KIQpyWrf2AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# results = []\n",
        "\n",
        "df['ext_analy_rev_cit_with_rel_result'] = ''\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    input_text = df.at[i, 'response_string_neurips']\n",
        "    generated_limitations = df.at[i, 'master_agent_ext_analy_rev_cit_with_rel']\n",
        "\n",
        "    if pd.isna(input_text) or pd.isna(generated_limitations):\n",
        "        results.append(None)\n",
        "        continue\n",
        "\n",
        "    prompt = evaluation_prompt_template.format(\n",
        "        input_text=input_text.strip(),\n",
        "        generated_limitations=generated_limitations.strip()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = azure_run_critic(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        result = None\n",
        "\n",
        "    df.at[i, \"ext_analy_rev_cit_with_rel_result\"] = result\n"
      ],
      "metadata": {
        "id": "iKJDOxLof0Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Faithfulness, Soundness, and Importance score"
      ],
      "metadata": {
        "id": "xFsZapKngNVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize empty columns\n",
        "df['faithfulness_score'] = None\n",
        "df['soundness_score'] = None\n",
        "df['importance_score'] = None\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    val = row['ext_analy_rev_cit_with_rel_result']\n",
        "\n",
        "    if pd.isna(val):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Clean and parse JSON\n",
        "        clean_json = re.sub(r'```json|```', '', val).strip()\n",
        "        parsed = json.loads(clean_json)\n",
        "\n",
        "        # Store ratings into new columns\n",
        "        df.at[idx, 'faithfulness_score'] = parsed['Faithfulness']['rating']\n",
        "        df.at[idx, 'soundness_score'] = parsed['Soundness']['rating']\n",
        "        df.at[idx, 'importance_score'] = parsed['Importance']['rating']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {idx} failed to parse:\", e)\n",
        "        continue\n",
        "\n",
        "avg_faith = df['faithfulness_score'].mean()\n",
        "avg_sound = df['soundness_score'].mean()\n",
        "avg_imp = df['importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ],
      "metadata": {
        "id": "dH20VLEpgMXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring score with Ground truth (Lim_and_OR_ground_truth_final) and LLM Generated response (master_agent_ext_analy_rev_cit_with_rel) by providing score from previous response (input and LLM generated text)"
      ],
      "metadata": {
        "id": "MrCLz7DbgWdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize result columns\n",
        "df['adjusted_score_ext_analy_rev_cit_with_rel_json'] = None  # Optional: for storing raw JSON string\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    try:\n",
        "        input_text = df.at[i, 'Lim_and_OR_ground_truth_final']\n",
        "        generated = df.at[i, 'master_agent_ext_analy_rev_cit_with_rel']\n",
        "        faith = df.at[i, 'faithfulness_score']\n",
        "        sound = df.at[i, 'soundness_score']\n",
        "        imp = df.at[i, 'importance_score']\n",
        "\n",
        "        if pd.isna(input_text) or pd.isna(generated) or pd.isna(faith) or pd.isna(sound) or pd.isna(imp):\n",
        "            continue\n",
        "\n",
        "        prompt = evaluation_prompt_template.format(\n",
        "            input_text=input_text.strip(),\n",
        "            generated_limitations=generated.strip(),\n",
        "            faith=int(faith),\n",
        "            sound=int(sound),\n",
        "            imp=int(imp)\n",
        "        )\n",
        "\n",
        "        result = azure_run_critic(prompt)\n",
        "        df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json'] = result  # Optional: Store full JSON output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {i} failed: {e}\")\n",
        "        continue\n",
        "\n"
      ],
      "metadata": {
        "id": "H52Xwlc9gV6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Initialize new columns\n",
        "df['adjusted_faithfulness_score'] = None\n",
        "df['adjusted_soundness_score'] = None\n",
        "df['adjusted_importance_score'] = None\n",
        "\n",
        "# Define regex patterns for each score\n",
        "faith_re = re.compile(r'\"Faithfulness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "sound_re = re.compile(r'\"Soundness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "imp_re   = re.compile(r'\"Importance\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "\n",
        "# Apply regex extraction row-wise\n",
        "for i in range(len(df)):\n",
        "    row = df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json']\n",
        "    if pd.isna(row):\n",
        "        continue\n",
        "\n",
        "    # Clean text from triple backticks and newline artifacts\n",
        "    cleaned = re.sub(r\"```json|```\", \"\", row).strip()\n",
        "\n",
        "    # Extract values using regex\n",
        "    faith_match = faith_re.search(cleaned)\n",
        "    sound_match = sound_re.search(cleaned)\n",
        "    imp_match   = imp_re.search(cleaned)\n",
        "\n",
        "    if faith_match:\n",
        "        df.at[i, 'adjusted_faithfulness_score'] = int(faith_match.group(1))\n",
        "    if sound_match:\n",
        "        df.at[i, 'adjusted_soundness_score'] = int(sound_match.group(1))\n",
        "    if imp_match:\n",
        "        df.at[i, 'adjusted_importance_score'] = int(imp_match.group(1))\n"
      ],
      "metadata": {
        "id": "qb24szSBgksd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_faith = df['adjusted_faithfulness_score'].mean()\n",
        "avg_sound = df['adjusted_soundness_score'].mean()\n",
        "avg_imp = df['adjusted_importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ],
      "metadata": {
        "id": "SwEdNrxfgkir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}