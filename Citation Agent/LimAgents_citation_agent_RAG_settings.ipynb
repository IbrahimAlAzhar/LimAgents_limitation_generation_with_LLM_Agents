{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RAG Settings"
      ],
      "metadata": {
        "id": "fKMCdrf4TNZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rank_bm25\n",
        "!pip3 -q install llama-index-llms-langchain\n",
        "!pip3 -q install langchain_community\n",
        "!pip3 -q install llama_index\n",
        "!pip3 -q install sentence_transformers\n",
        "!pip3 -q install langchain\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "import time\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "XJOk6kMkT0dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API key\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# LangChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.schema import Document as LCDocument\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.document_loaders import CSVLoader, DirectoryLoader, TextLoader, DataFrameLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# LlamaIndex\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.schema import Document as LIDoc\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "# Sentence Transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Tokenizer\n",
        "import tiktoken\n",
        "\n",
        "# -------------------------------\n",
        "# Token budget helpers\n",
        "# -------------------------------\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "def truncate_for_context(query: str, passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    q_tokens = enc.encode(query, disallowed_special=())\n",
        "    budget = max_tokens - len(q_tokens)\n",
        "    kept, used = [], 0\n",
        "    for p in passages:\n",
        "        p_toks = enc.encode(p, disallowed_special=())\n",
        "        if used + len(p_toks) > budget:\n",
        "            if budget - used > 0:\n",
        "                kept.append(enc.decode(p_toks[:(budget - used)]))\n",
        "            break\n",
        "        kept.append(p)\n",
        "        used += len(p_toks)\n",
        "    return kept\n",
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "def ensure_passages_within_budget(\n",
        "    query: str,\n",
        "    passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    total = count_tokens(query + \"\\n\\n\".join(passages), model=model)\n",
        "    if total <= max_tokens:\n",
        "        return passages\n",
        "    print(f\"Truncating context ({total} tokens)…\")\n",
        "    return truncate_for_context(query, passages, max_tokens=max_tokens, model=model)\n",
        "\n",
        "# -------------------------------\n",
        "# Retriever setup\n",
        "# -------------------------------\n",
        "hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def make_retriever_for_docs(docs, k=3):\n",
        "    # FAISS\n",
        "    faiss_store = FAISS.from_documents(docs, hf_emb)\n",
        "    faiss_r = faiss_store.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # BM25\n",
        "    bm25_r = BM25Retriever.from_documents(docs)\n",
        "    bm25_r.k = k\n",
        "\n",
        "    # Ensemble retriever\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# QA pipeline\n",
        "# -------------------------------\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def generate_limitations(question, retriever):\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=chat_llm,\n",
        "        chain_type=\"stuff\",   # simplest: stuff docs into prompt\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "    return qa.run(question).strip()\n"
      ],
      "metadata": {
        "id": "VehwjfzhTRM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os, sys, logging\n",
        "\n",
        "# ─── API Key ─────────────────────────────────────────────\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# ─── Logging ─────────────────────────────────────────────\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "# ─── Model + Embeddings ──────────────────────────────────\n",
        "MODEL_NAME         = \"gpt-4o-mini\"\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "\n",
        "chat_llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# SentenceTransformer (manual embeddings if needed)\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# LangChain wrapper for embeddings\n",
        "hf_emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ─── Helper to extract text from docs ─────────────────────\n",
        "def get_doc_text(d):\n",
        "    if hasattr(d, \"text\"):\n",
        "        return d.text\n",
        "    if hasattr(d, \"get_content\"):\n",
        "        return d.get_content()\n",
        "    if hasattr(d, \"content\"):\n",
        "        return d.content\n",
        "    raise AttributeError(f\"No text attr on {type(d)}\")\n",
        "\n",
        "# ─── Input sections and reference suffixes ────────────────\n",
        "main_cols = [\n",
        "    \"df_Abstract\",\n",
        "    \"df_Introduction\",\n",
        "    \"df_Related_Work\",\n",
        "    \"df_Methodology\",\n",
        "    \"df_Dataset\",\n",
        "    \"df_Conclusion\",\n",
        "    \"df_Experiment_and_Results\"\n",
        "]\n",
        "\n",
        "ref_suffixes = [\n",
        "    \"Introduction\",\n",
        "    \"Related_Work\",\n",
        "    \"Methodology\",\n",
        "    \"Dataset\",\n",
        "    \"Conclusion\",\n",
        "    \"Experiment_and_Results\",\n",
        "    \"Limitation\",\n",
        "    \"Extra\"\n",
        "]\n",
        "\n",
        "# ─── System Prompt ───────────────────────────────────────\n",
        "system_prompt = \"\"\"You are a helpful, respectful, and honest assistant\n",
        "for generating limitations or shortcomings of a research paper.\n",
        "Generate limitations or shortcomings for the following passages\n",
        "from the scientific paper.\n",
        "\"\"\"\n",
        "\n",
        "# ─── Unified helper for LLM calls ─────────────────────────\n",
        "def run_critic(prompt: str, *, system_prompt: str | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Wraps ChatOpenAI to (optionally) send a system prompt + user prompt,\n",
        "    and returns the assistant's reply as a stripped string.\n",
        "    \"\"\"\n",
        "    messages: list[SystemMessage | HumanMessage] = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(SystemMessage(content=system_prompt))\n",
        "\n",
        "    messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "    response = chat_llm.invoke(messages)\n",
        "    return response.content.strip()\n"
      ],
      "metadata": {
        "id": "B_5h-JDRTRHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rel_Prompt = '''You are a Relevance Evaluation Agent, an expert in assessing the relevance of retrieved text chunks from a vector\n",
        "database against an input query for the task of generating limitations of scientific articles. Your task is to evaluate the relevance\n",
        "of 10 retrieved text chunks against an input query, which consists of a scientific paper (including sections: Abstract, Introduction,\n",
        "Methodology, Related Work, Experiment and Results, Limitations, and Future Work) and its rewritten version. For each chunk, assign a\n",
        "relevance score from 1 (least relevant) to 10 (most relevant) based on semantic and contextual alignment with the input query, and\n",
        "provide a brief justification for the score.\n",
        "\n",
        "Input:\n",
        "\n",
        "Input Query: [The full text of the original scientific paper and its rewritten version]\n",
        "Retrieved Text Chunks: A list of 10 text chunks, each with a unique identifier and content, formatted as:\n",
        "\n",
        "Chunk 1: [chunk_id_1]: [retrieved_text_1]\n",
        "Chunk 2: [chunk_id_2]: [retrieved_text_2]\n",
        "Chunk 3: [chunk_id_3]: [retrieved_text_3]\n",
        "Chunk 4: [chunk_id_4]: [retrieved_text_4]\n",
        "Chunk 5: [chunk_id_5]: [retrieved_text_5]\n",
        "Chunk 6: [chunk_id_6]: [retrieved_text_6]\n",
        "Chunk 7: [chunk_id_7]: [retrieved_text_7]\n",
        "Chunk 8: [chunk_id_8]: [retrieved_text_8]\n",
        "Chunk 9: [chunk_id_9]: [retrieved_text_9]\n",
        "Chunk 10: [chunk_id_10]: [retrieved_text_10]\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Evaluate Relevance: For each of the 10 retrieved text chunks, assess its relevance to the input query based on semantic and contextual\n",
        "alignment with the original and rewritten scientific paper. Consider how closely the chunk matches key concepts, arguments, or details\n",
        "in the query.\n",
        "\n",
        "Assign Relevance Score:\n",
        "High Scores (8–10): The chunk has strong semantic and contextual alignment with the input query, closely matching key concepts or details.\n",
        "Prioritize chunks containing limitations (e.g., study constraints, challenges) or methodological summaries (e.g., study design, methods),\n",
        "boosting their score by 1–2 points if they align well with the query.\n",
        "\n",
        "Medium Scores (4–7): The chunk has moderate semantic and contextual alignment, containing relevant but less central content (e.g., results,\n",
        "general context, or partial methodological details).\n",
        "\n",
        "Low Scores (1–3): The chunk has minimal or no semantic and contextual alignment, such as unrelated content, generic statements, or\n",
        "off-topic information.\n",
        "\n",
        "Prioritize Limitations and Methodology: Chunks explicitly discussing limitations (e.g., sample size, data constraints, scope issues) or\n",
        "methodological summaries (e.g., study design, experimental setup) are highly relevant. Boost their score by 1–2 points if they align\n",
        "well with the input query, compared to other relevant content.\n",
        "\n",
        "Provide Justification: For each chunk, include a brief justification explaining the assigned score, referencing the chunk’s semantic and\n",
        "contextual alignment with the input query and noting whether it contains limitations or methodological summaries.\n",
        "\n",
        "Do Not Modify Text: Evaluate each chunk as provided, without modifying or paraphrasing the retrieved text.\n",
        "\n",
        "Handle Irrelevant Chunks: If a chunk is unrelated to the input query or lacks meaningful content, assign a score of 1 with an appropriate\n",
        "justification.\n",
        "\n",
        "Workflow:\n",
        "Plan: Review the input query (original and rewritten paper) and the 10 retrieved text chunks to understand their content and context.\n",
        "\n",
        "Reasoning:\n",
        "Step 1: For each chunk, identify its main topic or content (e.g., limitations, methodology, results, background).\n",
        "Step 2: Compare the chunk’s content to the input query, assessing semantic and contextual alignment with the paper’s sections\n",
        "(e.g., Limitations, Methodology).\n",
        "Step 3: Assign a relevance score (1–10) based on alignment, prioritizing limitations and methodological summaries.\n",
        "Step 4: Write a brief justification for the score, explaining the chunk’s relevance and any priority given to limitations or methodology.\n",
        "Step 5: Verify the score and justification are accurate and consistent with the chunk’s content and the input query.\n",
        "\n",
        "Analyze: Use text analysis tools to confirm semantic alignment (e.g., keyword matching for “limitation,” “constraint,” “methodology,” “sample size”) and assess relevance to the input query.\n",
        "Reflect: Ensure scores and justifications are fair, consistent, and reflect the chunk’s alignment with the query, re-evaluating any ambiguous cases.\n",
        "Continue: Iterate until all 10 chunks are evaluated with scores and justifications.\n",
        "\n",
        "Tool Use:\n",
        "Use text analysis tools to identify limitation-related or methodology-related keywords (e.g., “limited,” “constraint,” “sample size,” “methodology”) and assess semantic similarity between chunks and the input query.\n",
        "Use semantic similarity checks to confirm alignment between the chunk and the query’s key concepts.\n",
        "\n",
        "Chain of Thoughts: Document the reasoning process internally for each chunk. For example:\n",
        "“This chunk mentions a small sample size, a limitation, and aligns closely with the query’s focus, so it receives a high score (9).”\n",
        "“This chunk discusses results without addressing limitations or methodology, so it receives a medium score (6).”\n",
        "“This chunk is generic and unrelated to the query’s specific content, so it receives a low score (1).”\n",
        "\n",
        "Output Format: The output must be in strict JSON format, containing an array of 10 objects, one for each retrieved text chunk, with the\n",
        "following structure for each object:\n",
        "\"Chunk_number\": [Chunk number, e.g., \"Chunk 1\", \"Chunk 2\", ..., \"Chunk 10\"]\n",
        "\"relevance_score\": [Integer from 1 to 10]\n",
        "\"justification\": [Brief explanation of the score, referencing the chunk’s semantic and contextual alignment with the query and any emphasis on limitations or methodological summaries]\n",
        "\n",
        "Example: Input: Input Query: [Full text of the original scientific paper and its rewritten version] Retrieved Text Chunks:\n",
        "\n",
        "Chunk 1: chunk_001: The study was limited by a small sample size, which may affect generalizability.\n",
        "Chunk 2: chunk_002: The experiment used a randomized controlled trial design to test the algorithm.\n",
        "Chunk 3: chunk_003: The experiment achieved a 20% improvement in processing speed.\n",
        "...\n",
        "Chunk 10: chunk_010: Data processing is a key challenge in modern research.\n",
        "\n",
        "Output: [ { \"Chunk_number\": \"Chunk 1\", \"relevance_score\": 9, \"justification\": \"The chunk has strong semantic and contextual alignment with\n",
        "the input query, explicitly discussing a limitation (small sample size), which is a high-priority element for limitation generation.\" },\n",
        "{ \"Chunk_number\": \"Chunk 2\", \"relevance_score\": 8, \"justification\": \"The chunk aligns well with the input query by describing the\n",
        "methodological approach, a high-priority element, though it is slightly less central than limitations-related content.\" },\n",
        "{ \"Chunk_number\": \"Chunk 3\", \"relevance_score\": 6, \"justification\": \"The chunk has moderate semantic and contextual alignment,\n",
        "discussing experimental results, but lacks focus on limitations or methodology, resulting in a mid-range score.\" },\n",
        "...\n",
        "{ \"Chunk_number\": \"Chunk 10\", \"relevance_score\": 3, \"justification\": \"The chunk provides generic background information with minimal\n",
        "semantic and contextual alignment to the input query’s specific concepts or arguments.\" } ] '''"
      ],
      "metadata": {
        "id": "yNJzO_PjUQYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### measure relevance score with each chunk with input"
      ],
      "metadata": {
        "id": "kDuCghOKUcMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "hf_emb = HuggingFaceEmbeddings()\n",
        "enc = tiktoken.encoding_for_model(MODEL_NAME)\n",
        "\n",
        "# Initialize column\n",
        "df1[\"cited_by_top_20_raw\"] = None\n",
        "df1[\"cited_by_top_20_texts\"] = None\n",
        "df1[\"cited_by_top_20_meta\"] = None\n",
        "df1[\"retrieved_text_llm_asses\"] = None\n",
        "\n",
        "# Iterate through df1\n",
        "for i, row in df1.iterrows():\n",
        "    print(\"index\",i)\n",
        "    cited_list = row.get(\"cited_by_full_text\", [])\n",
        "    all_docs = []\n",
        "\n",
        "    for j, cited_dict in enumerate(cited_list):\n",
        "        abstract = cited_dict.get(\"abstractText\", \"\")\n",
        "        sections = cited_dict.get(\"sections\", [])\n",
        "        row_num = cited_dict.get(\"row_number\", \"\")\n",
        "        file_name = cited_dict.get(\"file_name\", \"\")\n",
        "\n",
        "        if isinstance(abstract, str) and abstract.strip():\n",
        "            all_docs.append(Document(\n",
        "                page_content=\"Abstract: \" + abstract.strip(),\n",
        "                metadata={\"row_number\": row_num, \"file_name\": file_name, \"position\": j}\n",
        "            ))\n",
        "\n",
        "        for sec in sections:\n",
        "            if isinstance(sec, dict):\n",
        "                heading = sec.get(\"heading\", \"\").strip()\n",
        "                text = sec.get(\"text\", \"\").strip()\n",
        "                if text:\n",
        "                    combined = f\"{heading}: {text}\" if heading else text\n",
        "                    all_docs.append(Document(\n",
        "                        page_content=combined,\n",
        "                        metadata={\"row_number\": row_num, \"file_name\": file_name, \"position\": j}\n",
        "                    ))\n",
        "\n",
        "    if not all_docs:\n",
        "        continue\n",
        "\n",
        "    # Split into chunks\n",
        "    splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
        "    chunked_docs = splitter.split_documents(all_docs)\n",
        "    print(f\"Total number of chunks for row {i}: {len(chunked_docs)}\")\n",
        "\n",
        "    # Create retrievers\n",
        "    faiss_store = FAISS.from_documents(chunked_docs, hf_emb)\n",
        "    faiss_r = faiss_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "    bm25_r = BM25Retriever.from_documents(chunked_docs)\n",
        "    bm25_r.k = 20\n",
        "\n",
        "    # Ensemble Retriever\n",
        "    ensemble = EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "    # Use row-specific query\n",
        "    example_query = row.get(\"response_string_neurips\", \"\")\n",
        "    input_query = (\n",
        "        f\"Scientific paper:\\n{row['response_string_neurips']}\\n\\n\"\n",
        "        f\"Rewritten version of scientific paper:\\n{row['Input_Query_rewrite']}\"\n",
        "    )\n",
        "    if not input_query.strip():\n",
        "        continue\n",
        "\n",
        "    # Retrieve top 20 and store\n",
        "    top20 = ensemble.get_relevant_documents(input_query)\n",
        "    top20 = top20[:20]\n",
        "\n",
        "    # Store top20 as list of strings in the column\n",
        "    df1.at[i, \"cited_by_top_20_raw\"] = top20  # stores full Document objects (content + metadata)\n",
        "    df1.at[i, \"cited_by_top_20_texts\"] = [doc.page_content for doc in top20]  # just the texts\n",
        "    df1.at[i, \"cited_by_top_20_meta\"] = [doc.metadata for doc in top20]  # just the metadata\n",
        "\n",
        "    # ---------- 6) Batch LLM Scoring ---------------\n",
        "    all_llm_scores = []\n",
        "\n",
        "    # Rel_Prompt = \"You are a helpful assistant tasked with evaluating text relevance.\"  # or load externally\n",
        "\n",
        "    prefix = (\n",
        "        f\"{Rel_Prompt}\\n\\n\"\n",
        "        f\"Input Query:\\n{example_query}\\n\\n\"\n",
        "        \"Here are up to 10 retrieved text chunks:\\n\"\n",
        "    )\n",
        "    prefix_len = count_tokens(prefix, model=MODEL_NAME)\n",
        "\n",
        "    question = (\n",
        "        \"\\\\nOn a scale of 1–10, how relevant is each chunk to the above Input Query? \"\n",
        "        \"Respond with JSON array with Chunk Number, Score, and Justification for each chunk.\"\n",
        "    )\n",
        "    question_len = count_tokens(question, model=MODEL_NAME)\n",
        "\n",
        "    for batch_start in (0, 10):\n",
        "        batch_docs = top20[batch_start:batch_start+10]\n",
        "        if not batch_docs:\n",
        "            continue  # skip empty batches\n",
        "        batch_texts = [d.page_content for d in batch_docs]\n",
        "\n",
        "        # truncate to token budget\n",
        "        available = MAX_CONTEXT_TOKENS - prefix_len - question_len\n",
        "        kept, used = [], 0\n",
        "        for p in batch_texts:\n",
        "            toks = enc.encode(p, disallowed_special=())\n",
        "            if used + len(toks) > available:\n",
        "                break\n",
        "            kept.append(p)\n",
        "            used += len(toks)\n",
        "\n",
        "        chunks_list = \"\\n\\n\".join(\n",
        "            f\"Chunk {batch_start+idx+1}: {text}\" for idx, text in enumerate(kept)\n",
        "        )\n",
        "        prompt = prefix + chunks_list + question\n",
        "\n",
        "        # Call the LLM to assess\n",
        "        raw = run_critic(prompt)\n",
        "        all_llm_scores.append(raw)  # you can also parse JSON if needed\n",
        "\n",
        "    # Save LLM assessments\n",
        "    df1.at[i, \"retrieved_text_llm_asses\"] = all_llm_scores\n",
        "\n",
        "    # Optional debug print\n",
        "    if i == 0 or i == 20:\n",
        "        print(f\"all_llm_scores for row {i}:\", all_llm_scores)\n",
        "\n",
        "df1.to_csv(\"df.csv\",index=False)"
      ],
      "metadata": {
        "id": "1gjZrLOvUQP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Placeholder: simulate loading df\n",
        "# df = pd.read_csv(\"your_path.csv\")\n",
        "\n",
        "# Prepare a new column to store high-score chunks\n",
        "df1[\"top_chunks_texts\"] = None\n",
        "\n",
        "def extract_top_chunks(row):\n",
        "    try:\n",
        "        raw = row[\"retrieved_text_llm_asses\"]\n",
        "        if isinstance(raw, str):\n",
        "            # Convert string to list\n",
        "            raw_list = ast.literal_eval(raw)\n",
        "        elif isinstance(raw, list):\n",
        "            raw_list = raw\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        all_chunks = []\n",
        "        for entry in raw_list:\n",
        "            # Extract the JSON block\n",
        "            match = re.search(r\"\\[.*\\]\", entry, re.DOTALL)\n",
        "            if not match:\n",
        "                continue\n",
        "            try:\n",
        "                chunk_json = json.loads(match.group())\n",
        "                for item in chunk_json:\n",
        "                    score = item.get(\"relevance_score\", 0)\n",
        "                    chunk_idx = int(item.get(\"Chunk_number\", \"Chunk 0\").split()[-1]) - 1\n",
        "                    if score >= 7:\n",
        "                        # Safely get the chunk from cited_by_top_20_texts\n",
        "                        text_list = row.get(\"cited_by_top_20_texts\", [])\n",
        "                        if isinstance(text_list, list) and 0 <= chunk_idx < len(text_list):\n",
        "                            all_chunks.append(text_list[chunk_idx])\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        return all_chunks if all_chunks else None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Apply to all rows\n",
        "df1[\"top_chunks_texts\"] = df1.apply(extract_top_chunks, axis=1)\n"
      ],
      "metadata": {
        "id": "PTuHWunMUoxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Only apply ast.literal_eval to non-null strings\n",
        "df1['top_chunks_texts'] = df1['top_chunks_texts'].apply(\n",
        "    lambda x: ast.literal_eval(x) if pd.notnull(x) else np.nan\n",
        ")"
      ],
      "metadata": {
        "id": "PjmHZI7FRucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['retrieved_text_llm_asses'] = (\n",
        "    df['retrieved_text_llm_asses']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "df['top20_docs'] = (\n",
        "    df['top20_docs']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "bfSit6KsRuX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['top20_docs']\n",
        "    asses = row['retrieved_text_llm_asses']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_in'] = df.apply(pick_high_relevance, axis=1)\n"
      ],
      "metadata": {
        "id": "fCFeOmeOR6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['cited_by_top_20_texts'] = (\n",
        "    df['cited_by_top_20_texts']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "Vr51ANeAR5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['retrieved_text_llm_asses_cited_by'] = (\n",
        "    df['retrieved_text_llm_asses_cited_by']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "1M2x0GI5R55Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_chunk_dicts(cell):\n",
        "    \"\"\"\n",
        "    cell is expected to be a list of strings, each string containing a\n",
        "    ```json ... ``` block holding a JSON array of chunk‐dicts.\n",
        "    This returns a flat list of all dicts.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for s in cell or []:\n",
        "        # 1) remove the ```json fences\n",
        "        s_clean = re.sub(r'^```json\\s*', '', s.strip())\n",
        "        s_clean = re.sub(r'```$',      '', s_clean.strip())\n",
        "\n",
        "        # 2) parse the JSON\n",
        "        try:\n",
        "            data = json.loads(s_clean)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        # 3) if it’s a list of dicts, extend; otherwise skip\n",
        "        if isinstance(data, list):\n",
        "            out.extend(d for d in data if isinstance(d, dict))\n",
        "    return out\n",
        "\n",
        "# apply to your DataFrame\n",
        "df['retrieved_text_llm_asses_cited_by_upd'] = df['retrieved_text_llm_asses_cited_by'].apply(extract_chunk_dicts)\n"
      ],
      "metadata": {
        "id": "WAjb1kQhRuSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['cited_by_top_20_texts']\n",
        "    asses = row['retrieved_text_llm_asses_cited_by_upd']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_by'] = df.apply(pick_high_relevance, axis=1)\n"
      ],
      "metadata": {
        "id": "ddTfqLlBSC8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Citation_agent = '''You are an expert scientific research assistant tasked with inferring potential limitations for an unspecified\n",
        "current scientific article based solely on its cited papers.\n",
        "You are given information from multiple cited papers, which are assumed to be referenced by the current article.\n",
        "Your goal is to analyze these cited works and identify possible limitations that the current paper may have, by\n",
        "comparing its presumed scope, methods, or results against the cited literature.\n",
        "Because the input paper itself is not provided, you must reason from the cited papers alone, identifying what\n",
        "gaps, stronger methods, broader coverage, or alternative results the cited works might expose in the hypothetical\n",
        "current paper that cites them.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Generate a list of scientifically grounded limitations that the current article might have, assuming it builds upon or is informed by the provided cited papers.\n",
        "\n",
        "Each limitation should:\n",
        "\n",
        "Be concise\n",
        "\n",
        "Reference the relevant cited paper(s) by title\n",
        "\n",
        "Clearly explain how the cited paper exposes a potential limitation\n",
        "\n",
        "Be plausible and insightful based on common scientific reasoning\n",
        "\n",
        "Workflow:\n",
        "Plan:\n",
        "Identify key insights, strengths, and scopes of the cited papers that could set a high bar or reveal blind spots\n",
        "in a hypothetical citing article.\n",
        "\n",
        "Reasoning: Let's think step by step to infer limitations:\n",
        "Review each cited paper to extract its methodology, findings, and scope.\n",
        "Ask: If a paper cited this work but did not adopt or address its insights, what limitation might arise?\n",
        "Identify where the cited paper offers better methodology, broader scope, or contradicting findings.\n",
        "Formulate each limitation as a plausible shortcoming of a hypothetical article that builds on—but possibly\n",
        "underutilizes—these cited works.\n",
        "\n",
        "Justify each limitation based on specific attributes of the cited paper (e.g., \"more comprehensive dataset\",\n",
        "\"stronger evaluation metric\", etc.)\n",
        "\n",
        "Analyze:\n",
        "Develop a set of inferred limitations, each tied to specific cited paper(s) and grounded in logical comparison.\n",
        "\n",
        "Reflect:\n",
        "Ensure coverage of all relevant cited papers and validate that each limitation is scientifically plausible in\n",
        "context.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, explanation, and reference to the cited paper(s) in the format Paper Title.\n",
        "\n",
        "Tool Use (if applicable):\n",
        "\n",
        "Use citation lookup tools or document content to extract accurate summaries.\n",
        "Do not assume details about the input paper—focus only on drawing limitations based on differences, omissions,\n",
        "or underuse of the cited works.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected [Paper X] because it uses a more robust method than the current article.\"\n",
        "\"The current article's simpler method may limit accuracy compared to [Paper X].\"\n",
        "\"I reviewed all cited papers to ensure no relevant gaps were missed.\"\n",
        "This narrative ensures transparency and justifies each identified limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please identify limitations that would be relevant for researchers who might cite this paper in future work.\n",
        "Consider what limitations future authors might mention when discussing this paper's contribution to the field,\n",
        "based on the cited papers context.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "GjfSnwMTTHoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate limitations using cited in + cited by papers"
      ],
      "metadata": {
        "id": "QBrBGIJkS3Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the column names to concatenate\n",
        "cols_to_concat = [\n",
        "    \"neurips_Abstract\",\n",
        "    \"neurips_Introduction\",\n",
        "    \"neurips_Related_Work\",\n",
        "    \"neurips_Methodology\",\n",
        "    \"neurips_Dataset\",\n",
        "    \"neurips_Conclusion\",\n",
        "    \"neurips_Experiment_and_Results\",\n",
        "    \"neurips_Extra\"\n",
        "]\n",
        "\n",
        "# Create a new column 'response_string_neurips' with labeled concatenation\n",
        "def concat_with_labels(row):\n",
        "    parts = []\n",
        "    for col in cols_to_concat:\n",
        "        if isinstance(row.get(col), str) and row[col].strip():\n",
        "            label = col.replace(\"neurips_\", \"\").replace(\"_\", \" \")\n",
        "            parts.append(f\"{label}: {row[col].strip()}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "df[\"response_string\"] = df.apply(concat_with_labels, axis=1)"
      ],
      "metadata": {
        "id": "PkDsmexeUH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Tokenization setup\n",
        "encoding   = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "max_tokens = 128000\n",
        "\n",
        "def truncate_to_max_tokens(text: str, max_length: int) -> str:\n",
        "    tokens = encoding.encode(text)\n",
        "    return encoding.decode(tokens[:max_length]) if len(tokens) > max_length else text\n",
        "\n",
        "# Make sure the output column exists\n",
        "df['citation_agent_in_by_8'] = ''\n",
        "\n",
        "# Process each row\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"Processing row {i}...\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # 1) Collect all items from relevance_8_cited_in\n",
        "    cited_in_list = row.get('relevance_8_cited_in', []) or []\n",
        "    cited_in_texts = []\n",
        "    for itm in cited_in_list:\n",
        "        # if it's a dict with 'text' key, grab that, otherwise str(itm)\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_in_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_in_texts.append(str(itm))\n",
        "\n",
        "    # 2) Collect all items from relevance_8_cited_by\n",
        "    cited_by_list = row.get('relevance_8_cited_by', []) or []\n",
        "    cited_by_texts = []\n",
        "    for itm in cited_by_list:\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_by_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_by_texts.append(str(itm))\n",
        "\n",
        "    # 3) Build the combined prompt section\n",
        "    cited_in_block = \"\\n\".join(cited_in_texts)\n",
        "    cited_by_block = \"\\n\".join(cited_by_texts)\n",
        "\n",
        "    combined_cited_input = (\n",
        "        \"Referenced papers:\\n\" + cited_in_block +\n",
        "        \"\\n\\nPapers who cited this paper:\\n\" + cited_by_block\n",
        "    )\n",
        "\n",
        "    input_paper = df['response_string'][i]\n",
        "    prompt = Citation_agent + (\n",
        "        \"You are an assistant tasked to generate limitations or shortcomings \"\n",
        "        \"in a scientific article. Below is the input paper:\\n\"\n",
        "        f\"{input_paper}\\n\\n\"\n",
        "        \" Below is the relevant text from both the papers \"\n",
        "        \"that this article cites and those that cite it.\\n\\n\"\n",
        "        f\"{combined_cited_input}\\n\\n\"\n",
        "        \"Please generate limitations based on this information.\"\n",
        "    )\n",
        "\n",
        "    # 5) Truncate and call LLM\n",
        "    truncated = truncate_to_max_tokens(prompt, max_tokens)\n",
        "    try:\n",
        "        llm_summary = azure_run_critic(truncated)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        llm_summary = \"ERROR\"\n",
        "\n",
        "    df.at[i, \"citation_agent_in_by_8\"] = llm_summary\n"
      ],
      "metadata": {
        "id": "HayadLbXSYQZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}