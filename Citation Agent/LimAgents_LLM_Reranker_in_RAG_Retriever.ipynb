{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2xoin4wg4Z0"
      },
      "outputs": [],
      "source": [
        "Rel_Prompt = '''You are a Relevance Evaluation Agent, an expert in assessing the relevance of retrieved text chunks from a vector\n",
        "database against an input query for the task of generating limitations of scientific articles. Your task is to evaluate the relevance\n",
        "of 10 retrieved text chunks against an input query, which consists of a scientific paper (including sections: Abstract, Introduction,\n",
        "Methodology, Related Work, Experiment and Results, Limitations, and Future Work) and its rewritten version. For each chunk, assign a\n",
        "relevance score from 1 (least relevant) to 10 (most relevant) based on semantic and contextual alignment with the input query, and\n",
        "provide a brief justification for the score.\n",
        "\n",
        "Input:\n",
        "\n",
        "Input Query: [The full text of the original scientific paper and its rewritten version]\n",
        "Retrieved Text Chunks: A list of 10 text chunks, each with a unique identifier and content, formatted as:\n",
        "\n",
        "Chunk 1: [chunk_id_1]: [retrieved_text_1]\n",
        "Chunk 2: [chunk_id_2]: [retrieved_text_2]\n",
        "Chunk 3: [chunk_id_3]: [retrieved_text_3]\n",
        "Chunk 4: [chunk_id_4]: [retrieved_text_4]\n",
        "Chunk 5: [chunk_id_5]: [retrieved_text_5]\n",
        "Chunk 6: [chunk_id_6]: [retrieved_text_6]\n",
        "Chunk 7: [chunk_id_7]: [retrieved_text_7]\n",
        "Chunk 8: [chunk_id_8]: [retrieved_text_8]\n",
        "Chunk 9: [chunk_id_9]: [retrieved_text_9]\n",
        "Chunk 10: [chunk_id_10]: [retrieved_text_10]\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Evaluate Relevance: For each of the 10 retrieved text chunks, assess its relevance to the input query based on semantic and contextual\n",
        "alignment with the original and rewritten scientific paper. Consider how closely the chunk matches key concepts, arguments, or details\n",
        "in the query.\n",
        "\n",
        "Assign Relevance Score:\n",
        "High Scores (8–10): The chunk has strong semantic and contextual alignment with the input query, closely matching key concepts or details.\n",
        "Prioritize chunks containing limitations (e.g., study constraints, challenges) or methodological summaries (e.g., study design, methods),\n",
        "boosting their score by 1–2 points if they align well with the query.\n",
        "\n",
        "Medium Scores (4–7): The chunk has moderate semantic and contextual alignment, containing relevant but less central content (e.g., results,\n",
        "general context, or partial methodological details).\n",
        "\n",
        "Low Scores (1–3): The chunk has minimal or no semantic and contextual alignment, such as unrelated content, generic statements, or\n",
        "off-topic information.\n",
        "\n",
        "Prioritize Limitations and Methodology: Chunks explicitly discussing limitations (e.g., sample size, data constraints, scope issues) or\n",
        "methodological summaries (e.g., study design, experimental setup) are highly relevant. Boost their score by 1–2 points if they align\n",
        "well with the input query, compared to other relevant content.\n",
        "\n",
        "Provide Justification: For each chunk, include a brief justification explaining the assigned score, referencing the chunk’s semantic and\n",
        "contextual alignment with the input query and noting whether it contains limitations or methodological summaries.\n",
        "\n",
        "Do Not Modify Text: Evaluate each chunk as provided, without modifying or paraphrasing the retrieved text.\n",
        "\n",
        "Handle Irrelevant Chunks: If a chunk is unrelated to the input query or lacks meaningful content, assign a score of 1 with an appropriate\n",
        "justification.\n",
        "\n",
        "Workflow:\n",
        "Plan: Review the input query (original and rewritten paper) and the 10 retrieved text chunks to understand their content and context.\n",
        "\n",
        "Reasoning:\n",
        "Step 1: For each chunk, identify its main topic or content (e.g., limitations, methodology, results, background).\n",
        "Step 2: Compare the chunk’s content to the input query, assessing semantic and contextual alignment with the paper’s sections\n",
        "(e.g., Limitations, Methodology).\n",
        "Step 3: Assign a relevance score (1–10) based on alignment, prioritizing limitations and methodological summaries.\n",
        "Step 4: Write a brief justification for the score, explaining the chunk’s relevance and any priority given to limitations or methodology.\n",
        "Step 5: Verify the score and justification are accurate and consistent with the chunk’s content and the input query.\n",
        "\n",
        "Analyze: Use text analysis tools to confirm semantic alignment (e.g., keyword matching for “limitation,” “constraint,” “methodology,” “sample size”) and assess relevance to the input query.\n",
        "Reflect: Ensure scores and justifications are fair, consistent, and reflect the chunk’s alignment with the query, re-evaluating any ambiguous cases.\n",
        "Continue: Iterate until all 10 chunks are evaluated with scores and justifications.\n",
        "\n",
        "Tool Use:\n",
        "Use text analysis tools to identify limitation-related or methodology-related keywords (e.g., “limited,” “constraint,” “sample size,” “methodology”) and assess semantic similarity between chunks and the input query.\n",
        "Use semantic similarity checks to confirm alignment between the chunk and the query’s key concepts.\n",
        "\n",
        "Chain of Thoughts: Document the reasoning process internally for each chunk. For example:\n",
        "“This chunk mentions a small sample size, a limitation, and aligns closely with the query’s focus, so it receives a high score (9).”\n",
        "“This chunk discusses results without addressing limitations or methodology, so it receives a medium score (6).”\n",
        "“This chunk is generic and unrelated to the query’s specific content, so it receives a low score (1).”\n",
        "\n",
        "Output Format: The output must be in strict JSON format, containing an array of 10 objects, one for each retrieved text chunk, with the\n",
        "following structure for each object:\n",
        "\"Chunk_number\": [Chunk number, e.g., \"Chunk 1\", \"Chunk 2\", ..., \"Chunk 10\"]\n",
        "\"relevance_score\": [Integer from 1 to 10]\n",
        "\"justification\": [Brief explanation of the score, referencing the chunk’s semantic and contextual alignment with the query and any emphasis on limitations or methodological summaries]\n",
        "\n",
        "Example: Input: Input Query: [Full text of the original scientific paper and its rewritten version] Retrieved Text Chunks:\n",
        "\n",
        "Chunk 1: chunk_001: The study was limited by a small sample size, which may affect generalizability.\n",
        "Chunk 2: chunk_002: The experiment used a randomized controlled trial design to test the algorithm.\n",
        "Chunk 3: chunk_003: The experiment achieved a 20% improvement in processing speed.\n",
        "...\n",
        "Chunk 10: chunk_010: Data processing is a key challenge in modern research.\n",
        "\n",
        "Output: [ { \"Chunk_number\": \"Chunk 1\", \"relevance_score\": 9, \"justification\": \"The chunk has strong semantic and contextual alignment with\n",
        "the input query, explicitly discussing a limitation (small sample size), which is a high-priority element for limitation generation.\" },\n",
        "{ \"Chunk_number\": \"Chunk 2\", \"relevance_score\": 8, \"justification\": \"The chunk aligns well with the input query by describing the\n",
        "methodological approach, a high-priority element, though it is slightly less central than limitations-related content.\" },\n",
        "{ \"Chunk_number\": \"Chunk 3\", \"relevance_score\": 6, \"justification\": \"The chunk has moderate semantic and contextual alignment,\n",
        "discussing experimental results, but lacks focus on limitations or methodology, resulting in a mid-range score.\" },\n",
        "...\n",
        "{ \"Chunk_number\": \"Chunk 10\", \"relevance_score\": 3, \"justification\": \"The chunk provides generic background information with minimal\n",
        "semantic and contextual alignment to the input query’s specific concepts or arguments.\" } ] '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_retriever_for_docs(docs, k=20):\n",
        "    # FAISS\n",
        "    faiss_store = FAISS.from_documents(docs, hf_emb)\n",
        "    faiss_r     = faiss_store.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # BM25\n",
        "    bm25_r      = BM25Retriever.from_documents(docs)\n",
        "    bm25_r.k    = k\n",
        "\n",
        "    # ensemble (50/50 weight)\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )"
      ],
      "metadata": {
        "id": "bbJSE6Yfg--j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME         = \"gpt-4o-mini\"\n",
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "def truncate_for_context(query: str,passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    enc       = tiktoken.encoding_for_model(model)\n",
        "    # allow all specials\n",
        "    q_tokens  = enc.encode(query, disallowed_special=())\n",
        "    budget    = max_tokens - len(q_tokens)\n",
        "    kept, used = [], 0\n",
        "    for p in passages:\n",
        "        p_toks = enc.encode(p, disallowed_special=())\n",
        "        if used + len(p_toks) > budget:\n",
        "            if budget - used > 0:\n",
        "                kept.append(enc.decode(p_toks[:(budget - used)]))\n",
        "            break\n",
        "        kept.append(p)\n",
        "        used += len(p_toks)\n",
        "    return kept\n",
        "\n",
        "\n",
        "def ensure_passages_within_budget(\n",
        "    query: str,\n",
        "    passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Returns `passages` truncated so that\n",
        "    count_tokens(query + concat(passages)) ≤ max_tokens.\n",
        "    \"\"\"\n",
        "    # count full size\n",
        "    total = count_tokens(query + \"\\n\\n\".join(passages), model=model)\n",
        "    if total <= max_tokens:\n",
        "        return passages\n",
        "\n",
        "    print(f\"Truncating context ({total} tokens)…\")\n",
        "    # truncate_for_context only returns the shorter passages list\n",
        "    return truncate_for_context(query, passages, max_tokens=max_tokens, model=model)\n"
      ],
      "metadata": {
        "id": "LKNikDs1g-3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "import tiktoken\n",
        "\n",
        "# ─── Constants & Helpers ───────────────────────────────────────────────────\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "df['top20_docs']            = ''\n",
        "df['top20_suffixes']        = ''\n",
        "df['top20_col_names']       = ''\n",
        "df['top20_ref_ids']         = ''\n",
        "df['retrieved_text_llm_asses'] = ''\n",
        "df['faiss_bm_top_5']        = ''\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    row = df.iloc[i]\n",
        "    print(\"i is\", i)\n",
        "\n",
        "    # 1) Embed own text\n",
        "    inputs = {\n",
        "        col: row[col].strip()\n",
        "        for col in main_cols\n",
        "        if isinstance(row.get(col), str) and row[col].strip()\n",
        "    }\n",
        "    if not inputs:\n",
        "        continue\n",
        "    own_emb = embed_model.encode(\n",
        "        \" \".join(inputs.values()), convert_to_tensor=True\n",
        "    )\n",
        "\n",
        "    # 2) Gather all ref sims\n",
        "    ref_sims = []\n",
        "    for ref_id in range(1, 116):\n",
        "        texts = [\n",
        "            row[f\"neurips_ref_{ref_id}_{suf}\"].strip()\n",
        "            for suf in ref_suffixes\n",
        "            if isinstance(row.get(f\"neurips_ref_{ref_id}_{suf}\"), str)\n",
        "               and row[f\"neurips_ref_{ref_id}_{suf}\"].strip()\n",
        "        ]\n",
        "        if not texts:\n",
        "            continue\n",
        "        ref_emb = embed_model.encode(\n",
        "            \" \".join(texts), convert_to_tensor=True\n",
        "        )\n",
        "        sim = cosine_similarity(\n",
        "            ref_emb.cpu().numpy().reshape(1, -1),\n",
        "            own_emb.cpu().numpy().reshape(1, -1)\n",
        "        )[0][0]\n",
        "        ref_sims.append((ref_id, sim))\n",
        "    if not ref_sims:\n",
        "        continue\n",
        "\n",
        "    # 3) Dump & chunk\n",
        "    selected = [rid for rid, _ in ref_sims]\n",
        "    keep_cols = [\n",
        "        f\"neurips_ref_{rid}_{suf}\"\n",
        "        for rid in selected\n",
        "        for suf in ref_suffixes\n",
        "        if isinstance(row.get(f\"neurips_ref_{rid}_{suf}\"), str)\n",
        "           and row[f\"neurips_ref_{rid}_{suf}\"].strip()\n",
        "    ]\n",
        "    tmp_csv = \"df_rag_train.csv\"\n",
        "    df.loc[[i], keep_cols].dropna(axis=1, how=\"all\") \\\n",
        "        .to_csv(tmp_csv, index=False)\n",
        "\n",
        "    df_rag = pd.read_csv(tmp_csv)\n",
        "    lc_docs = []\n",
        "    for col in df_rag.columns:\n",
        "        txt = df_rag.at[0, col]\n",
        "        if isinstance(txt, str) and txt.strip():\n",
        "            suf = col.rsplit(\"_\", 1)[-1]\n",
        "            lc_docs.append(Document(\n",
        "                page_content=txt.strip(),\n",
        "                metadata={\"column_suffix\": suf, \"col_name\": col}\n",
        "            ))\n",
        "\n",
        "    splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
        "    chunked = splitter.split_documents(lc_docs)\n",
        "\n",
        "    print(f\"Number of chunks created: {len(chunked)}\")\n",
        "\n",
        "    # 4) Build query\n",
        "    input_query = (\n",
        "        f\"Scientific paper:\\n{row['response_string_extensive']}\\n\\n\"\n",
        "        f\"Rewritten version of scientific paper:\\n{row['Input_Query_rewrite']}\"\n",
        "    )\n",
        "\n",
        "    # 5) Ensemble top-20 then slice\n",
        "    ensemble = make_retriever_for_docs(chunked, k=20)\n",
        "    all_hits = ensemble.get_relevant_documents(input_query)\n",
        "    top20 = all_hits[:20]\n",
        "\n",
        "    top20_texts, top20_sufs, top20_cols, top20_refs = [], [], [], []\n",
        "    for doc in top20:\n",
        "        top20_texts.append(doc.page_content)\n",
        "        top20_sufs.append(doc.metadata[\"column_suffix\"])\n",
        "        top20_cols.append(doc.metadata[\"col_name\"])\n",
        "        top20_refs.append(int(doc.metadata[\"col_name\"].split(\"_\")[2]))\n",
        "\n",
        "    df.at[i, \"top20_docs\"]      = top20_texts\n",
        "    df.at[i, \"top20_suffixes\"]  = top20_sufs\n",
        "    df.at[i, \"top20_col_names\"] = top20_cols\n",
        "    df.at[i, \"top20_ref_ids\"]   = top20_refs\n",
        "\n",
        "    # 6) Batch LLM calls of 10 chunks each with token budget\n",
        "    all_llm_scores = []\n",
        "    prefix = (\n",
        "        f\"{Rel_Prompt}\\n\\n\"\n",
        "        f\"Input Query:\\n{input_query}\\n\\n\"\n",
        "        \"Here are up to 10 retrieved text chunks:\\n\"\n",
        "    )\n",
        "    prefix_len = count_tokens(prefix, model=MODEL_NAME)\n",
        "    question = (\n",
        "        \"\\\\nOn a scale of 1–10, how relevant is each chunk to the above Input Query? Respond with JSON array with Chunk Number, Score, and Justification for each chunk.\"\n",
        "    )\n",
        "    question_len = count_tokens(question, model=MODEL_NAME)\n",
        "\n",
        "    for batch_start in (0, 10):\n",
        "        batch_docs  = top20[batch_start:batch_start+10]\n",
        "        batch_texts = [d.page_content for d in batch_docs]\n",
        "\n",
        "        # truncate passages to fit\n",
        "        available = MAX_CONTEXT_TOKENS - prefix_len - question_len\n",
        "        kept, used = [], 0\n",
        "        enc = tiktoken.encoding_for_model(MODEL_NAME)\n",
        "        for p in batch_texts:\n",
        "            toks = enc.encode(p, disallowed_special=())\n",
        "            if used + len(toks) > available:\n",
        "                break\n",
        "            kept.append(p)\n",
        "            used += len(toks)\n",
        "\n",
        "        chunks_list = \"\\n\\n\".join(\n",
        "            f\"Chunk {batch_start+idx+1}: {text}\"\n",
        "            for idx, text in enumerate(kept)\n",
        "        )\n",
        "        prompt = prefix + chunks_list + question\n",
        "\n",
        "        raw = run_critic(prompt)\n",
        "        m = re.search(r\"```json\\s*(\\[[\\s\\S]*?\\])\\s*```\", raw)\n",
        "        json_text = m.group(1) if m else raw\n",
        "        try:\n",
        "            batch_scores = json.loads(json_text)\n",
        "        except json.JSONDecodeError:\n",
        "            batch_scores = [\n",
        "                s.strip().strip('\"')\n",
        "                for s in json_text.strip(\"[] \\n\").split(\",\")\n",
        "                if s.strip()\n",
        "            ]\n",
        "        all_llm_scores.extend(batch_scores)\n",
        "\n",
        "    df.at[i, \"retrieved_text_llm_asses\"] = all_llm_scores\n",
        "\n",
        "    if i == 0 or i==20:\n",
        "        print('all_llm_scores', all_llm_scores)\n",
        "\n",
        "    # 7) Baseline top-5 final\n",
        "    final5 = top20[:5]\n",
        "    df.at[i, \"faiss_bm_top_5\"] = \"\\n\\n\".join(d.page_content for d in final5)\n",
        "\n",
        "# 8) Save and timing\n",
        "df.to_csv(\"/df_neurips_lim_OR_with_cited_data_rerank_LLM_rag_listwise.csv\",index=False)\n",
        "print(\"Total time:\", time.time() - start)\n"
      ],
      "metadata": {
        "id": "Su835jb-g-vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_prompts = {\n",
        "    \"Extractor\": Extractor,\n",
        "    \"Analyzer\":  Analyzer,\n",
        "    \"Reviewer\":  Reviewer,\n",
        "    \"Citation\":  Citation,\n",
        "}\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "parsed_feedback_rows = []\n",
        "metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "judge_feedback = []\n",
        "generated_limitations = []\n",
        "\n",
        "\n",
        "df[\"failed_parse_assessment\"] = \"\"\n",
        "\n",
        "# Initialize df_all before the loop\n",
        "df_all = pd.DataFrame()\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"\\nProcessing row {i}\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # build per‐row CSV\n",
        "    inputs = {col: row[col].strip() for col in main_cols\n",
        "              if isinstance(row.get(col), str) and row[col].strip()}\n",
        "    if not inputs:\n",
        "        print(f\"Row {i} has no valid main sections.\"); continue\n",
        "    own_text = \" \".join(inputs.values())\n",
        "    own_emb  = embed_model.encode(own_text, convert_to_tensor=True)\n",
        "\n",
        "    keep_cols = []\n",
        "    # 2) Measure similarity for each referenced paper\n",
        "    ref_sims = []\n",
        "    for ref_id in range(1, 116):\n",
        "        # gather all non‐empty sections for this reference\n",
        "        texts = []\n",
        "        for suf in ref_suffixes:\n",
        "            col = f\"neurips_ref_{ref_id}_{suf}\"\n",
        "            t   = row.get(col, \"\")\n",
        "            if isinstance(t, str) and t.strip():\n",
        "                texts.append(t.strip())\n",
        "        if not texts:\n",
        "            continue\n",
        "        # concatenate and embed\n",
        "        ref_text = \" \".join(texts)\n",
        "        ref_emb  = embed_model.encode(ref_text, convert_to_tensor=True)\n",
        "        # cosine similarity requires numpy arrays\n",
        "        sim = cosine_similarity(\n",
        "            ref_emb.cpu().numpy().reshape(1, -1),\n",
        "            own_emb.cpu().numpy().reshape(1, -1)\n",
        "        )[0][0]\n",
        "        ref_sims.append((ref_id, sim))\n",
        "\n",
        "    if not ref_sims:\n",
        "        print(\" No references to filter; skipping row.\")\n",
        "        continue\n",
        "\n",
        "    # 3) Gap‐based filtering on the full ref_sims list\n",
        "    # Sort descending by similarity, compute adjacent gaps, pick those before the largest gap\n",
        "    ref_sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    sims = [sim for _, sim in ref_sims]\n",
        "    # print(\"sims are\",sims)\n",
        "    if len(sims) < 2:\n",
        "        # Fewer than 2 refs → keep them all\n",
        "        selected = [rid for rid, _ in ref_sims]\n",
        "    else:\n",
        "        # compute gaps between adjacent sims\n",
        "        gaps = [sims[i] - sims[i+1] for i in range(len(sims) - 1)]\n",
        "        # find the index of the largest jump\n",
        "        max_gap_idx = gaps.index(max(gaps))\n",
        "        # keep every ref up through that jump\n",
        "        selected = [rid for rid, _ in ref_sims[: max_gap_idx + 1]]\n",
        "    # print(\"gaps are\", gaps)\n",
        "    # print(f\"Selected {len(selected)} refs by gap filtering\")\n",
        "    # print(f\" Selected {len(selected)} refs after threshold+gap filtering (mean={mean_sim:.3f})\")\n",
        "\n",
        "    # 5) rebuild your keep_cols using only those selected ref_ids\n",
        "    keep_cols = []\n",
        "    for ref_id in selected:\n",
        "        for suf in ref_suffixes:\n",
        "            col = f\"neurips_ref_{ref_id}_{suf}\"\n",
        "            t   = row.get(col, \"\")\n",
        "            if isinstance(t, str) and t.strip():\n",
        "                keep_cols.append(col)\n",
        "\n",
        "    keep_cols = [\n",
        "    c for c in keep_cols\n",
        "    if c in df.columns and isinstance(row[c], str) and row[c].strip()\n",
        "    ]\n",
        "    # print(\"keep cols\",keep_cols)\n",
        "\n",
        "    # print(\"keep cols\", keep_cols)\n",
        "    csv_path = \"/media/ibrahim/Extreme SSD/Limitations Data/RAG/RAG1/df_rag_train.csv\"\n",
        "    # drop the column which has 'NaN' value\n",
        "    df.loc[[i], keep_cols].dropna(axis=1, how=\"all\").to_csv(csv_path, index=False)\n",
        "\n",
        "    # load & chunk\n",
        "    df_rag = pd.read_csv(csv_path)\n",
        "    lc_docs = []\n",
        "    for col in df_rag.columns:\n",
        "        text = df_rag.loc[0, col]\n",
        "        if isinstance(text, str) and text.strip():\n",
        "            lc_docs.append(\n",
        "                Document(\n",
        "                    page_content=text.strip(),\n",
        "                    metadata={\"source_column\": col}\n",
        "                )\n",
        "            )\n",
        "    # lc_docs  = CSVLoader(file_path=csv_path).load()\n",
        "    chunked  = CharacterTextSplitter(chunk_size=512, chunk_overlap=64).split_documents(lc_docs)\n",
        "    # retrieve top‐k docs\n",
        "    retriever = make_retriever_for_docs(chunked, k=3)\n",
        "    # calling 'get_relevant_documents' from langchain.retriever\n",
        "    docs      = retriever.get_relevant_documents(row[\"response_string_all\"])\n",
        "    # 'passages' contains the relevant documents from vector database\n",
        "    passages  = [d.page_content for d in docs]\n",
        "    # passages_str = \"\\n\\n\".join(passages)\n",
        "\n",
        "    # build query (input paper + system prompt)\n",
        "    query = \"Here are the all sections of a paper: \" + row[\"response_string_all\"] + '\\n\\n' + system_prompt\n",
        "    # print(\"query is\",query)\n",
        "\n",
        "    # tokenize + truncate if needed, passages contains retrieved text\n",
        "    passages = [p.replace(\"<|endoftext|>\", \"\") for p in passages]\n",
        "\n",
        "    passages = ensure_passages_within_budget(query, passages)\n",
        "    # assemble final prompt\n",
        "    # prompt = (\n",
        "    #     \"Context:\\n\" + \"\\n\\n\".join(passages) +\n",
        "    #     \"\\n\\nQuestion: \" + row[\"response_string_all\"] +\n",
        "    #     \"\\nAnswer (limitations):\"\n",
        "    # )\n",
        "    retrieved_text = \"\\n\\n\".join(passages)  # passages contains the content from cited papers (vector database)\n",
        "    # print(\"retrieved text is\", retrieved_text)\n",
        "    # ── Store them in df ──\n",
        "    df.at[i, \"query\"] = query\n",
        "    df.at[i, \"retrieved_text\"] = retrieved_text\n",
        "\n",
        "    extractor_agent = run_critic(Extractor + query)\n",
        "    analyzer_agent = run_critic(Analyzer + query)\n",
        "    reviewer_agent = run_critic(Reviewer + query)\n",
        "    citation_agent = run_critic(Citation + retrieved_text)\n",
        "\n",
        "    # ── Store each agent’s output in df ──\n",
        "    df.at[i, \"extractor_agent\"] = extractor_agent\n",
        "    df.at[i, \"analyzer_agent\"]  = analyzer_agent\n",
        "    df.at[i, \"reviewer_agent\"]  = reviewer_agent\n",
        "    df.at[i, \"citation_agent\"]  = citation_agent\n",
        "\n",
        "    agent_texts = {\n",
        "        \"Extractor\": extractor_agent,\n",
        "        \"Analyzer\":  analyzer_agent,\n",
        "        \"Reviewer\":  reviewer_agent,\n",
        "        \"Citation\":  citation_agent\n",
        "    }\n",
        "\n",
        "    combined, row_scores, raw_judge = llm_assessment(\n",
        "        agent_texts=agent_texts,\n",
        "        agent_prompts=agent_prompts,\n",
        "        metrics=metrics\n",
        "    )\n",
        "    # If parsing failed (combined is empty), store raw_judge in a new column\n",
        "    if not combined:\n",
        "        df.at[i, \"failed_parse_assessment\"] = raw_judge\n",
        "        # Optionally, skip further processing for this row:\n",
        "        continue\n",
        "\n",
        "    flattened_records = []\n",
        "    # storing the value\n",
        "    wide_rec = {\"df\": i}\n",
        "\n",
        "    for agent_name, data in combined.items():\n",
        "        # 1) Top‐level total_score\n",
        "        wide_rec[f\"{agent_name}_total_score\"] = data.get(\"total_score\", None)\n",
        "\n",
        "        # 2) Per‐metric fields from data[\"evaluation\"]\n",
        "        evaluation = data.get(\"evaluation\", {})\n",
        "        for metric in metrics:\n",
        "            metric_info = evaluation.get(metric, {})\n",
        "            # Numeric score\n",
        "            wide_rec[f\"{agent_name}_{metric}_score\"] = metric_info.get(\"score\", None)\n",
        "            # Strengths, issues, suggestions\n",
        "            wide_rec[f\"{agent_name}_{metric}_strengths\"]   = metric_info.get(\"strengths\", \"\")\n",
        "            wide_rec[f\"{agent_name}_{metric}_issues\"]      = metric_info.get(\"issues\", \"\")\n",
        "            wide_rec[f\"{agent_name}_{metric}_suggestions\"] = metric_info.get(\"suggestions\", \"\")\n",
        "\n",
        "    # Now write each key/value from wide_rec back into df at row i:\n",
        "    for col_name, value in wide_rec.items():\n",
        "        # Skip the \"df\" entry, since that just equals i\n",
        "        if col_name == \"df\":\n",
        "            continue\n",
        "        df.at[i, col_name] = value\n",
        "\n",
        "df.to_csv(\"/media/ibrahim/Extreme SSD/Limitations Data/NeurIPS_new/self_feedback/df_neurips_self_feedback.csv\",index=False)"
      ],
      "metadata": {
        "id": "QLzJ0VTNnYam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feedback 2nd time"
      ],
      "metadata": {
        "id": "UWhZzmRGnbJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Regenerate_PROMPT = ''' You are task is generating limitations based on Judge Agent feedback.\n",
        "\n",
        "[Criterion]: Strengths: [strengths]. Issues: [issues]. Suggestions: [suggestions]. Can you generate limitations focusing on Strengths and minimize Issues ?\n",
        "Ensure alignment with your role.'''"
      ],
      "metadata": {
        "id": "xr5nRa8vna08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "AGENT_BASE_PROMPTS = {\n",
        "     \"Extractor\":  Extractor,   # the prefix string you use when calling run_critic\n",
        "     \"Analyzer\":   Analyzer,\n",
        "     \"Reviewer\":   Reviewer,\n",
        "     \"Citation\":   Citation\n",
        "}\n",
        "# Assume df now has columns for each agent’s scores, strengths, issues, suggestions,\n",
        "# as well as \"prompt\" and \"text_blob\".\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    for agent in agents:\n",
        "        # 1) Collect all metrics for this agent that have score < 8\n",
        "        feedback_parts = []\n",
        "        for metric in metrics:\n",
        "            score_col = f\"{agent}_{metric}_score\"\n",
        "            score = row.get(score_col, None)\n",
        "            if score is not None and score < 8:\n",
        "                strengths_col   = f\"{agent}_{metric}_strengths\"\n",
        "                issues_col      = f\"{agent}_{metric}_issues\"\n",
        "                suggestions_col = f\"{agent}_{metric}_suggestions\"\n",
        "\n",
        "                strengths   = row.get(strengths_col, \"\")\n",
        "                issues      = row.get(issues_col, \"\")\n",
        "                suggestions = row.get(suggestions_col, \"\")\n",
        "\n",
        "                feedback_parts.append(\n",
        "                    f\"{metric} Feedback:\\n\"\n",
        "                    f\"Strengths: {strengths}\\n\"\n",
        "                    f\"Issues: {issues}\\n\"\n",
        "                    f\"Suggestions: {suggestions}\"\n",
        "                )\n",
        "\n",
        "        # 2) If there is at least one low‐score metric, build a feedback blob and regenerate\n",
        "        if feedback_parts:\n",
        "            feedback_blob = \"\\n\\n\".join(feedback_parts)\n",
        "\n",
        "            full_prompt = (\n",
        "                AGENT_BASE_PROMPTS[agent] +\n",
        "                row[\"prompt\"] +\n",
        "                \"\\n\\n\" +\n",
        "                Regenerate_PROMPT +\n",
        "                \"\\n\\n\" +\n",
        "                feedback_blob +\n",
        "                \"\\n\\n\" +\n",
        "                row[\"text_blob\"]\n",
        "            )\n",
        "\n",
        "            regenerated_output = run_critic(full_prompt)\n",
        "\n",
        "            # 3) Store the regenerated result back into df under a new column\n",
        "            regen_col = f\"{agent}_regenerated\"\n",
        "            df.at[i, regen_col] = regenerated_output\n",
        "\n",
        "df.to_csv(\"df_neurips_self_feedback.csv\",index=False)"
      ],
      "metadata": {
        "id": "wwBG_p6gnauo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Master agent"
      ],
      "metadata": {
        "id": "CFZ0mIl1newv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COORDINATOR_PROMPT = '''\n",
        "    You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided by four agents:\n",
        "    1. **Extractor** (explicit limitations from the article),\n",
        "    2. **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. **Reviewer** (limitations from an open review perspective),\n",
        "    4. **Citation** (limitations based on cited papers).\n",
        "\n",
        "    **Goals**:\n",
        "    1. Combine all limitations into a cohesive, non-redundant list.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize author-stated limitations, supplementing with inferred, peer-review, or citation-based limitations if they add value.\n",
        "    4. Resolve discrepancies between agents’ outputs by cross-referencing the article and cited papers, using tools to verify content.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report, with citations for external sources.\n",
        "\n",
        "    **Workflow** (inspired by SYS_PROMPT_SWEBENCH):\n",
        "    1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "    2. **Analyze**: Combine limitations, prioritizing author-stated ones, and verify alignment with the article.\n",
        "    3. **Reflect**: Check for completeness, scientific rigor, and clarity; resolve discrepancies using article content or tools.\n",
        "    4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "    **Output Format**:\n",
        "    - Numbered list of final limitations.\n",
        "    - For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived], [Cited-papers]).\n",
        "    - Include citations for external sources (e.g., web/X posts, cited papers) in the format [Source Name](ID).\n",
        "    **Tool Use**:\n",
        "    - Use text extraction tools to verify article content.\n",
        "    - Use citation lookup tools to cross-reference cited papers.\n",
        "    - Use web/X search tools to resolve discrepancies involving external context.\n",
        "\n",
        "    **Input**: '''\n"
      ],
      "metadata": {
        "id": "OH5AJYvmnga_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# taking the self-feedback if it exists otherwise acutal one\n",
        "def master_agent(extractor_text, analyzer_text, reviewer_text, citation_text):\n",
        "    \"\"\"\n",
        "    Takes the outputs of the four specialized agents and produces\n",
        "    the final coordinated limitations via a GPT call.\n",
        "    \"\"\"\n",
        "    coord_prompt = (\n",
        "        COORDINATOR_PROMPT\n",
        "        + f\"**Extractor Agent**:\\n{extractor_text}\\n\\n\"\n",
        "        + f\"**Analyzer Agent**:\\n{analyzer_text}\\n\\n\"\n",
        "        + f\"**Reviewer Agent**:\\n{reviewer_text}\\n\\n\"\n",
        "        + f\"**Citation Agent**:\\n{citation_text}\\n\\n\"\n",
        "        + \"Produce a single, numbered list of final limitations, noting each source in brackets.\"\n",
        "    )\n",
        "    return run_critic(coord_prompt)"
      ],
      "metadata": {
        "id": "aMfPnsmdnhra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure pandas recognizes NaN\n",
        "import numpy as np\n",
        "\n",
        "# Add a column to store the master agent’s output\n",
        "df[\"final_limitations\"] = \"\"\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # 1) Choose the extractor text: regenerated if available, otherwise original\n",
        "    if pd.notna(row[\"Extractor_regenerated\"]):\n",
        "        extractor_text = row[\"Extractor_regenerated\"]\n",
        "    else:\n",
        "        extractor_text = row[\"extractor_agent\"]\n",
        "\n",
        "    # 2) Analyzer text\n",
        "    if pd.notna(row[\"Analyzer_regenerated\"]):\n",
        "        analyzer_text = row[\"Analyzer_regenerated\"]\n",
        "    else:\n",
        "        analyzer_text = row[\"analyzer_agent\"]\n",
        "\n",
        "    # 3) Reviewer text\n",
        "    if pd.notna(row[\"Reviewer_regenerated\"]):\n",
        "        reviewer_text = row[\"Reviewer_regenerated\"]\n",
        "    else:\n",
        "        reviewer_text = row[\"reviewer_agent\"]\n",
        "\n",
        "    # 4) Citation text\n",
        "    if pd.notna(row[\"Citation_regenerated\"]):\n",
        "        citation_text = row[\"Citation_regenerated\"]\n",
        "    else:\n",
        "        citation_text = row[\"citation_agent\"]\n",
        "\n",
        "    # 5) Call the master_agent with the chosen texts\n",
        "    final = master_agent(\n",
        "        extractor_text=extractor_text,\n",
        "        analyzer_text=analyzer_text,\n",
        "        reviewer_text=reviewer_text,\n",
        "        citation_text=citation_text\n",
        "    )\n",
        "\n",
        "    # 6) Store the result back into df\n",
        "    df.at[idx, \"final_limitations\"] = final\n"
      ],
      "metadata": {
        "id": "SYcFWT5lnitV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}