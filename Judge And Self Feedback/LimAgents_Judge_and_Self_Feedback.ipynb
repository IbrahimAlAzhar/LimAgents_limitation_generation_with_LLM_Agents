{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "70b107f5-9e3a-43ca-85f3-30ad7db077f2",
        "3c6e798d-bf09-4a94-ab8b-1ffbbc59d71c",
        "1aefc3c5-0875-46f6-a09e-9dccd0cd6cc9",
        "9c1c24d4-929f-4097-9665-536e08b436cb",
        "076c73d7-6df7-4735-9be8-8608bb255f65",
        "1487abb9-4a26-4c3b-b5da-b2e1f2590898",
        "aa5de425-10c3-4498-ad9e-cd4369be77de",
        "df870de6-9f65-482a-94dd-f12f30337364",
        "540c0cc5-f2f2-4e89-a22b-c55e2dce2eab",
        "4a58025a-de8f-4bc7-9bf4-c43819fbda59",
        "d91db639-f11a-447f-96b8-1d2b923894ee",
        "cd9dc36a-6023-4b5f-91ac-c2985cf1cfad"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a949350a-bf0f-4597-802a-95f2dc5e29b5"
      },
      "source": [
        "### LLM Judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd7801dd-8d5a-4a2c-aeac-b6728fa3c551"
      },
      "outputs": [],
      "source": [
        "JUDGE_PROMPT = ''' You are a Judge Agent, an expert in evaluating scientific text quality with a focus on limitation generation for\n",
        "scientific articles. Your task is to assess the outputs of four agents—Extractor (explicit limitations from the article), Analyzer\n",
        "(inferred limitations from critical analysis), Reviewer (peer-review limitations), and Citation (limitations based on cited papers).\n",
        "For each agent’s output, assign a numerical score (0–100) and provide specific feedback based on defined criteria. The evaluation is\n",
        "reference-free, relying on the output’s inherent quality and alignment with each agent’s role.\n",
        "\n",
        "Evaluation Criteria:\n",
        "Depth: How critical and insightful is the limitation? Does it reveal significant issues in the study’s design, findings, or implications?\n",
        "(20% weight)\n",
        "\n",
        "\n",
        "\n",
        "Originality: Is the limitation a generic critique or a novel, context-specific insight? (20% weight)\n",
        "\n",
        "Actionability: Can researchers realistically address the limitation in future work? Does it provide clear paths for improvement?\n",
        "(30% weight)\n",
        "\n",
        "Topic Coverage: How broadly does the set of limitations cover relevant aspects (e.g., methodology, scope for Extractor/Analyzer; peer\n",
        "review standards for Reviewer; cited paper gaps for Citation)? (30% weight)\n",
        "\n",
        "Workflow: Plan: Review each agent’s role and expected output (Extractor: explicit limitations; Analyzer: inferred methodological gaps;\n",
        "Reviewer: peer-review critiques; Citation: cited paper gaps). Identify tools (e.g., text analysis, citation lookup) to verify content\n",
        "if needed.\n",
        "\n",
        "Reasoning: Let’s think step by step to evaluate each output: Step 1: Read the agent’s output and confirm its alignment with the agent’s\n",
        "role. Step 2: Assess each criterion (Depth, Originality, Actionability, Topic Coverage), noting strengths and weaknesses. Step 3: Assign\n",
        "a score (0–10) for each criterion based on quality, then calculate the weighted total (0–100). Step 4: Generate feedback for each\n",
        "criterion, specifying what was done well and what needs improvement. Step 5: Verify the evaluation by cross-checking with the article\n",
        "or cited papers using tools, if necessary.\n",
        "\n",
        "Analyze: Use tools to verify article or cited paper content to ensure accurate evaluation (e.g., confirm Extractor’s quotes, Citation’s\n",
        "references). Reflect: Ensure the score and feedback are fair, consistent, and actionable. Re-evaluate if any criterion seems misjudged.\n",
        "Continue: Iterate until the evaluation is complete for all agents.\n",
        "\n",
        "Tool Use: Use text analysis tools to verify article content (e.g., Extractor’s quotes, Analyzer’s methodology). Use citation lookup\n",
        "tools to confirm cited paper details (e.g., Citation’s references). Use web/X search tools to validate Reviewer’s external context,\n",
        "if needed.\n",
        "\n",
        "Chain of Thoughts: Document the evaluation process explicitly. For example: “The Extractor’s output identifies a limitation but\n",
        "lacks critical insight, reducing Depth.” “The Analyzer’s limitation is generic, affecting Originality.” “The Reviewer’s output is\n",
        "actionable but misses ethical considerations, limiting Topic Coverage.” This narrative ensures transparency and justifies the score\n",
        "and feedback.\n",
        "\n",
        "Scoring: For each criterion, assign a score (0–10) based on quality: 0–3: Poor (major issues, e.g., superficial, generic, not actionable,\n",
        "narrow coverage). 4–6: Fair (moderate issues, e.g., somewhat insightful, partially actionable, incomplete coverage). 7–8: Good\n",
        "(minor issues, e.g., mostly critical, slightly generic, broadly actionable). 9–10: Excellent (no issues, e.g., highly insightful,\n",
        "novel, clearly actionable, comprehensive coverage).\n",
        "\n",
        "Calculate the total score: Sum (criterion score × weight), where weights are Depth (0.2), Originality (0.2), Actionability (0.3),\n",
        "Topic Coverage (0.3).\n",
        "\n",
        "Example: Depth (8 × 0.2 = 1.6), Originality (7 × 0.2 = 1.4), Actionability (9 × 0.3 = 2.7), Topic Coverage (6 × 0.3 = 1.8),\n",
        "Total = (1.6 + 1.4 + 2.7 + 1.8) × 10 = 75.\n",
        "\n",
        "Input:\n",
        "Extractor Agent: [extractor_agent output]\n",
        "Analyzer Agent: [analyzer_agent output]\n",
        "Reviewer Agent: [reviewer_agent output]\n",
        "Citation Agent: [citation_agent output]\n",
        "\n",
        "Output Format: The output must strictly be in JSON format, starting with ```json\\n{...}.\n",
        "For each agent (Extractor, Analyzer, Reviewer, Citation), provide a JSON object with the following structure:\n",
        "\n",
        "{\n",
        "  \"agent\": \"[Agent Name]\",\n",
        "  \"total_score\": [Numerical score, 0–100],\n",
        "  \"evaluation\": {\n",
        "    \"Depth\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Originality\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Actionability\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Topic_Coverage\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    }\n",
        "  }\n",
        "}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aa76056-d2ea-48c0-80df-eba9ea2656b3"
      },
      "source": [
        "### assessment and lim generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76d6c0d6-4a5e-4b64-91c1-e5083d0226b3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def llm_assessment(agent_texts: dict,\n",
        "                   agent_prompts: dict,\n",
        "                   metrics=None):\n",
        "    \"\"\"\n",
        "    Performs the LLM assessment (collective judge) to generate scores and feedback for each agent.\n",
        "    If parsing fails, returns the raw LLM response in a third return value.\n",
        "\n",
        "    Returns:\n",
        "      combined: dict mapping agent name to parsed JSON evaluation data (empty if parse failed)\n",
        "      row_scores: dict of per-agent score keys (scores or None)\n",
        "      raw_response: the unparsed LLM output string\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "\n",
        "    # 1) Fire off the collective judge prompt\n",
        "    raw_response = run_critic(\n",
        "        JUDGE_PROMPT +\n",
        "        \"\".join(f\"**{agent} Agent**:\\n{agent_texts[agent]}\\n\\n\"\n",
        "                for agent in agent_prompts) +\n",
        "        JUDGE_PROMPT\n",
        "    )\n",
        "    # print(\"collective judge response:\\n\", raw_response)\n",
        "\n",
        "    # 2) Extract JSON-fenced blocks\n",
        "    blocks = re.findall(r\"```json\\n(.*?)```\", raw_response, re.DOTALL)\n",
        "\n",
        "    if not blocks:\n",
        "        # No JSON blocks found at all → return empty combined and scores, plus raw text\n",
        "        # print(\"⚠️ Warning: No JSON-fenced sections found in collective_judge.\")\n",
        "        return {}, {f\"{agent}_score\": None for agent in agent_prompts}, raw_response\n",
        "\n",
        "    combined = {}\n",
        "    for b in blocks:\n",
        "        try:\n",
        "            parsed = json.loads(b)\n",
        "            agent_name = parsed.get(\"agent\")\n",
        "            if agent_name:\n",
        "                combined[agent_name] = parsed\n",
        "            # else:\n",
        "            #     print(\"⚠️ Warning: JSON block missing 'agent' field:\", b)\n",
        "        except json.JSONDecodeError:\n",
        "            # print(\"⚠️ Warning: Failed to parse JSON block:\", b)\n",
        "            # skip it\n",
        "            pass\n",
        "\n",
        "    # If combined is still empty, parsing failed entirely\n",
        "    if not combined:\n",
        "        # print(\"⚠️ Warning: Parsed no valid agent entries. Returning empty scores.\")\n",
        "        return {}, {f\"{agent}_score\": None for agent in agent_prompts}, raw_response\n",
        "\n",
        "    # 3) Build row_scores from combined\n",
        "    row_scores = {}\n",
        "    for agent, data in combined.items():\n",
        "        row_scores[f\"{agent}_score\"] = data.get(\"total_score\")\n",
        "\n",
        "    return combined, row_scores, raw_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70b107f5-9e3a-43ca-85f3-30ad7db077f2"
      },
      "source": [
        "### limitation generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900f49a6-713e-4b5f-9150-ce1a1c2c2acc"
      },
      "outputs": [],
      "source": [
        "system_prompt = '''You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\n",
        " Generate limitations or shortcomings for the following passages from the scientific paper.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_prompts = {\n",
        "    \"Extractor\": Extractor,\n",
        "    \"Analyzer\":  Analyzer,\n",
        "    \"Reviewer\":  Reviewer,\n",
        "    \"Citation\":  Citation,\n",
        "}\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "parsed_feedback_rows = []\n",
        "metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "judge_feedback = []\n",
        "generated_limitations = []\n",
        "\n",
        "\n",
        "df[\"failed_parse_assessment\"] = \"\"\n",
        "\n",
        "# Initialize df_all before the loop\n",
        "df_all = pd.DataFrame()\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"\\nProcessing row {i}\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # build per‐row CSV\n",
        "    inputs = {col: row[col].strip() for col in main_cols\n",
        "              if isinstance(row.get(col), str) and row[col].strip()}\n",
        "    if not inputs:\n",
        "        print(f\"Row {i} has no valid main sections.\"); continue\n",
        "    own_text = \" \".join(inputs.values())\n",
        "    own_emb  = embed_model.encode(own_text, convert_to_tensor=True)\n",
        "\n",
        "    keep_cols = []\n",
        "    # 2) Measure similarity for each referenced paper\n",
        "    ref_sims = []\n",
        "    for ref_id in range(1, 116):\n",
        "        # gather all non‐empty sections for this reference\n",
        "        texts = []\n",
        "        for suf in ref_suffixes:\n",
        "            col = f\"neurips_ref_{ref_id}_{suf}\"\n",
        "            t   = row.get(col, \"\")\n",
        "            if isinstance(t, str) and t.strip():\n",
        "                texts.append(t.strip())\n",
        "        if not texts:\n",
        "            continue\n",
        "        # concatenate and embed\n",
        "        ref_text = \" \".join(texts)\n",
        "        ref_emb  = embed_model.encode(ref_text, convert_to_tensor=True)\n",
        "        # cosine similarity requires numpy arrays\n",
        "        sim = cosine_similarity(\n",
        "            ref_emb.cpu().numpy().reshape(1, -1),\n",
        "            own_emb.cpu().numpy().reshape(1, -1)\n",
        "        )[0][0]\n",
        "        ref_sims.append((ref_id, sim))\n",
        "\n",
        "    if not ref_sims:\n",
        "        print(\" No references to filter; skipping row.\")\n",
        "        continue\n",
        "\n",
        "    ref_sims.sort(key=lambda x: x[1], reverse=True)\n",
        "    sims = [sim for _, sim in ref_sims]\n",
        "    # print(\"sims are\",sims)\n",
        "    if len(sims) < 2:\n",
        "        # Fewer than 2 refs → keep them all\n",
        "        selected = [rid for rid, _ in ref_sims]\n",
        "    else:\n",
        "        # compute gaps between adjacent sims\n",
        "        gaps = [sims[i] - sims[i+1] for i in range(len(sims) - 1)]\n",
        "        # find the index of the largest jump\n",
        "        max_gap_idx = gaps.index(max(gaps))\n",
        "        # keep every ref up through that jump\n",
        "        selected = [rid for rid, _ in ref_sims[: max_gap_idx + 1]]\n",
        "\n",
        "    keep_cols = []\n",
        "    for ref_id in selected:\n",
        "        for suf in ref_suffixes:\n",
        "            col = f\"neurips_ref_{ref_id}_{suf}\"\n",
        "            t   = row.get(col, \"\")\n",
        "            if isinstance(t, str) and t.strip():\n",
        "                keep_cols.append(col)\n",
        "\n",
        "    keep_cols = [\n",
        "    c for c in keep_cols\n",
        "    if c in df.columns and isinstance(row[c], str) and row[c].strip()\n",
        "    ]\n",
        "\n",
        "    csv_path = \"df_rag_train.csv\"\n",
        "    # drop the column which has 'NaN' value\n",
        "    df.loc[[i], keep_cols].dropna(axis=1, how=\"all\").to_csv(csv_path, index=False)\n",
        "\n",
        "    # load & chunk\n",
        "    df_rag = pd.read_csv(csv_path)\n",
        "    lc_docs = []\n",
        "    for col in df_rag.columns:\n",
        "        text = df_rag.loc[0, col]\n",
        "        if isinstance(text, str) and text.strip():\n",
        "            lc_docs.append(\n",
        "                Document(\n",
        "                    page_content=text.strip(),\n",
        "                    metadata={\"source_column\": col}\n",
        "                )\n",
        "            )\n",
        "    # lc_docs  = CSVLoader(file_path=csv_path).load()\n",
        "    chunked  = CharacterTextSplitter(chunk_size=512, chunk_overlap=64).split_documents(lc_docs)\n",
        "    # retrieve top‐k docs\n",
        "    retriever = make_retriever_for_docs(chunked, k=3)\n",
        "    # calling 'get_relevant_documents' from langchain.retriever\n",
        "    docs      = retriever.get_relevant_documents(row[\"response_string_all\"])\n",
        "    # 'passages' contains the relevant documents from vector database\n",
        "    passages  = [d.page_content for d in docs]\n",
        "    # passages_str = \"\\n\\n\".join(passages)\n",
        "\n",
        "    # build query (input paper + system prompt)\n",
        "    query = \"Here are the all sections of a paper: \" + row[\"response_string_all\"] + '\\n\\n' + system_prompt\n",
        "    # print(\"query is\",query)\n",
        "\n",
        "    # tokenize + truncate if needed, passages contains retrieved text\n",
        "    passages = [p.replace(\"<|endoftext|>\", \"\") for p in passages]\n",
        "\n",
        "    passages = ensure_passages_within_budget(query, passages)\n",
        "\n",
        "    retrieved_text = \"\\n\\n\".join(passages)  # passages contains the content from cited papers (vector database)\n",
        "    # print(\"retrieved text is\", retrieved_text)\n",
        "    # ── Store them in df ──\n",
        "    df.at[i, \"query\"] = query\n",
        "    df.at[i, \"retrieved_text\"] = retrieved_text\n",
        "\n",
        "    extractor_agent = run_critic(Extractor + query)\n",
        "    analyzer_agent = run_critic(Analyzer + query)\n",
        "    reviewer_agent = run_critic(Reviewer + query)\n",
        "    citation_agent = run_critic(Citation + retrieved_text)\n",
        "\n",
        "    # ── Store each agent’s output in df ──\n",
        "    df.at[i, \"extractor_agent\"] = extractor_agent\n",
        "    df.at[i, \"analyzer_agent\"]  = analyzer_agent\n",
        "    df.at[i, \"reviewer_agent\"]  = reviewer_agent\n",
        "    df.at[i, \"citation_agent\"]  = citation_agent\n",
        "\n",
        "    agent_texts = {\n",
        "        \"Extractor\": extractor_agent,\n",
        "        \"Analyzer\":  analyzer_agent,\n",
        "        \"Reviewer\":  reviewer_agent,\n",
        "        \"Citation\":  citation_agent\n",
        "    }\n",
        "\n",
        "    combined, row_scores, raw_judge = llm_assessment(\n",
        "        agent_texts=agent_texts,\n",
        "        agent_prompts=agent_prompts,\n",
        "        metrics=metrics\n",
        "    )\n",
        "    # If parsing failed (combined is empty), store raw_judge in a new column\n",
        "    if not combined:\n",
        "        df.at[i, \"failed_parse_assessment\"] = raw_judge\n",
        "        # Optionally, skip further processing for this row:\n",
        "        continue\n",
        "\n",
        "    flattened_records = []\n",
        "    # storing the value\n",
        "    wide_rec = {\"df\": i}\n",
        "\n",
        "    for agent_name, data in combined.items():\n",
        "        # 1) Top‐level total_score\n",
        "        wide_rec[f\"{agent_name}_total_score\"] = data.get(\"total_score\", None)\n",
        "\n",
        "        # 2) Per‐metric fields from data[\"evaluation\"]\n",
        "        evaluation = data.get(\"evaluation\", {})\n",
        "        for metric in metrics:\n",
        "            metric_info = evaluation.get(metric, {})\n",
        "            # Numeric score\n",
        "            wide_rec[f\"{agent_name}_{metric}_score\"] = metric_info.get(\"score\", None)\n",
        "            # Strengths, issues, suggestions\n",
        "            wide_rec[f\"{agent_name}_{metric}_strengths\"]   = metric_info.get(\"strengths\", \"\")\n",
        "            wide_rec[f\"{agent_name}_{metric}_issues\"]      = metric_info.get(\"issues\", \"\")\n",
        "            wide_rec[f\"{agent_name}_{metric}_suggestions\"] = metric_info.get(\"suggestions\", \"\")\n",
        "\n",
        "    # Now write each key/value from wide_rec back into df at row i:\n",
        "    for col_name, value in wide_rec.items():\n",
        "        # Skip the \"df\" entry, since that just equals i\n",
        "        if col_name == \"df\":\n",
        "            continue\n",
        "        df.at[i, col_name] = value\n",
        "\n",
        "df.to_csv(\"self_feedback/df_neurips_self_feedback.csv\",index=False)"
      ],
      "metadata": {
        "id": "ISm18R3u7ojD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "3c6e798d-bf09-4a94-ab8b-1ffbbc59d71c"
      },
      "source": [
        "### parsing feedback with regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba3707c1-8f8f-4363-bcea-ec3fce3e437d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neurips_self_feedback.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27c5e767-cfc4-4a31-8684-6524ba983350"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# ─── 2. Define which agents and metrics to extract ───\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "fields = [\"score\", \"strengths\", \"issues\", \"suggestions\"]\n",
        "\n",
        "# ─── 3. Create all target columns in advance ───\n",
        "\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        for field in fields:\n",
        "            col_name = f\"{agent}_{metric}_{field}\"\n",
        "            df[col_name] = None  # initialize with None (becomes NaN in pandas)\n",
        "\n",
        "# ─── 4. Iterate over each row and use regex to fill in the new columns ───\n",
        "\n",
        "for idx, cell in df[\"failed_parse_assessment\"].items():\n",
        "    if not isinstance(cell, str):\n",
        "        # If the cell is not a string, continue (all new columns stay None/NaN)\n",
        "        continue\n",
        "\n",
        "    # 4a) We assume the JSON is wrapped in ```json … ```. Extract the {...} portion\n",
        "    match = re.search(r\"\\{.*\\}\", cell, flags=re.DOTALL)\n",
        "    if not match:\n",
        "        # If no braces‐enclosed content is found, skip this row\n",
        "        continue\n",
        "\n",
        "    inner_json = match.group(0)\n",
        "\n",
        "    #    We use non‐greedy `.*?` and DOTALL to allow line breaks.\n",
        "    for agent in agents:\n",
        "        for metric in metrics:\n",
        "            pattern = (\n",
        "                rf'\"{agent}\"\\s*:\\s*\\{{'                              # \"Extractor\": {\n",
        "                rf'.*?\"{metric}\"\\s*:\\s*\\{{'                           #    \"Depth\": {\n",
        "                rf'.*?\"score\"\\s*:\\s*(?P<{agent}_{metric}_score>\\d+)'   #       \"score\": 0\n",
        "                rf'.*?\"strengths\"\\s*:\\s*\"(?P<{agent}_{metric}_strengths>.*?)\"'  # \"strengths\": \"…\"\n",
        "                rf'.*?\"issues\"\\s*:\\s*\"(?P<{agent}_{metric}_issues>.*?)\"'       # \"issues\": \"…\"\n",
        "                rf'.*?\"suggestions\"\\s*:\\s*\"(?P<{agent}_{metric}_suggestions>.*?)\"' # \"suggestions\": \"…\"\n",
        "                rf'.*?\\}}'                                             #    }\n",
        "            )\n",
        "\n",
        "            m = re.search(pattern, inner_json, flags=re.DOTALL)\n",
        "            if m:\n",
        "                # Extract each named group if it matched\n",
        "                df.at[idx, f\"{agent}_{metric}_score\"] = m.group(f\"{agent}_{metric}_score\")\n",
        "                df.at[idx, f\"{agent}_{metric}_strengths\"] = m.group(f\"{agent}_{metric}_strengths\")\n",
        "                df.at[idx, f\"{agent}_{metric}_issues\"] = m.group(f\"{agent}_{metric}_issues\")\n",
        "                df.at[idx, f\"{agent}_{metric}_suggestions\"] = m.group(f\"{agent}_{metric}_suggestions\")\n",
        "            # If regex didn’t match, leave those columns as None/NaN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aefc3c5-0875-46f6-a09e-9dccd0cd6cc9"
      },
      "source": [
        "### regenerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb666e7f-9e37-427c-9e8b-695ca2451f1f"
      },
      "outputs": [],
      "source": [
        "Regenerate_PROMPT = '''\n",
        "\n",
        "You are tasked with generating limitations based on feedback from the Judge Agent.\n",
        "Feedback Structure:\n",
        "Strengths: [strengths]\n",
        "Issues: [issues]\n",
        "Suggestions: [suggestions]\n",
        "Task: Create a set of limitations that:\n",
        "\n",
        "Builds upon the identified strengths to reinforce positive aspects.\n",
        "Minimizes the impact of issues by addressing them constructively.\n",
        "Incorporates suggestions to ensure actionable improvements.\n",
        "Ensure the limitations are clear, concise, and aligned with your role as [specify role, e.g., a content generator, analyst, etc.].'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify all columns ending with \"_score\"\n",
        "score_cols = [col for col in df.columns if col.endswith(\"_score\")]\n",
        "\n",
        "# For each such column, count how many entries are < 8\n",
        "for col in score_cols:\n",
        "    # Ensure the column is numeric (coerce non-numeric to NaN)\n",
        "    numeric_series = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    count_lt_8 = (numeric_series < 8).sum()\n",
        "    print(f\"{col}: {count_lt_8} rows with value < 8\")\n"
      ],
      "metadata": {
        "id": "g9R5X1hX7wSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ─── 1. Definitions ───\n",
        "\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "\n",
        "AGENT_BASE_PROMPTS = {\n",
        "    \"Extractor\": Extractor,\n",
        "    \"Analyzer\":  Analyzer,\n",
        "    \"Reviewer\":  Reviewer,\n",
        "    \"Citation\":  Citation\n",
        "}\n",
        "\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        score_col = f\"{agent}_{metric}_score\"\n",
        "        df[score_col] = pd.to_numeric(df[score_col], errors=\"coerce\")\n",
        "\n",
        "# ─── 3. Initialize regenerated‐output columns ───\n",
        "\n",
        "for agent in agents:\n",
        "    df[f\"{agent}_regenerated\"] = None\n",
        "\n",
        "# ─── 4. Loop over each row ───\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    print(\"i is\",i)\n",
        "    input_string   = row.get(\"query\", \"\")\n",
        "    retrieved_text = row.get(\"retrieved_text\", \"\")\n",
        "\n",
        "    for agent in agents:\n",
        "        feedback_parts = []\n",
        "\n",
        "        # 4a. Gather feedback for any metric with score < 8\n",
        "        for metric in metrics:\n",
        "            score = row.get(f\"{agent}_{metric}_score\", None)\n",
        "            # if any of them or multiple less than 8, add them and send with feedback for regenerate\n",
        "            if pd.notna(score) and score < 8:\n",
        "                strengths   = row.get(f\"{agent}_{metric}_strengths\", \"\")\n",
        "                issues      = row.get(f\"{agent}_{metric}_issues\", \"\")\n",
        "                suggestions = row.get(f\"{agent}_{metric}_suggestions\", \"\")\n",
        "\n",
        "                feedback_parts.append(\n",
        "                    f\"{metric} Feedback:\\n\"\n",
        "                    f\"  Strengths: {strengths}\\n\"\n",
        "                    f\"  Issues: {issues}\\n\"\n",
        "                    f\"  Suggestions: {suggestions}\"\n",
        "                )\n",
        "\n",
        "        # 4b. If any metric is < 8, build regeneration prompt\n",
        "        if feedback_parts:\n",
        "            feedback_blob = \"\\n\\n\".join(feedback_parts)\n",
        "\n",
        "            # Choose the correct “seed text” based on agent\n",
        "            if agent == \"Citation\":\n",
        "                seed_text = retrieved_text + system_prompt\n",
        "            else:\n",
        "                seed_text = input_string\n",
        "\n",
        "            full_prompt = (\n",
        "                AGENT_BASE_PROMPTS[agent]\n",
        "                + seed_text\n",
        "                + \"\\n\\n\"\n",
        "                + Regenerate_PROMPT\n",
        "                + \"\\n\\n\"\n",
        "                + feedback_blob\n",
        "            )\n",
        "            # 4c. Call the LLM to regenerate and store the result\n",
        "            regenerated_output = run_critic(full_prompt)\n",
        "            df.at[i, f\"{agent}_regenerated\"] = regenerated_output\n",
        "\n",
        "df.to_csv(\"df_neurips_self_feedback.csv\",index=False)"
      ],
      "metadata": {
        "id": "Y1Qw1af971pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "9c1c24d4-929f-4097-9665-536e08b436cb"
      },
      "source": [
        "### Judge"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call judge LLM and store the raw response in dataframe\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    for agent in agents:\n",
        "        regen_col = f\"{agent}_regenerated\"\n",
        "        feedback_col = f\"self_feedback_{agent}\"\n",
        "\n",
        "        # If the regenerated text is exactly None, store 0\n",
        "        if row.get(regen_col) is None:\n",
        "            df.at[i, feedback_col] = 0\n",
        "            continue\n",
        "\n",
        "        # Otherwise, build a single‐agent judge prompt\n",
        "        single_agent_text = row[regen_col]\n",
        "        prompt = (\n",
        "            JUDGE_PROMPT\n",
        "            + f\"**{agent} Agent**:\\n{single_agent_text}\\n\\n\"\n",
        "        )\n",
        "\n",
        "        # Call run_critic and store the response\n",
        "        df.at[i, feedback_col] = run_critic(prompt)\n"
      ],
      "metadata": {
        "id": "7ZsbH3uX7-M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "076c73d7-6df7-4735-9be8-8608bb255f65"
      },
      "source": [
        "### parsing feedback with regex (again)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# ─── 1. Define agents, metrics, and fields ───\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "fields = [\"score\", \"strengths\", \"issues\", \"suggestions\"]\n",
        "\n",
        "# ─── 2. Create new target columns with suffix \"_2\" ───\n",
        "\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        for field in fields:\n",
        "            col_name = f\"{agent}_{metric}_{field}_2\"\n",
        "            df[col_name] = None  # initialize with None (will appear as NaN)\n",
        "\n",
        "# ─── 3. Iterate over each row and each agent-specific self_feedback ───\n",
        "\n",
        "for idx in df.index:\n",
        "    for agent in agents:\n",
        "        feedback_col = f\"self_feedback_{agent}\"\n",
        "        cell = df.at[idx, feedback_col]\n",
        "\n",
        "        # If the agent-specific feedback is exactly 0, skip parsing for this agent\n",
        "        if cell == 0:\n",
        "            continue\n",
        "\n",
        "        # Otherwise, only proceed if it's a non-empty string\n",
        "        if not isinstance(cell, str):\n",
        "            continue\n",
        "\n",
        "        # Extract JSON block between the first \"{\" and the last \"}\"\n",
        "        match = re.search(r\"\\{.*\\}\", cell, flags=re.DOTALL)\n",
        "        if not match:\n",
        "            continue\n",
        "\n",
        "        inner_json = match.group(0)\n",
        "\n",
        "        # Attempt to extract each metric's fields from that JSON blob\n",
        "        for metric in metrics:\n",
        "            pattern = (\n",
        "                rf'\"{agent}\"\\s*:\\s*\\{{'                                     # \"Extractor\": {\n",
        "                rf'.*?\"{metric}\"\\s*:\\s*\\{{'                                  #   \"Depth\": {\n",
        "                rf'.*?\"score\"\\s*:\\s*(?P<{agent}_{metric}_score_2>\\d+)'      #      \"score\": 0\n",
        "                rf'.*?\"strengths\"\\s*:\\s*\"(?P<{agent}_{metric}_strengths_2>.*?)\"'  # \"strengths\": \"…\"\n",
        "                rf'.*?\"issues\"\\s*:\\s*\"(?P<{agent}_{metric}_issues_2>.*?)\"'          # \"issues\": \"…\"\n",
        "                rf'.*?\"suggestions\"\\s*:\\s*\"(?P<{agent}_{metric}_suggestions_2>.*?)\"'# \"suggestions\": \"…\"\n",
        "                rf'.*?\\}}'                                                    #   }\n",
        "            )\n",
        "\n",
        "            m = re.search(pattern, inner_json, flags=re.DOTALL)\n",
        "            if m:\n",
        "                df.at[idx, f\"{agent}_{metric}_score_2\"]       = m.group(f\"{agent}_{metric}_score_2\")\n",
        "                df.at[idx, f\"{agent}_{metric}_strengths_2\"]   = m.group(f\"{agent}_{metric}_strengths_2\")\n",
        "                df.at[idx, f\"{agent}_{metric}_issues_2\"]      = m.group(f\"{agent}_{metric}_issues_2\")\n",
        "                df.at[idx, f\"{agent}_{metric}_suggestions_2\"] = m.group(f\"{agent}_{metric}_suggestions_2\")\n",
        "            # If no match, leave those columns as None/NaN\n"
      ],
      "metadata": {
        "id": "lQWx9pfc8A_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "1487abb9-4a26-4c3b-b5da-b2e1f2590898"
      },
      "source": [
        "### measure score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71aac441-637e-47b1-8c9c-5f9cefbff05f",
        "outputId": "c71dc2e1-7339-4309-b9f6-e121d93d5893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average values for '_score' columns (excluding zeros):\n",
            "Extractor_Depth_score             6.173729\n",
            "Extractor_Originality_score       5.334746\n",
            "Extractor_Actionability_score     7.131356\n",
            "Extractor_Topic_Coverage_score    7.250965\n",
            "Analyzer_Depth_score              7.992537\n",
            "Analyzer_Originality_score        7.370647\n",
            "Analyzer_Actionability_score      8.631841\n",
            "Analyzer_Topic_Coverage_score     8.121891\n",
            "Reviewer_Depth_score              8.109453\n",
            "Reviewer_Originality_score        7.246269\n",
            "Reviewer_Actionability_score      8.266169\n",
            "Reviewer_Topic_Coverage_score     8.333333\n",
            "Citation_Depth_score              7.828358\n",
            "Citation_Originality_score        6.962687\n",
            "Citation_Actionability_score      8.236318\n",
            "Citation_Topic_Coverage_score     8.303483\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# score first time\n",
        "import numpy as np\n",
        "\n",
        "# Identify all columns ending with \"_score\"\n",
        "score_cols = [col for col in df.columns if col.endswith(\"_score\")]\n",
        "\n",
        "# Replace zeros with NaN so they are excluded from the mean\n",
        "score_averages = df[score_cols].replace(0, np.nan).mean()\n",
        "\n",
        "print(\"Average values for '_score' columns (excluding zeros):\")\n",
        "print(score_averages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cea4aed8-84ee-4ccc-b93d-2ac93c39b0ee",
        "outputId": "1335aa18-e9d2-4b87-9c11-e116b6092e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average values for '_score_2' columns (fallback to base column if needed):\n",
            "Extractor_Depth_score_2             6.817955\n",
            "Extractor_Originality_score_2       6.032419\n",
            "Extractor_Actionability_score_2     7.725686\n",
            "Extractor_Topic_Coverage_score_2    7.315920\n",
            "Analyzer_Depth_score_2              8.144279\n",
            "Analyzer_Originality_score_2        7.898010\n",
            "Analyzer_Actionability_score_2      8.743781\n",
            "Analyzer_Topic_Coverage_score_2     8.054726\n",
            "Reviewer_Depth_score_2              8.467662\n",
            "Reviewer_Originality_score_2        7.475124\n",
            "Reviewer_Actionability_score_2      8.810945\n",
            "Reviewer_Topic_Coverage_score_2     8.631841\n",
            "Citation_Depth_score_2              8.674129\n",
            "Citation_Originality_score_2        8.077114\n",
            "Citation_Actionability_score_2      8.417910\n",
            "Citation_Topic_Coverage_score_2     8.850746\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Identify all columns ending with \"_score_2\"\n",
        "score2_cols = [col for col in df.columns if col.endswith(\"_score_2\")]\n",
        "\n",
        "averages = {}\n",
        "\n",
        "for col2 in score2_cols:\n",
        "    # Derive the corresponding base column by removing the trailing \"_2\"\n",
        "    col1 = col2[:-2]\n",
        "\n",
        "    # 1) Take the \"_2\" values, coercing to numeric and replacing 0 with NaN\n",
        "    s2 = pd.to_numeric(df[col2], errors='coerce').replace(0, np.nan)\n",
        "\n",
        "    # 2) If the base column exists, use its nonzero values where s2 is NaN\n",
        "    if col1 in df.columns:\n",
        "        s1 = pd.to_numeric(df[col1], errors='coerce').replace(0, np.nan)\n",
        "        # Fill NaNs in s2 with the values from s1\n",
        "        pref = s2.fillna(s1)\n",
        "    else:\n",
        "        pref = s2\n",
        "\n",
        "    # 3) Compute the mean ignoring NaNs\n",
        "    averages[col2] = pref.mean()\n",
        "\n",
        "# Convert to a pandas Series for nicer output (optional)\n",
        "avg_series = pd.Series(averages)\n",
        "print(\"Average values for '_score_2' columns (fallback to base column if needed):\")\n",
        "print(avg_series)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "aa5de425-10c3-4498-ad9e-cd4369be77de"
      },
      "source": [
        "### regenerate limitations (again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b22ae3e-8f70-45b2-8598-9ae49b783041"
      },
      "outputs": [],
      "source": [
        "Regenerate_PROMPT = '''\n",
        "\n",
        "You are tasked with generating limitations based on feedback from the Judge Agent.\n",
        "Feedback Structure:\n",
        "Strengths: [strengths]\n",
        "Issues: [issues]\n",
        "Suggestions: [suggestions]\n",
        "Task: Create a set of limitations that:\n",
        "\n",
        "Builds upon the identified strengths to reinforce positive aspects.\n",
        "Minimizes the impact of issues by addressing them constructively.\n",
        "Incorporates suggestions to ensure actionable improvements.\n",
        "Ensure the limitations are clear, concise, and aligned with your role as [specify role, e.g., a content generator, analyst, etc.].'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify all columns ending with \"_score\"\n",
        "score_cols = [col for col in df.columns if col.endswith(\"_score_2\")]\n",
        "\n",
        "# For each such column, count how many entries are < 8\n",
        "for col in score_cols:\n",
        "    # Ensure the column is numeric (coerce non-numeric to NaN)\n",
        "    numeric_series = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    count_lt_8 = (numeric_series < 8).sum()\n",
        "    print(f\"{col}: {count_lt_8} rows with value < 8\")\n"
      ],
      "metadata": {
        "id": "ks920DXX8IK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ─── 1. Definitions ───\n",
        "\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "\n",
        "AGENT_BASE_PROMPTS = {\n",
        "    \"Extractor\": Extractor,\n",
        "    \"Analyzer\":  Analyzer,\n",
        "    \"Reviewer\":  Reviewer,\n",
        "    \"Citation\":  Citation\n",
        "}\n",
        "\n",
        "# These should be defined elsewhere in your script:\n",
        "# Regenerate_PROMPT = \"Regenerate your previous answer by considering the feedback below:\\n\"\n",
        "# run_critic(...) should also be defined.\n",
        "\n",
        "# ─── 2. Ensure “_score_2” columns are numeric ───\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        score2_col = f\"{agent}_{metric}_score_2\"\n",
        "        if score2_col in df.columns:\n",
        "            df[score2_col] = pd.to_numeric(df[score2_col], errors=\"coerce\")\n",
        "\n",
        "# ─── 3. Initialize regenerated‐2 columns ───\n",
        "\n",
        "for agent in agents:\n",
        "    df[f\"{agent}_regenerated_2\"] = None\n",
        "\n",
        "# ─── 4. Loop over each row, regenerating based on “_score_2” feedback ───\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    input_string   = row.get(\"query\", \"\")\n",
        "    retrieved_text = row.get(\"retrieved_text\", \"\")\n",
        "\n",
        "    for agent in agents:\n",
        "        feedback_parts = []\n",
        "\n",
        "        # 4a. Gather feedback for any metric where “_score_2” < 8\n",
        "        for metric in metrics:\n",
        "            score2 = row.get(f\"{agent}_{metric}_score_2\", None)\n",
        "            if pd.notna(score2) and score2 < 8:\n",
        "                strengths2   = row.get(f\"{agent}_{metric}_strengths_2\", \"\")\n",
        "                issues2      = row.get(f\"{agent}_{metric}_issues_2\", \"\")\n",
        "                suggestions2 = row.get(f\"{agent}_{metric}_suggestions_2\", \"\")\n",
        "\n",
        "                feedback_parts.append(\n",
        "                    f\"{metric} Feedback:\\n\"\n",
        "                    f\"  Strengths: {strengths2}\\n\"\n",
        "                    f\"  Issues: {issues2}\\n\"\n",
        "                    f\"  Suggestions: {suggestions2}\"\n",
        "                )\n",
        "\n",
        "        # 4b. If any metric_2 is < 8, build regeneration prompt\n",
        "        if feedback_parts:\n",
        "            feedback_blob = \"\\n\\n\".join(feedback_parts)\n",
        "\n",
        "            # Choose “seed text” based on agent\n",
        "            if agent == \"Citation\":\n",
        "                seed_text = retrieved_text + system_prompt\n",
        "            else:\n",
        "                seed_text = input_string + system_prompt\n",
        "\n",
        "            full_prompt = (\n",
        "                AGENT_BASE_PROMPTS[agent]\n",
        "                + seed_text\n",
        "                + \"\\n\\n\"\n",
        "                + Regenerate_PROMPT\n",
        "                + \"\\n\\n\"\n",
        "                + feedback_blob\n",
        "            )\n",
        "            # 4c. Call the LLM to regenerate and store in “_regenerated_2”\n",
        "            regenerated_output = run_critic(full_prompt)\n",
        "            df.at[i, f\"{agent}_regenerated_2\"] = regenerated_output\n",
        "\n",
        "\n",
        "df.to_csv(\n",
        "    \"df_neurips_self_feedback1.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "m9jGzDrw8LO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "df870de6-9f65-482a-94dd-f12f30337364"
      },
      "source": [
        "### Judge (again)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "# 1) Create the new \"self_feedback_2_<Agent>\" columns up front\n",
        "for agent in agents:\n",
        "    df[f\"self_feedback_2_{agent}\"] = None\n",
        "\n",
        "# 2) Loop over each row and each agent\n",
        "for i, row in df.iterrows():\n",
        "    for agent in agents:\n",
        "        regen_col   = f\"{agent}_regenerated_2\"\n",
        "        feedback_col = f\"self_feedback_2_{agent}\"\n",
        "\n",
        "        # If the regenerated text is NaN or None, store 0\n",
        "        if pd.isna(row.get(regen_col)):\n",
        "            df.at[i, feedback_col] = 0\n",
        "            continue\n",
        "\n",
        "        # Otherwise, build a single‐agent judge prompt\n",
        "        single_agent_text = row[regen_col]\n",
        "        prompt = (\n",
        "            JUDGE_PROMPT\n",
        "            + f\"**{agent} Agent**:\\n{single_agent_text}\\n\\n\"\n",
        "        )\n",
        "        # Call run_critic and store the raw response\n",
        "        df.at[i, feedback_col] = run_critic(prompt)\n",
        "\n",
        "df.to_csv(\n",
        "    \"df_neurips_self_feedback1.csv\",\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "Epsfc3sR8Pd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "540c0cc5-f2f2-4e89-a22b-c55e2dce2eab"
      },
      "source": [
        "### parsing (again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "750a9d7e-72c3-4daa-b8c3-c0c9eaa7e4ad"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "agents = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "fields = [\"score\", \"strengths\", \"issues\", \"suggestions\"]\n",
        "\n",
        "# 1) Create the new \"_3\" columns as before\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        for field in fields:\n",
        "            col_name = f\"{agent}_{metric}_{field}_3\"\n",
        "            df[col_name] = None\n",
        "\n",
        "# 2) Loop over each row and parse self_feedback_2_<Agent>\n",
        "for idx in df.index:\n",
        "    for agent in agents:\n",
        "        feedback_col = f\"self_feedback_2_{agent}\"\n",
        "        cell = df.at[idx, feedback_col]\n",
        "\n",
        "        # If it’s exactly 0, skip\n",
        "        if cell == 0:\n",
        "            continue\n",
        "\n",
        "        # Otherwise it must be a string containing JSON\n",
        "        if not isinstance(cell, str):\n",
        "            continue\n",
        "\n",
        "        # Extract the JSON object (including braces) in case there’s extra text\n",
        "        m_obj = re.search(r\"\\{.*\\}\", cell, flags=re.DOTALL)\n",
        "        if not m_obj:\n",
        "            continue\n",
        "\n",
        "        json_blob = m_obj.group(0)\n",
        "\n",
        "        # Now look for the “evaluation” block, then inside it each metric\n",
        "        # e.g.  \"evaluation\": { \"Depth\": { \"score\": 5, \"strengths\": \"…\", … }, … }\n",
        "        for metric in metrics:\n",
        "            # Build a pattern that finds, inside \"evaluation\", the right metric\n",
        "            pattern = (\n",
        "                rf'\"evaluation\"\\s*:\\s*\\{{'                # \"evaluation\": {\n",
        "                rf'.*?\"{metric}\"\\s*:\\s*\\{{'                #    \"Depth\": {\n",
        "                rf'.*?\"score\"\\s*:\\s*(?P<{agent}_{metric}_score_3>\\d+),'\n",
        "                                                      #       \"score\": 5,\n",
        "                rf'.*?\"strengths\"\\s*:\\s*\"(?P<{agent}_{metric}_strengths_3>.*?)\",'\n",
        "                                                      #       \"strengths\": \"…\",\n",
        "                rf'.*?\"issues\"\\s*:\\s*\"(?P<{agent}_{metric}_issues_3>.*?)\",'\n",
        "                                                      #       \"issues\": \"…\",\n",
        "                rf'.*?\"suggestions\"\\s*:\\s*\"(?P<{agent}_{metric}_suggestions_3>.*?)\"'\n",
        "                                                      #       \"suggestions\": \"…\"\n",
        "                rf'.*?\\}}'                                 #    }\n",
        "            )\n",
        "\n",
        "            m = re.search(pattern, json_blob, flags=re.DOTALL)\n",
        "            if m:\n",
        "                df.at[idx, f\"{agent}_{metric}_score_3\"]       = m.group(f\"{agent}_{metric}_score_3\")\n",
        "                df.at[idx, f\"{agent}_{metric}_strengths_3\"]   = m.group(f\"{agent}_{metric}_strengths_3\")\n",
        "                df.at[idx, f\"{agent}_{metric}_issues_3\"]      = m.group(f\"{agent}_{metric}_issues_3\")\n",
        "                df.at[idx, f\"{agent}_{metric}_suggestions_3\"] = m.group(f\"{agent}_{metric}_suggestions_3\")\n",
        "            # If no match for that metric, leave the _3 columns as None/NaN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "4a58025a-de8f-4bc7-9bf4-c43819fbda59"
      },
      "source": [
        "### measure score (LLM metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "54d0b703-e84d-47ea-a886-b4113637f109",
        "outputId": "b3d6e490-cd31-4423-dd7b-1adfaac5e181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average values for '_score_3' columns (with fallback to '_score_2' then '_score_1'):\n",
            "Extractor_Depth_score_3             7.457711\n",
            "Extractor_Originality_score_3       6.475124\n",
            "Extractor_Actionability_score_3     8.296020\n",
            "Extractor_Topic_Coverage_score_3    7.778607\n",
            "Analyzer_Depth_score_3              7.708904\n",
            "Analyzer_Originality_score_3        7.369863\n",
            "Analyzer_Actionability_score_3      8.684932\n",
            "Analyzer_Topic_Coverage_score_3     7.876712\n",
            "Reviewer_Depth_score_3              7.919643\n",
            "Reviewer_Originality_score_3        6.919643\n",
            "Reviewer_Actionability_score_3      8.763393\n",
            "Reviewer_Topic_Coverage_score_3     7.888393\n",
            "Citation_Depth_score_3              8.556270\n",
            "Citation_Originality_score_3        7.935691\n",
            "Citation_Actionability_score_3      8.382637\n",
            "Citation_Topic_Coverage_score_3     8.790997\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Identify all columns ending with \"_score_3\"\n",
        "score3_cols = [col for col in df.columns if col.endswith(\"_score_3\")]\n",
        "\n",
        "averages_3 = {}\n",
        "\n",
        "for col3 in score3_cols:\n",
        "    # Derive the corresponding \"_score_2\" and \"_score_1\" column names\n",
        "    col2 = col3.replace(\"_score_3\", \"_score_2\")\n",
        "    col1 = col3.replace(\"_score_3\", \"_score_1\")\n",
        "\n",
        "    # 1) Pull \"_score_3\" values, coerce to numeric, replace 0 with NaN\n",
        "    s3 = pd.to_numeric(df[col3], errors=\"coerce\").replace(0, np.nan)\n",
        "\n",
        "    # 2) If \"_score_2\" exists, pull it similarly\n",
        "    if col2 in df.columns:\n",
        "        s2 = pd.to_numeric(df[col2], errors=\"coerce\").replace(0, np.nan)\n",
        "    else:\n",
        "        s2 = pd.Series([np.nan] * len(df), index=df.index)\n",
        "\n",
        "    # 3) If \"_score_1\" exists, pull it similarly\n",
        "    if col1 in df.columns:\n",
        "        s1 = pd.to_numeric(df[col1], errors=\"coerce\").replace(0, np.nan)\n",
        "    else:\n",
        "        s1 = pd.Series([np.nan] * len(df), index=df.index)\n",
        "\n",
        "    # 4) Fill NaNs in s3 from s2, then fill any remaining NaNs from s1\n",
        "    pref = s3.fillna(s2).fillna(s1)\n",
        "\n",
        "    # 5) Compute the mean ignoring NaNs\n",
        "    averages_3[col3] = pref.mean()\n",
        "\n",
        "# Convert to a pandas Series for readability\n",
        "avg_series_3 = pd.Series(averages_3)\n",
        "print(\"Average values for '_score_3' columns (with fallback to '_score_2' then '_score_1'):\")\n",
        "print(avg_series_3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d91db639-f11a-447f-96b8-1d2b923894ee"
      },
      "source": [
        "### master agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56080848-1374-4099-9236-2d465e8001f9"
      },
      "outputs": [],
      "source": [
        "COORDINATOR_PROMPT = '''\n",
        "    You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided by four agents:\n",
        "    1. **Extractor** (explicit limitations from the article),\n",
        "    2. **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. **Reviewer** (limitations from an open review perspective),\n",
        "    4. **Citation** (limitations based on cited papers).\n",
        "\n",
        "    **Goals**:\n",
        "    1. Combine all limitations into a cohesive, non-redundant list.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize author-stated limitations, supplementing with inferred, peer-review, or citation-based limitations if they add value.\n",
        "    4. Resolve discrepancies between agents’ outputs by cross-referencing the article and cited papers, using tools to verify content.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report, with citations for external sources.\n",
        "\n",
        "    **Workflow** (inspired by SYS_PROMPT_SWEBENCH):\n",
        "    1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "    2. **Analyze**: Combine limitations, prioritizing author-stated ones, and verify alignment with the article.\n",
        "    3. **Reflect**: Check for completeness, scientific rigor, and clarity; resolve discrepancies using article content or tools.\n",
        "    4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "    **Output Format**:\n",
        "    - Numbered list of final limitations.\n",
        "    - For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived], [Cited-papers]).\n",
        "    - Include citations for external sources (e.g., web/X posts, cited papers) in the format [Source Name](ID).\n",
        "    **Tool Use**:\n",
        "    - Use text extraction tools to verify article content.\n",
        "    - Use citation lookup tools to cross-reference cited papers.\n",
        "    - Use web/X search tools to resolve discrepancies involving external context.\n",
        "\n",
        "    **Input**: '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4db14df-d709-43a9-87b1-aef55563de40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# taking the self-feedback if it exists otherwise acutal one\n",
        "def master_agent(extractor_text, analyzer_text, reviewer_text, citation_text):\n",
        "    \"\"\"\n",
        "    Takes the outputs of the four specialized agents and produces\n",
        "    the final coordinated limitations via a GPT call.\n",
        "    \"\"\"\n",
        "    coord_prompt = (\n",
        "        COORDINATOR_PROMPT\n",
        "        + f\"**Extractor Agent**:\\n{extractor_text}\\n\\n\"\n",
        "        + f\"**Analyzer Agent**:\\n{analyzer_text}\\n\\n\"\n",
        "        + f\"**Reviewer Agent**:\\n{reviewer_text}\\n\\n\"\n",
        "        + f\"**Citation Agent**:\\n{citation_text}\\n\\n\"\n",
        "        + \"Produce a single, numbered list of final limitations, noting each source in brackets.\"\n",
        "    )\n",
        "    return run_critic(coord_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42984650-76d6-40e1-bdb7-ebb9746c471a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure pandas recognizes NaN\n",
        "import numpy as np\n",
        "\n",
        "# Add a column to store the master agent’s output\n",
        "df[\"final_limitations\"] = \"\"\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # 1) Choose the extractor text: regenerated if available, otherwise original\n",
        "    if pd.notna(row[\"Extractor_regenerated\"]):\n",
        "        extractor_text = row[\"Extractor_regenerated\"]\n",
        "    else:\n",
        "        extractor_text = row[\"extractor_agent\"]\n",
        "\n",
        "    # 2) Analyzer text\n",
        "    if pd.notna(row[\"Analyzer_regenerated\"]):\n",
        "        analyzer_text = row[\"Analyzer_regenerated\"]\n",
        "    else:\n",
        "        analyzer_text = row[\"analyzer_agent\"]\n",
        "\n",
        "    # 3) Reviewer text\n",
        "    if pd.notna(row[\"Reviewer_regenerated\"]):\n",
        "        reviewer_text = row[\"Reviewer_regenerated\"]\n",
        "    else:\n",
        "        reviewer_text = row[\"reviewer_agent\"]\n",
        "\n",
        "    # 4) Citation text\n",
        "    if pd.notna(row[\"Citation_regenerated\"]):\n",
        "        citation_text = row[\"Citation_regenerated\"]\n",
        "    else:\n",
        "        citation_text = row[\"citation_agent\"]\n",
        "\n",
        "    # 5) Call the master_agent with the chosen texts\n",
        "    final = master_agent(\n",
        "        extractor_text=extractor_text,\n",
        "        analyzer_text=analyzer_text,\n",
        "        reviewer_text=reviewer_text,\n",
        "        citation_text=citation_text\n",
        "    )\n",
        "\n",
        "    # 6) Store the result back into df\n",
        "    df.at[idx, \"final_limitations\"] = final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "cd9dc36a-6023-4b5f-91ac-c2985cf1cfad"
      },
      "source": [
        "### Evaluations (measuring Ground Truth coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6c51b5d-0973-443a-a647-e8a15420e2ea"
      },
      "outputs": [],
      "source": [
        "# convert list to string and split\n",
        "def process_single_limitation(limitation_text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text on '**' and return the segments\n",
        "    that occur before each '**'.\n",
        "    \"\"\"\n",
        "    parts = limitation_text.split(\"**\")\n",
        "    # parts at even indices (0,2,4,…) are the “previous” segments\n",
        "    prev_texts = [\n",
        "        part.strip()\n",
        "        for idx, part in enumerate(parts)\n",
        "        if idx % 2 == 0    # even indices\n",
        "           and part.strip()  # non-empty\n",
        "    ]\n",
        "    return prev_texts\n",
        "\n",
        "# Apply to your DataFrame column\n",
        "df_generated_limitations_1[\"final\"] = (\n",
        "    df_generated_limitations_1[\"final\"]\n",
        "    .apply(process_single_limitation)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35012434-1334-42ba-9b8a-d5327136eeff"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "def enumerate_and_filter(cell):\n",
        "    \"\"\"\n",
        "    Given a cell that is either:\n",
        "      - A Python list of strings, or\n",
        "      - A string repr of such a list,\n",
        "    this will:\n",
        "      1. turn it into a list of sublists,\n",
        "      2. remove any sublist equal to ['-'],\n",
        "      3. prefix each remaining sublist's string with its 1-based index,\n",
        "      4. return a new list-of-lists.\n",
        "    \"\"\"\n",
        "    # 1) Parse string repr if necessary\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            lst = ast.literal_eval(cell)\n",
        "        except Exception:\n",
        "            # not a literal list → treat the entire cell as one string\n",
        "            lst = [cell]\n",
        "    else:\n",
        "        lst = cell\n",
        "\n",
        "    # 2) Ensure list-of-lists\n",
        "    lol = []\n",
        "    for item in lst:\n",
        "        if isinstance(item, list):\n",
        "            lol.append(item)\n",
        "        else:\n",
        "            # assume it's a bare string\n",
        "            lol.append([str(item)])\n",
        "\n",
        "    # 3) Filter out ['-'] sublists\n",
        "    filtered = [sub for sub in lol if not (len(sub) == 1 and sub[0].strip() == \"-\")]\n",
        "\n",
        "    # 4) Enumerate: prefix each sublist’s only element with \"i. \"\n",
        "    enumerated = [[f\"{i+1}. {sub[0]}\"] for i, sub in enumerate(filtered)]\n",
        "\n",
        "    return enumerated\n",
        "\n",
        "# Example usage on your DataFrame\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1'].apply(enumerate_and_filter))\n",
        "\n",
        "# Remove the first sublist in each list-of-lists\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1']\n",
        "    .apply(lambda lol: lol[1:] if isinstance(lol, list) and len(lol) > 0 else [])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24832114-7d80-47b8-ad9f-279f59410052"
      },
      "outputs": [],
      "source": [
        "def process_single_limitation(limitation_text):\n",
        "    # Split into different limitations (separated by \\n\\n)\n",
        "    limitations = limitation_text.split('\\n\\n')\n",
        "    processed_limitations = []\n",
        "\n",
        "    for limitation in limitations:\n",
        "        # Remove numbering (e.g., \"1. **Limited Literature Review**\" → \"**Limited Literature Review**\")\n",
        "        cleaned_limitation = limitation.split('. ', 1)[-1] if '. ' in limitation else limitation\n",
        "\n",
        "        # Split into sentences (using '.')\n",
        "        sentences = [s.strip() for s in cleaned_limitation.split('.') if s.strip()]\n",
        "\n",
        "        if sentences:\n",
        "            processed_limitations.append(sentences)\n",
        "\n",
        "    return processed_limitations\n",
        "\n",
        "# df_generated_limitations_2['generated_limitations_1'] = df_generated_limitations_2['generated_limitations_1'].apply(process_single_limitation)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(process_single_limitation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64bd1673-b1ea-4b81-a78d-d49f750a1cae"
      },
      "outputs": [],
      "source": [
        "# add numbering of LLM generated limitations\n",
        "def add_numbering_to_limitations(list_of_lists):\n",
        "    if not isinstance(list_of_lists, list):\n",
        "        return list_of_lists  # Skip if not a list\n",
        "\n",
        "    numbered_list = []\n",
        "    for idx, sublist in enumerate(list_of_lists, start=1):\n",
        "        if sublist:  # Ensure sublist is not empty\n",
        "            # Add numbering to the first element of the sublist\n",
        "            numbered_sublist = [f\"{idx}. {sublist[0]}\"] + sublist[1:]\n",
        "            numbered_list.append(numbered_sublist)\n",
        "    return numbered_list\n",
        "\n",
        "# Apply to the column (modifies existing column)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(add_numbering_to_limitations)\n",
        "# df_generated_limitations_2['generated_limitations_1']  = df_generated_limitations_2['generated_limitations_1'] .apply(add_numbering_to_limitations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aee4413c-ac7d-48af-8c11-17c2f607844e"
      },
      "outputs": [],
      "source": [
        "def remove_empty_limitation_entries(entries):\n",
        "    \"\"\"\n",
        "    Remove sublists where the only element indicates\n",
        "    'does not mention any limitations' (case-insensitive).\n",
        "    \"\"\"\n",
        "    if not isinstance(entries, list):\n",
        "        return entries\n",
        "\n",
        "    filtered = []\n",
        "    target_phrase = \"does not mention any limitations\"\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) == 1:\n",
        "            text = sublist[0].lower()\n",
        "            # skip any sublist whose sole item matches the target phrase\n",
        "            if target_phrase in text:\n",
        "                continue\n",
        "        filtered.append(sublist)\n",
        "    return filtered\n",
        "\n",
        "# Apply to your DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = (\n",
        "    df_generated_limitations_2[\"generated_limitations_1\"]\n",
        "    .apply(remove_empty_limitation_entries)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef895b14-c2d8-4a22-a643-1605ed8347b6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_source_entries(entries):\n",
        "    \"\"\"\n",
        "    Remove sublists where any string in the sublist contains any of the specified keywords\n",
        "    (case-insensitive comparison).\n",
        "    \"\"\"\n",
        "    if not isinstance(entries, list):\n",
        "        return entries  # Return non-list entries as-is\n",
        "\n",
        "    # keywords = ['author-stated', 'inferred', 'peer-review-derived']  # Lowercase for matching\n",
        "\n",
        "    keywords = [\n",
        "        'the authors did not outline any limitations',\n",
        "        'the article does not explicitly',\n",
        "        'the authors do not provide any specific limitations'\n",
        "    ]\n",
        "\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list):\n",
        "            # Convert entire sublist to lowercase string for case-insensitive search\n",
        "            sublist_text = ' '.join(map(str, sublist)).lower()\n",
        "            # Check if NONE of the keywords are in the sublist text\n",
        "            if not any(keyword in sublist_text for keyword in keywords):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list items\n",
        "\n",
        "    return filtered\n",
        "\n",
        "# Apply to DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: remove_source_entries(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a99fdb0-40d3-4c0d-80ea-42be0372d88a"
      },
      "outputs": [],
      "source": [
        "# organize numbers\n",
        "def renumber_limitations(limitations_list):\n",
        "    \"\"\"\n",
        "    Reorganizes numbered limitations to be sequential (1, 2, 3, ...)\n",
        "    while preserving all other content.\n",
        "    \"\"\"\n",
        "    if not isinstance(limitations_list, list):\n",
        "        return limitations_list\n",
        "\n",
        "    renumbered = []\n",
        "    for i, sublist in enumerate(limitations_list, start=1):\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            # Process the first item in the sublist (where the number appears)\n",
        "            first_item = sublist[0]\n",
        "\n",
        "            # Remove existing numbering (e.g., \"2. :\" → \":\")\n",
        "            content = re.sub(r'^\\d+\\.\\s*:\\s*', '', first_item)\n",
        "\n",
        "            # Add new numbering\n",
        "            renumbered_item = f\"{i}. : {content}\"\n",
        "\n",
        "            # Reconstruct the sublist with the renumbered first item\n",
        "            new_sublist = [renumbered_item] + sublist[1:]\n",
        "            renumbered.append(new_sublist)\n",
        "        else:\n",
        "            renumbered.append(sublist)  # Keep non-list or empty entries\n",
        "\n",
        "    return renumbered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: renumber_limitations(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27f34323-0036-443a-a1e6-df9387d6a94c"
      },
      "outputs": [],
      "source": [
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "def remove_here_are_the(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting any sub-list that starts with \"1. Here are the\"\n",
        "    (case-insensitive and whitespace-tolerant).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0].strip().lower()  # Clean whitespace and lowercase\n",
        "            if not first_item.startswith(\"1. here are the\"):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to every row in the DataFrame\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_here_are_the(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2f7b705-a276-4c04-9d9e-a3dfac5355ee"
      },
      "outputs": [],
      "source": [
        "df_lim['combined3'] = [[] for _ in range(len(df_lim))]\n",
        "\n",
        "# Generate combinations for each row (skip indices 153 and 385)\n",
        "for i in range(len(df_lim)):\n",
        "    # if i in {153, 385}:  # Skip these indices\n",
        "    #     continue  # Leaves df_lim['combined3'][i] as an empty list\n",
        "\n",
        "    combined_list = []\n",
        "    list1 = df_lim[\"Lim_and_OR_ground_truth_final\"][i]\n",
        "    list2 = df_generated_limitations_2[\"generated_limitations_1\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df_lim.at[i, 'combined3'] = combined_list[:100]  # Truncate if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "942b4a11-f1bf-4649-97eb-f4cd84913a2e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "start_time = time.time()\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "all_generated_summary = []\n",
        "\n",
        "for i in range(len(df_lim)): # len(df_lim)\n",
        "    # Skip rows 153 and 385\n",
        "    # if i in [153, 385]:\n",
        "    #     all_generated_summary.append([])  # Add empty list for these indices\n",
        "    #     continue\n",
        "\n",
        "    generated_summary = []\n",
        "    for description1, description2 in df_lim['combined3'][i]: # df_lim['combined3']\n",
        "        prompt = '''Check whether 'list2' contains a topic or limitation from 'list1' or 'list1' contains a topic or limitation from 'list2'.\n",
        "        Your answer should be \"Yes\" or \"No\" \\n. List 1:''' + str(description1) + \"List2: \" + str(description2)\n",
        "        summary_text = \"\"\n",
        "        stream = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt,\n",
        "                }\n",
        "            ],\n",
        "            stream=True,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        for chunk in stream:\n",
        "            summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "            summary_text += summary_chunk\n",
        "\n",
        "        summary_chunks = []\n",
        "        summary_chunks.append(summary_text)\n",
        "        generated_summary.append((summary_chunks, \"list1\", description1, \"list2\", description2))\n",
        "\n",
        "    all_generated_summary.append(generated_summary)\n",
        "    time.sleep(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total runtime: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bbd12ba-b59f-45dc-9dec-70f8baba6f9d"
      },
      "outputs": [],
      "source": [
        "# ground truth coverage\n",
        "data = []\n",
        "row_num = 1  # Start row_num from 1, increment for each sublist\n",
        "\n",
        "# Extract data from nested_list2\n",
        "for sublist in all_generated_summary:\n",
        "    for is_match, list1_label, ground_truth, list2_label, llm_generated in sublist:\n",
        "        # Each tup is in the form of (list1, s1, s2, s3, s4)\n",
        "        # Append data to list as a dictionary to maintain column order\n",
        "        data.append({\n",
        "            'row_num': row_num,\n",
        "            'is_match': is_match[0],\n",
        "            'ground_truth': ground_truth,\n",
        "            'llm_generated': llm_generated\n",
        "        })\n",
        "    row_num += 1  # Increment row_num for each new sublist\n",
        "\n",
        "# Create DataFrame from the list of dictionaries\n",
        "df4 = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "# Update the function to handle lists in each row\n",
        "def extract_first_number_from_list(row):\n",
        "    for text in row:  # Iterate through each string in the list\n",
        "        match = re.match(r'^(\\d+)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "    return None  # Return None if no number is found\n",
        "\n",
        "# Apply the updated function to the 'ground_truth' column\n",
        "df4['section'] = df4['ground_truth'].apply(extract_first_number_from_list)\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "ck = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in df4.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['section'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            ck += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['section']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    ck += 1\n",
        "print(ck)\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = df4['ground_truth'].ne(df4['ground_truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "group_counts = df4.groupby(unique_blocks)['ground_truth'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(group_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dfd1bd8-43dc-4f9c-849f-8b1a9f6f0e0d"
      },
      "outputs": [],
      "source": [
        "# LLM generated coverage\n",
        "\n",
        "def extract_first_number(text):\n",
        "    import re\n",
        "    # Check if the input is a list\n",
        "    if isinstance(text, list):\n",
        "        # Join the list elements into a single string\n",
        "        text = \" \".join(text)\n",
        "    # Use regex to extract the first number\n",
        "    match = re.match(r'^(\\d+)', text)\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "# Apply the updated function to extract numbers\n",
        "df4['order'] = df4['llm_generated'].apply(extract_first_number)\n",
        "\n",
        "# Sort the DataFrame by 'row_num' and then by the extracted order\n",
        "df_recall = df4.sort_values(by=['row_num', 'order'])\n",
        "\n",
        "# Reset index for clean indices in the new DataFrame\n",
        "df_recall = df_recall.reset_index(drop=True)\n",
        "\n",
        "# Reorder the columns by placing 'llm_generated' before 'ground_truth'\n",
        "df_recall = df_recall[['row_num', 'is_match', 'llm_generated', 'ground_truth', 'section', 'order']]\n",
        "\n",
        "# Initialize variables\n",
        "current_order = None\n",
        "group_has_yes = False\n",
        "ck = 0  # Count of order groups with at least one 'Yes'\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in df_recall.iterrows():\n",
        "    # Check if we're still in the same order group\n",
        "    if row['order'] == current_order:\n",
        "        # Check if current row has 'Yes'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            group_has_yes = True\n",
        "    else:\n",
        "        # We've entered a new order group\n",
        "        # First check if previous group had any 'Yes'\n",
        "        if group_has_yes:\n",
        "            ck += 1\n",
        "        # Reset for new order group\n",
        "        current_order = row['order']\n",
        "        group_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last group after loop ends\n",
        "if group_has_yes:\n",
        "    ck += 1\n",
        "\n",
        "print(\"Number of order groups with at least one 'Yes':\", ck)\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = df_recall['llm_generated'].ne(df_recall['llm_generated'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "group_counts = df_recall.groupby(unique_blocks)['llm_generated'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(group_counts)"
      ]
    }
  ]
}