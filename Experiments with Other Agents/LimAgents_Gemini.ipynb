{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKG85YJsmJFb"
      },
      "outputs": [],
      "source": [
        "!pip -q install google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero shot limitation generation"
      ],
      "metadata": {
        "id": "bR9-8_IdmcMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['response_string_all'] = df.apply(lambda row: f\"\"\"Abstract: {row['df_Abstract']}\n",
        "Introduction: {row['df_Introduction']}\n",
        "Related Work: {row['df_Related_Work']}\n",
        "Methodology: {row['df_Methodology']}\n",
        "Dataset: {row['df_Dataset']}\n",
        "Conclusion: {row['df_Conclusion']}\n",
        "Experiment and Results: {row['df_Experiment_and_Results']}\n",
        "Other1: {row['Extra']}\n",
        "Other2: {row['Extra_Top']}\n",
        "\"\"\", axis=1)\n",
        "\n",
        "# To run this code you need:\n",
        "#   pip install google-genai pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "df[\"generated_limitations_gemini\"] = ''\n",
        "\n",
        "# Ensure no literal \"nan\" creeps into the prompt\n",
        "df[\"response_string_all\"] = df[\"response_string_all\"].fillna(\"\")\n",
        "df[\"retrieved_text\"]      = df[\"retrieved_text\"].fillna(\"\")\n",
        "\n",
        "# ——— Global list to hold each generated limitation result ———\n",
        "generated_limitations: list[str] = []\n",
        "\n",
        "# ——— Initialize the Gemini client ———\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "if client is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY environment variable not set or invalid.\")\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\"\n",
        "gen_config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "instruction = (\n",
        "    \"You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\\n Generate limitations or shortcomings from the scientific paper.':\\n\\n\"\n",
        ")\n",
        "# instruction  = Extractor\n",
        "\n",
        "total = len(df)\n",
        "print(f\"→ Starting generation for {total} samples using model '{model}'\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Build the LLM input by concatenating the two fields\n",
        "    part1 = str(row[\"response_string_all\"])\n",
        "    part2 = str(row[\"retrieved_text\"])\n",
        "    combined_input = (\n",
        "        part1\n",
        "        + \"\\n\\nThis is the retrieved text from cited papers:\\n\"\n",
        "        + part2\n",
        "    )\n",
        "\n",
        "    # Prepend our instruction\n",
        "    full_prompt = instruction + combined_input\n",
        "    # full_prompt = instruction + part1\n",
        "\n",
        "    # Build the request payload\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=full_prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Send to Gemini (streaming) and collect all text\n",
        "    try:\n",
        "        collected = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=gen_config,\n",
        "        ):\n",
        "            collected += chunk.text\n",
        "\n",
        "        # Even if `collected` is \"\", append it as-is\n",
        "        generated_limitations.append(collected.strip())\n",
        "        print(f\"  ✓ Row {idx+1} done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error on row {idx+1}: {e}\")\n",
        "        generated_limitations.append(f\"ERROR: {e}\")\n",
        "\n",
        "    # Throttle to respect ~15 RPM (i.e. ~4 seconds/request)\n",
        "    if idx < total - 1:\n",
        "        time.sleep(5.0)\n",
        "\n",
        "# After the loop, put everything into a new column\n",
        "df[\"generated_limitations_gemini\"] = generated_limitations\n",
        "\n",
        "print(\"\\n→ All done. Check df['generated_limitations'] for results.\")\n"
      ],
      "metadata": {
        "id": "HxMlmiaCmay8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractor"
      ],
      "metadata": {
        "id": "m01Pn97lmhZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To run this code you need:\n",
        "#   pip install google-genai pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "df[\"generated_limitations_extractor\"] = ''\n",
        "\n",
        "# Ensure no literal \"nan\" creeps into the prompt\n",
        "df[\"response_string_all\"] = df[\"response_string_all\"].fillna(\"\")\n",
        "df[\"retrieved_text\"]      = df[\"retrieved_text\"].fillna(\"\")\n",
        "\n",
        "# ——— Global list to hold each generated limitation result ———\n",
        "generated_limitations_extractor: list[str] = []\n",
        "\n",
        "# ——— Initialize the Gemini client ———\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "if client is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY environment variable not set or invalid.\")\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\"\n",
        "gen_config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "instruction = (\n",
        "    \"Given the following text, identify and list any limitations or drawbacks mentioned or implied:\\n\\n\" + Extractor\n",
        ")\n",
        "# instruction  = Extractor\n",
        "\n",
        "total = len(df)\n",
        "print(f\"→ Starting generation for {total} samples using model '{model}'\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Build the LLM input by concatenating the two fields\n",
        "    part1 = str(row[\"response_string_all\"])\n",
        "\n",
        "    full_prompt = instruction + part1\n",
        "\n",
        "    # Build the request payload\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=full_prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Send to Gemini (streaming) and collect all text\n",
        "    try:\n",
        "        collected = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=gen_config,\n",
        "        ):\n",
        "            collected += chunk.text\n",
        "\n",
        "        # Even if `collected` is \"\", append it as-is\n",
        "        generated_limitations.append(collected.strip())\n",
        "        print(f\"  ✓ Row {idx+1} done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error on row {idx+1}: {e}\")\n",
        "        generated_limitations.append(f\"ERROR: {e}\")\n",
        "\n",
        "    # Throttle to respect ~15 RPM (i.e. ~4 seconds/request)\n",
        "    if idx < total - 1:\n",
        "        time.sleep(4.0)\n",
        "\n",
        "# After the loop, put everything into a new column\n",
        "df[\"generated_limitations_extractor\"][:100] = generated_limitations_extractor\n",
        "\n",
        "print(\"\\n→ All done. Check df['generated_limitations'] for results.\")\n"
      ],
      "metadata": {
        "id": "zSm_yTYemgmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzer Agent"
      ],
      "metadata": {
        "id": "XaOWLcq8mncH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To run this code you need:\n",
        "#   pip install google-genai pandas\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "df[\"generated_limitations_analyzer\"] = ''\n",
        "\n",
        "# Ensure no literal \"nan\" creeps into the prompt\n",
        "df[\"response_string_all\"] = df[\"response_string_all\"].fillna(\"\")\n",
        "\n",
        "# ——— Global list to hold each generated limitation result ———\n",
        "generated_limitations_analyzer: list[str] = []\n",
        "\n",
        "# ——— Initialize the Gemini client ———\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "if client is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY environment variable not set or invalid.\")\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\"\n",
        "gen_config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "instruction = (\n",
        "    \"Given the following text, identify and list any limitations or drawbacks mentioned or implied:\\n\\n\" + Analyzer\n",
        ")\n",
        "# instruction  = Extractor\n",
        "\n",
        "total = len(df)\n",
        "print(f\"→ Starting generation for {total} samples using model '{model}'\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Build the LLM input by concatenating the two fields\n",
        "    part1 = str(row[\"response_string_all\"])\n",
        "    full_prompt = instruction + part1\n",
        "\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=full_prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Send to Gemini (streaming) and collect all text\n",
        "    try:\n",
        "        collected = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=gen_config,\n",
        "        ):\n",
        "            collected += chunk.text\n",
        "\n",
        "        # Even if `collected` is \"\", append it as-is\n",
        "        generated_limitations.append(collected.strip())\n",
        "        print(f\"  ✓ Row {idx+1} done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error on row {idx+1}: {e}\")\n",
        "        generated_limitations.append(f\"ERROR: {e}\")\n",
        "\n",
        "    # Throttle to respect ~15 RPM (i.e. ~4 seconds/request)\n",
        "    if idx < total - 1:\n",
        "        time.sleep(4.0)\n",
        "\n",
        "# After the loop, put everything into a new column\n",
        "df[\"generated_limitations_analyzer\"][:100] = generated_limitations_analyzer\n",
        "\n",
        "print(\"\\n→ All done. Check df['generated_limitations'] for results.\")\n"
      ],
      "metadata": {
        "id": "d0i7GDdZmnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reviewer"
      ],
      "metadata": {
        "id": "LCuHpymxmrqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "df[\"generated_limitations_reviewer\"] = ''\n",
        "\n",
        "df[\"response_string_all\"] = df[\"response_string_all\"].fillna(\"\")\n",
        "\n",
        "generated_limitations_reviewer: list[str] = []\n",
        "\n",
        "# ——— Initialize the Gemini client ———\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "if client is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY environment variable not set or invalid.\")\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\"\n",
        "gen_config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "instruction = (\n",
        "    \"Given the following text, identify and list any limitations or drawbacks mentioned or implied:\\n\\n\" + Reviewer\n",
        ")\n",
        "# instruction  = Extractor\n",
        "\n",
        "total = len(df)\n",
        "print(f\"→ Starting generation for {total} samples using model '{model}'\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Build the LLM input by concatenating the two fields\n",
        "    part1 = str(row[\"response_string_all\"])\n",
        "\n",
        "    full_prompt = instruction + part1\n",
        "\n",
        "\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=full_prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Send to Gemini (streaming) and collect all text\n",
        "    try:\n",
        "        collected = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=gen_config,\n",
        "        ):\n",
        "            collected += chunk.text\n",
        "\n",
        "        # Even if `collected` is \"\", append it as-is\n",
        "        generated_limitations.append(collected.strip())\n",
        "        print(f\"  ✓ Row {idx+1} done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error on row {idx+1}: {e}\")\n",
        "        generated_limitations.append(f\"ERROR: {e}\")\n",
        "\n",
        "    # Throttle to respect ~15 RPM (i.e. ~4 seconds/request)\n",
        "    if idx < total - 1:\n",
        "        time.sleep(4.0)\n",
        "\n",
        "# After the loop, put everything into a new column\n",
        "df[\"generated_limitations_reviewer\"][:100] = generated_limitations_reviewer\n",
        "\n",
        "print(\"\\n→ All done. Check df['generated_limitations'] for results.\")\n"
      ],
      "metadata": {
        "id": "uhAWYGzgmrdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citation"
      ],
      "metadata": {
        "id": "VzgQy3sDmyPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "df[\"generated_limitations_citation\"] = ''\n",
        "\n",
        "df[\"retrieved_text\"]      = df[\"retrieved_text\"].fillna(\"\")\n",
        "\n",
        "# ——— Global list to hold each generated limitation result ———\n",
        "generated_limitations_citation: list[str] = []\n",
        "\n",
        "# ——— Initialize the Gemini client ———\n",
        "client = genai.Client(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
        "if client is None:\n",
        "    raise ValueError(\"GEMINI_API_KEY environment variable not set or invalid.\")\n",
        "\n",
        "model = \"gemini-1.5-flash\"  # or \"gemini-1.5-pro\"\n",
        "gen_config = types.GenerateContentConfig(response_mime_type=\"text/plain\")\n",
        "\n",
        "instruction = (\n",
        "    \"Given the following text, identify and list any limitations or drawbacks mentioned or implied:\\n\\n\" + Citation\n",
        ")\n",
        "# instruction  = Extractor\n",
        "\n",
        "total = len(df)\n",
        "print(f\"→ Starting generation for {total} samples using model '{model}'\")\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    part2 = str(row[\"retrieved_text\"])\n",
        "    combined_input = (\n",
        "        \"\\n\\nThis is the retrieved text from cited papers:\\n\"\n",
        "        + part2\n",
        "    )\n",
        "\n",
        "    # Prepend our instruction\n",
        "    full_prompt = instruction + combined_input\n",
        "    # full_prompt = instruction + part1\n",
        "\n",
        "    contents = [\n",
        "        types.Content(\n",
        "            role=\"user\",\n",
        "            parts=[types.Part.from_text(text=full_prompt)]\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Send to Gemini (streaming) and collect all text\n",
        "    try:\n",
        "        collected = \"\"\n",
        "        for chunk in client.models.generate_content_stream(\n",
        "            model=model,\n",
        "            contents=contents,\n",
        "            config=gen_config,\n",
        "        ):\n",
        "            collected += chunk.text\n",
        "\n",
        "        # Even if `collected` is \"\", append it as-is\n",
        "        generated_limitations.append(collected.strip())\n",
        "        print(f\"  ✓ Row {idx+1} done.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error on row {idx+1}: {e}\")\n",
        "        generated_limitations.append(f\"ERROR: {e}\")\n",
        "\n",
        "    # Throttle to respect ~15 RPM (i.e. ~4 seconds/request)\n",
        "    if idx < total - 1:\n",
        "        time.sleep(4.0)\n",
        "\n",
        "# After the loop, put everything into a new column\n",
        "df[\"generated_limitations_citation\"][:100] = generated_limitations_citation\n",
        "\n",
        "print(\"\\n→ All done. Check df['generated_limitations'] for results.\")\n"
      ],
      "metadata": {
        "id": "kL-wWccnmyCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}