{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "loading and processing data"
      ],
      "metadata": {
        "id": "AA6gq0pTtG0M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhLZb6JSqyf7"
      },
      "outputs": [],
      "source": [
        "# Re-import everything after code execution environment reset\n",
        "import os\n",
        "import json\n",
        "import base64\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "json_dir = \"NeurIPS21_22_Image_data\"\n",
        "image_dir = os.path.join(json_dir, \"NeurIPS21_22\")\n",
        "\n",
        "# Function to encode image to base64\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "# Function to extract figure ID from caption\n",
        "def extract_fig_id_from_caption(caption):\n",
        "    match = re.match(r'^(Figure|Table)\\s+\\d+', caption)\n",
        "    return match.group(0) if match else None\n",
        "\n",
        "# Create new columns\n",
        "df[\"caption\"] = [[] for _ in range(len(df))]\n",
        "df[\"fig_id\"] = [[] for _ in range(len(df))]\n",
        "df[\"image_path\"] = [[] for _ in range(len(df))]\n",
        "df[\"image_base64\"] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Iterate through each row in df\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)): # len(df)\n",
        "    xml_file = row.get(\"xml_file_name\")\n",
        "    if not isinstance(xml_file, str) or not xml_file.endswith(\".tei.xml\"):\n",
        "        print(f\"Row {idx} skipped due to missing or invalid xml_file: {xml_file}\")\n",
        "        continue\n",
        "\n",
        "    file_num = xml_file.replace(\"neurips.\", \"\").replace(\".tei.xml\", \"\")\n",
        "    print('file_num',file_num)\n",
        "    json_path = os.path.join(json_dir, f\"NeurIPS21_22neurips.{file_num}.json\")\n",
        "    print('json_path',json_path)\n",
        "    if not os.path.exists(json_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    for item in data:\n",
        "        caption = item.get(\"caption\", \"\")\n",
        "        fig_id = extract_fig_id_from_caption(caption)\n",
        "        if not fig_id:\n",
        "            continue\n",
        "\n",
        "        image_filename_pattern = f\"fig_neurips.{file_num}-{fig_id.replace(' ', '')}-*.png\"\n",
        "        matching_files = [\n",
        "            f for f in os.listdir(image_dir)\n",
        "            if re.match(image_filename_pattern.replace(\"*\", r\".+\"), f)\n",
        "        ]\n",
        "\n",
        "        if not matching_files:\n",
        "            continue\n",
        "\n",
        "        image_path = os.path.join(image_dir, matching_files[0])\n",
        "        try:\n",
        "            image_b64 = encode_image_to_base64(image_path)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        df.at[idx, \"caption\"].append(caption)\n",
        "        df.at[idx, \"fig_id\"].append(fig_id)\n",
        "        df.at[idx, \"image_path\"].append(image_path)\n",
        "        df.at[idx, \"image_base64\"].append(image_b64)\n",
        "\n",
        "df.to_csv(\"df_neurips_limitation_and_OR_with_cited_data_image_lim.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['response_string_final_neurips'] = df.apply(lambda row: f\"\"\"Abstract: {row['neurips_Abstract']}\n",
        "Introduction: {row['neurips_Introduction']}\n",
        "Related Work: {row['neurips_Related_Work']}\n",
        "Methodology: {row['neurips_Methodology']}\n",
        "Dataset: {row['neurips_Dataset']}\n",
        "Conclusion: {row['neurips_Conclusion']}\n",
        "Experiment and Results: {row['neurips_Experiment_and_Results']}\n",
        "Other1: {row['neurips_Extra']}\n",
        "\"\"\", axis=1)"
      ],
      "metadata": {
        "id": "LaMtVsRLtEDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from openai import AzureOpenAI\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "# â”€â”€â”€ Azure OpenAI client setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://scientific-lim-resource.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\")\n",
        "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
        "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
        "\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        "    api_version=\"2025-01-01-preview\",\n",
        ")\n",
        "\n",
        "\n",
        "# Define figure-specific prompt\n",
        "def get_figure_prompt(caption: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a highly skilled assistant for scientific document analysis. Your task is to identify relevant **figure** descriptions\n",
        "**from the provided paragraph**, given a figure caption.\n",
        "\n",
        "- Do not generate or paraphrase content.\n",
        "- Do not make assumptions.\n",
        "- Only extract the most relevant span or sentence(s) from the paragraph that describe the given figure caption.\n",
        "- If no description is found, say \"None\".\n",
        "\n",
        "Figure Caption: {caption}\n",
        "\n",
        "Paragraph:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "# Define table-specific prompt\n",
        "def get_table_prompt(caption: str, context: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a highly skilled assistant for scientific document analysis. Your task is to identify relevant **table** descriptions\n",
        "**from the provided paragraph**, given a table caption.\n",
        "\n",
        "- Do not generate or paraphrase content.\n",
        "- Do not make assumptions.\n",
        "- Only extract the most relevant span or sentence(s) from the paragraph that describe the given table caption.\n",
        "- If no description is found, say \"None\".\n",
        "\n",
        "Table Caption: {caption}\n",
        "\n",
        "Paragraph:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "# Function to call LLM with the appropriate prompt\n",
        "def find_relevant_figure_description(caption: str, context: str, retries: int = 1):\n",
        "    fig_label_match = re.match(r\"^(Figure|Table)\\s*\\d+\", caption)\n",
        "    fig_label = fig_label_match.group(0) if fig_label_match else \"\"\n",
        "    is_table = \"table\" in fig_label.lower()\n",
        "\n",
        "    prompt = get_table_prompt(caption, context) if is_table else get_figure_prompt(caption, context)\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=messages,\n",
        "                temperature=0\n",
        "            )\n",
        "            return fig_label + \": \" + resp.choices[0].message.content.strip() if fig_label else resp.choices[0].message.content.strip()\n",
        "        except RateLimitError:\n",
        "            if attempt < retries:\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                return \"Error: RateLimit\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "# Process the DataFrame (currently limited to the first row for testing)\n",
        "# df[\"fig_desc\"] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Apply extraction\n",
        "# Ensure the column exists\n",
        "for idx in tqdm(range(120, len(df)), total=len(df) - 120):\n",
        "    # Skip rows that already have filled fig_desc\n",
        "    if isinstance(df.at[idx, \"fig_desc\"], list) and df.at[idx, \"fig_desc\"]:\n",
        "        continue  # already processed\n",
        "    row = df.iloc[idx]\n",
        "    captions = row.get(\"caption\", [])\n",
        "    context = row.get(\"response_string_final_neurips\", \"\")\n",
        "\n",
        "    if isinstance(captions, str):\n",
        "        try:\n",
        "            captions = eval(captions)\n",
        "        except:\n",
        "            captions = []\n",
        "\n",
        "    descriptions = []\n",
        "    for caption in captions:\n",
        "        if caption and isinstance(context, str):\n",
        "            desc = find_relevant_figure_description(caption, context)\n",
        "            descriptions.append(desc)\n",
        "        else:\n",
        "            descriptions.append(\"None\")\n",
        "\n",
        "    df.at[idx, \"fig_desc\"] = descriptions\n",
        "\n",
        "    if idx % 5 == 0:\n",
        "        df.to_csv(\"df_neurips_limitation_and_OR_with_cited_data_image_lim.csv\",index=False)\n",
        "        print(f\"Saved interim CSV at row {idx}\")\n",
        "\n",
        "df.to_csv(\"df_neurips_limitation_and_OR_with_cited_data_image_lim.csv\",index=False)"
      ],
      "metadata": {
        "id": "ki2NiWfytNr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = '''\n",
        "You are a critical assistant specialized in analyzing scientific visualizations.\n",
        "\n",
        "Your task is to examine the **figure (image)**, along with its **caption** and **textual description**, and identify potential **limitations, weaknesses, or concerns** relevant to scientific research quality.\n",
        "\n",
        "Please evaluate the figure based on the following key aspects:\n",
        "\n",
        "1. ðŸ”¬ **Methodological Flaws**:\n",
        "   - Missing baselines or controls?\n",
        "   - Poor experimental design or overfitting?\n",
        "   - Too narrow in scope for generalization?\n",
        "\n",
        "2. ðŸ“‰ **Reproducibility or Generalization Issues**:\n",
        "   - Missing error bars/confidence intervals?\n",
        "   - Non-representative samples or cherry-picking?\n",
        "   - No mention of repeated trials or statistical support?\n",
        "\n",
        "3. ðŸ§  **Interpretability & Visual Clarity**:\n",
        "   - Are axes, labels, legends clear?\n",
        "   - Is the visual layout cluttered or confusing?\n",
        "   - Is the figure colorblind-safe and easy to interpret?\n",
        "\n",
        "4. âš ï¸ **Bias & Misrepresentation**:\n",
        "   - Is the axis scaled to exaggerate results?\n",
        "   - Are negative/failure cases omitted?\n",
        "   - Does the figure mislead through design?\n",
        "\n",
        "5. ðŸ§© **Design & Consistency**:\n",
        "   - Caption and image alignment?\n",
        "   - Consistency with other figures?\n",
        "   - Proper reference to figure in main text?\n",
        "\n",
        "---\n",
        "\n",
        "### Your Input:\n",
        "\n",
        "- **Figure Caption**:\n",
        "{caption}\n",
        "\n",
        "- **Textual Description** (from the main article):\n",
        "{description}\n",
        "\n",
        "- **Attached Image**:\n",
        "(base64 image supplied separately)\n",
        "\n",
        "---\n",
        "\n",
        "### Instructions:\n",
        "\n",
        "Please return concise bullet points** identifying actual limitations of the figure. Be factual and based on the image, caption, and description only. If no issue is detected, respond:\n",
        "**â€œNo major issues detected.â€**\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "yJPMBlQotNl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "# â”€â”€â”€ Azure OpenAI Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "endpoint = os.getenv(\"ENDPOINT_URL\", \"https://scientific-lim-resource.openai.azure.com\")\n",
        "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o\")\n",
        "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        "    api_version=\"2025-01-01-preview\",\n",
        ")\n",
        "\n",
        "# â”€â”€â”€ Run LLM Call â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def azure_run_critic_with_image(prompt: str, image_base64: str, retries: int = 1):\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": prompt},\n",
        "            {\"type\": \"image_url\", \"image_url\": {\n",
        "                \"url\": f\"data:image/png;base64,{image_base64}\"\n",
        "            }},\n",
        "        ]\n",
        "    }]\n",
        "\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=messages,\n",
        "                temperature=0,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0,\n",
        "                stream=False\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except RateLimitError:\n",
        "            if attempt < retries:\n",
        "                print(\"Rate limit hit; sleeping 60s then retryingâ€¦\")\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\"\n",
        "\n",
        "# â”€â”€â”€ Process DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df['image_limitations'] = [[] for _ in range(len(df))]\n",
        "\n",
        "for idx, row in df.iterrows():  # or use df.iloc[:N] for testing subset\n",
        "    print(\"Idx is\",idx)\n",
        "    captions = row.get(\"caption\", [])\n",
        "    image_b64s = row.get(\"image_base64\", [])\n",
        "    descriptions = row.get(\"fig_desc\", [])\n",
        "\n",
        "    limitations = []\n",
        "\n",
        "    for cap, desc, img_b64 in zip(captions, descriptions, image_b64s):\n",
        "        if not (cap and desc and img_b64):\n",
        "            limitations.append(\"None or missing data\")\n",
        "            continue\n",
        "\n",
        "        # Build the prompt using both caption and description\n",
        "        prompt = PROMPT_TEMPLATE.format(caption=cap, description=desc)\n",
        "\n",
        "        try:\n",
        "            output = azure_run_critic_with_image(prompt, img_b64)\n",
        "        except Exception as e:\n",
        "            output = f\"Error: {str(e)}\"\n",
        "\n",
        "        limitations.append(output)\n",
        "\n",
        "    # Update the DataFrame\n",
        "    df.at[idx, 'image_limitations'] = limitations\n",
        "\n",
        "    # Optionally save after every 5 rows\n",
        "    if idx % 5 == 0:\n",
        "        df.to_csv(\"df_with_image_limitations.csv\", index=False)\n",
        "        print(f\"Saved at row {idx}\")\n",
        "\n",
        "# Final save\n",
        "df.to_csv(\"df_with_image_limitations.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "d-RDSDlIteh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_67VK7ptebE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}