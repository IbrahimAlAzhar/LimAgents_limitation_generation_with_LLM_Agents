{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4660e9d8-f256-4849-8954-d4512aed771b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neurips_limitation_and_OR_with_cited_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebbbb464-fc43-4fc8-a233-9397ee6c1fd6"
      },
      "outputs": [],
      "source": [
        "Graph = '''\n",
        "You are an expert in scientific research analysis. I have processed a scientific article using Stanford CoreNLP, resulting in a\n",
        "JSON output containing structured data such as tokenized text, part-of-speech tags, named entity recognition, dependency parses, and optionally sentiment or coreference information. Using this JSON data, identify and generate a concise list of potential limitations of the study described in the article. Consider aspects such as:\n",
        "\n",
        "Methodological limitations (e.g., study design, data collection, or analysis methods).\n",
        "Sample size or population constraints (e.g., small sample, lack of diversity, or selection bias).\n",
        "Generalizability issues (e.g., limited scope of findings or context-specific results).\n",
        "Data quality or measurement issues (e.g., reliance on self-reported data or missing variables).\n",
        "Any other relevant limitations inferred from the text or study description.\n",
        "Please provide the limitations in a clear, bullet-point format, ensuring each limitation is specific, evidence-based, and derived from the content or structure of the provided JSON data. If certain information is ambiguous or missing in the JSON, note any assumptions made. Here is the JSON data for analysis:\n",
        "\n",
        "[Insert JSON data here]\n",
        "\n",
        "Output the limitations in the following format:\n",
        "\n",
        "Limitation 1: [Description of the limitation, with reference to specific elements in the JSON if applicable].\n",
        "Limitation 2: [Description of the limitation, with reference to specific elements in the JSON if applicable].\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b941a1c-63a2-449f-b3f7-2df0d11c1cf1"
      },
      "source": [
        "### LLM agents (lim gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9eca8bd-7e90-4c44-b39a-966fa68c628e"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "system_prompt = '''You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\n",
        " Generate limitations or shortcomings for the following passages from the scientific paper.'''\n",
        "\n",
        "# ─── your existing setup ─────────────────────────────────────────────────────\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# ─── new unified helper ───────────────────────────────────────────────────────\n",
        "def run_critic(prompt: str,*,system_prompt: str | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Wraps ChatOpenAI to (optionally) send a system prompt + your user prompt,\n",
        "    and returns the assistant's reply as a stripped string.\n",
        "    \"\"\"\n",
        "    messages: list[SystemMessage | HumanMessage] = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(SystemMessage(content=system_prompt))\n",
        "\n",
        "    messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "    # this will wait for the full response (no streaming)\n",
        "    response = chat_llm.invoke(messages)\n",
        "    return response.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c8d6454-6194-47c3-ac26-3e9dd3d142af"
      },
      "outputs": [],
      "source": [
        "# # assesment\n",
        "import re\n",
        "import json\n",
        "\n",
        "def llm_assessment(agent_texts: dict,\n",
        "                   agent_prompts: dict,\n",
        "                   metrics=None):\n",
        "    if metrics is None:\n",
        "        metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "\n",
        "    # 1) Fire off the collective judge prompt\n",
        "    raw_response = run_critic(\n",
        "        JUDGE_PROMPT +\n",
        "        \"\".join(f\"**{agent} Agent**:\\n{agent_texts[agent]}\\n\\n\"\n",
        "                for agent in agent_prompts)\n",
        "    )\n",
        "    return raw_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0dda36c0-d22c-47f9-bbbe-f40fe5fc6058"
      },
      "source": [
        "### coreNLP (graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72e421c1-afab-4702-bcde-53957e3686a8"
      },
      "outputs": [],
      "source": [
        "!pip -q install stanza\n",
        "!pip -q install networkx\n",
        "!pip -q install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db2cee86-5b32-4569-8823-794b90dd9b73"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74f7c6ea-5131-443e-8950-727daf7a0b55",
        "outputId": "0103a6ca-e444-4305-a875-908155af1137"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-18 20:49:58 WARNING: Directory /home/ibrahim/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "\n",
        "# This will download the CoreNLP models and start the server.\n",
        "# It might take a few minutes the first time you run it.\n",
        "stanza.install_corenlp()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from stanza.server import CoreNLPClient\n",
        "import stanza\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_graph_from_text(text, client):\n",
        "    \"\"\"Builds a dependency graph for a single sentence using an existing CoreNLPClient.\"\"\"\n",
        "    ann = client.annotate(text)\n",
        "    if not ann.sentence:\n",
        "        return nx.DiGraph()\n",
        "    sentence = ann.sentence[0]\n",
        "    G = nx.DiGraph()\n",
        "    for dep in sentence.enhancedPlusPlusDependencies.edge:\n",
        "        gov = sentence.token[dep.source - 1]\n",
        "        depn = sentence.token[dep.target - 1]\n",
        "        gov_label = f\"{gov.word} ({gov.pos})\"\n",
        "        dep_label = f\"{depn.word} ({depn.pos})\"\n",
        "        G.add_edge(gov_label, dep_label, label=dep.dep)\n",
        "    return G\n",
        "\n",
        "def visualize_graph(graph):\n",
        "    if not graph.nodes():\n",
        "        return\n",
        "    pos = nx.spring_layout(graph, k=1.5, iterations=50)\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    nx.draw(graph, pos, with_labels=True, node_size=3000,\n",
        "            node_color='lightgreen', font_size=10, font_weight='bold',\n",
        "            width=2, edge_color='grey', arrowsize=20)\n",
        "    edge_labels = nx.get_edge_attributes(graph, 'label')\n",
        "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels,\n",
        "                                 font_color='darkred', font_size=9)\n",
        "    plt.title(\"Dependency Graph from CoreNLP\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Download & install the CoreNLP models (only needs to run once ever)\n",
        "    stanza.install_corenlp()\n",
        "\n",
        "    # 2) Spin up a single server\n",
        "    with CoreNLPClient(\n",
        "            annotators=['tokenize', 'ssplit', 'pos', 'lemma', 'depparse'],\n",
        "            memory='4G',\n",
        "            be_quiet=True\n",
        "        ) as client:\n",
        "\n",
        "        df['Graph_Agent_json'] = None\n",
        "\n",
        "        # 3) Loop through your texts, reusing the same client\n",
        "        for idx, text in df['Input_Query_rewrite'].items():\n",
        "            graph = create_graph_from_text(text or \"\", client)\n",
        "            if graph.nodes():\n",
        "                node_link = nx.node_link_data(graph)\n",
        "                df.at[idx, 'Graph_Agent_json'] = json.dumps(node_link, indent=2)\n",
        "            else:\n",
        "                df.at[idx, 'Graph_Agent_json'] = None\n",
        "\n",
        "            print(f\"Processed row {idx}\")\n",
        "            time.sleep(40)\n",
        "\n",
        "        # 4) (Optionally) visualize the first one\n",
        "        first_json = df.at[0, 'Graph_Agent_json']\n",
        "        if first_json:\n",
        "            first_graph = create_graph_from_text(df.at[0, 'Input_Query_rewrite'], client)\n",
        "            visualize_graph(first_graph)\n",
        "\n",
        "    print(\"Finished generating JSON for all rows.\")\n"
      ],
      "metadata": {
        "id": "XtV_p8ACqNHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8b4dbd4-d4d6-45fb-bf50-388cfcacb355"
      },
      "source": [
        "### Limitation generation (graph)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import tiktoken\n",
        "import os\n",
        "import base64\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from openai import AzureOpenAI\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "# ─── token helpers ─────────────────────────────────────────────────────────────\n",
        "\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "endpoint = os.getenv(\"ENDPOINT_URL\", \"\")\n",
        "deployment = os.getenv(\"DEPLOYMENT_NAME\", \"gpt-4o-mini\")\n",
        "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n",
        "\n",
        "# from azure.ai.openai import AzureOpenAI, RateLimitError  # adjust imports if needed\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        "    api_version=\"2025-01-01-preview\",\n",
        ")\n",
        "\n",
        "def azure_run_critic(prompt: str, retries: int = 1):\n",
        "    for attempt in range(retries + 1):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=deployment,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0,\n",
        "                top_p=1,\n",
        "                frequency_penalty=0,\n",
        "                presence_penalty=0,\n",
        "                stream=False\n",
        "            )\n",
        "            return resp.choices[0].message.content.strip()\n",
        "        except RateLimitError:\n",
        "            if attempt < retries:\n",
        "                time.sleep(60)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"\\nProcessing row {i}\")\n",
        "\n",
        "    graph_input  = Graph + df['Graph_Agent_json'][i]\n",
        "    graph_agent = azure_run_critic(graph_input)\n",
        "\n",
        "    df.at[i, \"graph_agent\"]  = graph_agent\n",
        "\n",
        "df.to_csv(\"df_neurips_limitation_and_OR_with_cited_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "21JqTCYVqTPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4a36b0-d371-4c2c-86dd-f924d59d670b"
      },
      "outputs": [],
      "source": [
        "# only from extractor, analyzer, reviewer and graph\n",
        "COORDINATOR_PROMPT = '''\n",
        "    You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided\n",
        "    by four agents:\n",
        "    1. **Extractor** (explicit limitations from the article),\n",
        "    2. **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. **Reviewer** (limitations from an open review perspective),\n",
        "    4. **Graph** (derived limitations from graph),\n",
        "    5. **Citation** (limitations based on cited papers).\n",
        "\n",
        "\n",
        "    **Goals**:\n",
        "    1. Combine all limitations into a cohesive, non-redundant list.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize limitations explicitly mentioned by the authors, supplementing them with inferred or peer-review-based limitations\n",
        "    only if they add value.\n",
        "    4. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report, with citations for\n",
        "    external sources.\n",
        "\n",
        "    **Workflow** (inspired by SYS_PROMPT_SWEBENCH):\n",
        "    1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "    2. **Analyze**: Combine limitations, prioritizing author-stated ones, and verify alignment with the article.\n",
        "    3. **Reflect**: Check for completeness, scientific rigor, and clarity; resolve discrepancies using article content or tools.\n",
        "    4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "    **Output Format**:\n",
        "    - Numbered list of final limitations.\n",
        "    - For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived],\n",
        "    [Cited-papers], [Graph]).\n",
        "    - Include citations for external sources (e.g., web/X posts, cited papers) in the format [Source Name](ID).\n",
        "    **Tool Use**:\n",
        "    - Use text extraction tools to verify article content.\n",
        "    - Use citation lookup tools to cross-reference cited papers.\n",
        "    - Use web/X search tools to resolve discrepancies involving external context.\n",
        "\n",
        " '''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from openai import AzureOpenAI\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "df['master_agent_with_graph'] = ''\n",
        "\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    extractor_text = df1['extractor_agent'][i]\n",
        "    analyzer_text = df1['analyzer_agent'][i]\n",
        "    reviewer_text = df1['reviewer_agent'][i]\n",
        "    graph_text = df['graph_agent'][i]\n",
        "    citation_text = df1['citation_agent'][i]\n",
        "    coord_prompt = (\n",
        "            COORDINATOR_PROMPT\n",
        "            + f\"**Extractor Agent**:\\n{extractor_text}\\n\\n\"\n",
        "            + f\"**Analyzer Agent**:\\n{analyzer_text}\\n\\n\"\n",
        "            + f\"**Reviewer Agent**:\\n{reviewer_text}\\n\\n\"\n",
        "            + f\"**Graph Agent**:\\n{graph_text}\\n\\n\"\n",
        "            + f\"**Citation Agent**:\\n{citation_text}\\n\\n\"\n",
        "            + \"Produce a single, numbered list of final limitations, noting each source in brackets.\"\n",
        "        )\n",
        "    df.at[i, 'master_agent_with_graph'] = run_critic(coord_prompt)\n",
        "\n",
        "df.to_csv(\"df_neurips_limitation_and_OR_with_cited_data.csv\",index=False)"
      ],
      "metadata": {
        "id": "X1d0RCQXqYgH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}