{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7796ee43-e185-4ad5-90d0-3c41b0a76d1c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neruips_21_22_final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d07881c7-e7fd-4855-a3c9-74280a7ede8d"
      },
      "outputs": [],
      "source": [
        "# Define the column names to concatenate\n",
        "cols_to_concat = [\n",
        "    \"neurips_Abstract\",\n",
        "    \"neurips_Introduction\",\n",
        "    \"neurips_Related_Work\",\n",
        "    \"neurips_Methodology\",\n",
        "    \"neurips_Dataset\",\n",
        "    \"neurips_Conclusion\",\n",
        "    \"neurips_Experiment_and_Results\",\n",
        "    \"neurips_Extra\"\n",
        "]\n",
        "\n",
        "# Create a new column 'response_string_neurips' with labeled concatenation\n",
        "def concat_with_labels(row):\n",
        "    parts = []\n",
        "    for col in cols_to_concat:\n",
        "        if isinstance(row.get(col), str) and row[col].strip():\n",
        "            label = col.replace(\"neurips_\", \"\").replace(\"_\", \" \")\n",
        "            parts.append(f\"{label}: {row[col].strip()}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "df[\"response_string_neurips\"] = df.apply(concat_with_labels, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3cb2a11-ab4d-4217-b322-32565c462304"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cded06b1-482f-4dda-9fa6-8a768c453c84"
      },
      "outputs": [],
      "source": [
        "extractor_prompt = '''You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article and\n",
        "extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as the Discussion, Conclusion.\n",
        "List each limitation verbatim, including direct quotes where possible, and provide\n",
        "a brief context (e.g., what aspect of the study the limitation pertains to). Ensure accuracy and avoid inferring or adding\n",
        "limitations not explicitly stated. If no limitations are mentioned, state this clearly. Output your findings in a structured\n",
        "format with bullet points.\\n\\n'''\n",
        "analyzer_prompt = '''You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze the\n",
        "        provided scientific article and identify potential limitations that are not explicitly stated by the authors. Focus on aspects\n",
        "        such as study design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions.\n",
        "        For each inferred limitation, provide a clear explanation of why it is a limitation and how it impacts the study’s validity,\n",
        "        reliability, or generalizability. Ensure your inferences are grounded in the article’s content and avoid speculative assumptions.\n",
        "        Output your findings in a structured format with bullet points, including a brief justification for each limitation.\\n\\n'''\n",
        "\n",
        "reviewer_prompt = '''You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research. Your task\n",
        "        is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "        that might be raised in an open review process, considering common critiques such as reproducibility, transparency,\n",
        "        generalizability, or ethical considerations. If possible, leverage insights from similar studies or common methodological\n",
        "        issues in the field (search the web or X posts if needed for context). For each limitation, explain why it would be a\n",
        "        concern in an open review and how it aligns with peer review standards. Output your findings in a structured format with\n",
        "        bullet points, ensuring each limitation is relevant to the article’s content.:\\n\\n'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b7c2657-d237-4efb-8705-a6171a20bbea"
      },
      "outputs": [],
      "source": [
        "# lim gen\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"\\nProcessing row {i}\")\n",
        "    extractor_input = extractor_prompt + df['response_string_neurips'][i]\n",
        "    analyzer_input = analyzer_prompt + df['response_string_neurips'][i]\n",
        "    reviewer_input = reviewer_prompt + df['response_string_neurips'][i]\n",
        "\n",
        "    extractor_agent = azure_run_critic(extractor_input)\n",
        "    analyzer_agent = azure_run_critic(analyzer_input)\n",
        "    reviewer_agent = azure_run_critic(reviewer_input)\n",
        "\n",
        "    df.at[i, \"extractor_agent\"]  = extractor_agent\n",
        "    df.at[i, \"analyzer_agent\"]  = analyzer_agent\n",
        "    df.at[i, \"reviewer_agent\"]  = reviewer_agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "109e32e5-d5db-4722-99e4-1e826152c39c"
      },
      "source": [
        "relevance score 8 or more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fec82eda-e9c2-4fb2-b1bb-71f5c15a5b32"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['retrieved_text_llm_asses'] = (\n",
        "    df['retrieved_text_llm_asses']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "df['top20_docs'] = (\n",
        "    df['top20_docs']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "\n",
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['top20_docs']\n",
        "    asses = row['retrieved_text_llm_asses']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_in'] = df.apply(pick_high_relevance, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73c1aa9c-d547-4f63-89ac-c63c3cf20e16"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['cited_by_top_20_texts'] = (\n",
        "    df['cited_by_top_20_texts']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "\n",
        "df['retrieved_text_llm_asses_cited_by'] = (\n",
        "    df['retrieved_text_llm_asses_cited_by']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_chunk_dicts(cell):\n",
        "    \"\"\"\n",
        "    cell is expected to be a list of strings, each string containing a\n",
        "    ```json ... ``` block holding a JSON array of chunk‐dicts.\n",
        "    This returns a flat list of all dicts.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for s in cell or []:\n",
        "        # 1) remove the ```json fences\n",
        "        s_clean = re.sub(r'^```json\\s*', '', s.strip())\n",
        "        s_clean = re.sub(r'```$',      '', s_clean.strip())\n",
        "\n",
        "        # 2) parse the JSON\n",
        "        try:\n",
        "            data = json.loads(s_clean)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        # 3) if it’s a list of dicts, extend; otherwise skip\n",
        "        if isinstance(data, list):\n",
        "            out.extend(d for d in data if isinstance(d, dict))\n",
        "    return out\n",
        "\n",
        "# apply to your DataFrame\n",
        "df['retrieved_text_llm_asses_cited_by_upd'] = df['retrieved_text_llm_asses_cited_by'].apply(extract_chunk_dicts)\n",
        "\n",
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['cited_by_top_20_texts']\n",
        "    asses = row['retrieved_text_llm_asses_cited_by_upd']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_by'] = df.apply(pick_high_relevance, axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f47777-b702-489b-801e-61537949f9c0"
      },
      "source": [
        "citation agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec7f5d37-9e9c-4d53-8233-64372c625d81"
      },
      "source": [
        "relevance score 8 or more and input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "546c2304-ea5b-4b58-aede-ccccd6cebbd1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Tokenization setup\n",
        "encoding   = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "max_tokens = 128000\n",
        "\n",
        "def truncate_to_max_tokens(text: str, max_length: int) -> str:\n",
        "    tokens = encoding.encode(text)\n",
        "    return encoding.decode(tokens[:max_length]) if len(tokens) > max_length else text\n",
        "\n",
        "# Make sure the output column exists\n",
        "df['citation_agent_in_by_8'] = ''\n",
        "\n",
        "# Process each row\n",
        "for i in range(5): # len(df)\n",
        "    print(f\"Processing row {i}...\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # 1) Collect all items from relevance_8_cited_in\n",
        "    cited_in_list = row.get('relevance_8_cited_in', []) or []\n",
        "    cited_in_texts = []\n",
        "    for itm in cited_in_list:\n",
        "        # if it's a dict with 'text' key, grab that, otherwise str(itm)\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_in_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_in_texts.append(str(itm))\n",
        "\n",
        "    # 2) Collect all items from relevance_8_cited_by\n",
        "    cited_by_list = row.get('relevance_8_cited_by', []) or []\n",
        "    cited_by_texts = []\n",
        "    for itm in cited_by_list:\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_by_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_by_texts.append(str(itm))\n",
        "\n",
        "    # 3) Build the combined prompt section\n",
        "    cited_in_block = \"\\n\".join(cited_in_texts)\n",
        "    cited_by_block = \"\\n\".join(cited_by_texts)\n",
        "\n",
        "    combined_cited_input = (\n",
        "        \"Referenced papers:\\n\" + cited_in_block +\n",
        "        \"\\n\\nPapers who cited this paper:\\n\" + cited_by_block\n",
        "    )\n",
        "\n",
        "    input_paper = df['response_string_neurips'][i]\n",
        "    # print(\"input paper\",input_paper)\n",
        "    prompt = (\n",
        "        \"You are an assistant tasked to generate limitations or shortcomings \"\n",
        "        \"in a scientific article. Below is the input paper:\\n\"\n",
        "        f\"{input_paper}\\n\\n\"\n",
        "        \" Below is the relevant text from both the papers \"\n",
        "        \"that this article cites and those that cite it.\\n\\n\"\n",
        "        f\"{combined_cited_input}\\n\\n\"\n",
        "        \"Please generate limitations based on this information.\"\n",
        "    )\n",
        "\n",
        "    # 5) Truncate and call LLM\n",
        "    truncated = truncate_to_max_tokens(prompt, max_tokens)\n",
        "    try:\n",
        "        llm_summary = azure_run_critic(truncated)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        llm_summary = \"ERROR\"\n",
        "\n",
        "    df.at[i, \"citation_agent_in_by_8\"] = llm_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "422cade5-7194-4e14-94b3-5e5f22684802"
      },
      "source": [
        "### master agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9112a5a5-c8ce-4d1b-a3cb-7c41396ea44f"
      },
      "outputs": [],
      "source": [
        "COORDINATOR_PROMPT = '''\n",
        "    You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided by four agents:\n",
        "    1. **Extractor** (explicit limitations from the article),\n",
        "    2. **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. **Reviewer** (limitations from an open review perspective),\n",
        "    4. **Citation** (limitations based on cited papers).\n",
        "\n",
        "    **Goals**:\n",
        "    1. Combine all limitations into a cohesive, non-redundant list.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize author-stated limitations, supplementing with inferred, peer-review, or citation-based limitations if they add value.\n",
        "    4. Resolve discrepancies between agents’ outputs by cross-referencing the article and cited papers, using tools to verify content.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report, with citations for external sources.\n",
        "\n",
        "    **Workflow** (inspired by SYS_PROMPT_SWEBENCH):\n",
        "    1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "    2. **Analyze**: Combine limitations, prioritizing author-stated ones, and verify alignment with the article.\n",
        "    3. **Reflect**: Check for completeness, scientific rigor, and clarity; resolve discrepancies using article content or tools.\n",
        "    4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "    **Output Format**:\n",
        "    - Numbered list of final limitations.\n",
        "    - For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived], [Cited-papers]).\n",
        "    - Include citations for external sources (e.g., web/X posts, cited papers) in the format [Source Name](ID).\n",
        "    **Tool Use**:\n",
        "    - Use text extraction tools to verify article content.\n",
        "    - Use citation lookup tools to cross-reference cited papers.\n",
        "    - Use web/X search tools to resolve discrepancies involving external context.\n",
        "\n",
        "    **Input**: '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b4018a6-06cc-4860-ae24-de80de541f9b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# taking the self-feedback if it exists otherwise acutal one\n",
        "# def master_agent(extractor_text, analyzer_text, reviewer_text, citation_text):\n",
        "def master_agent(extractor_text, analyzer_text,reviewer_text,citation_text):\n",
        "    \"\"\"\n",
        "    Takes the outputs of the four specialized agents and produces\n",
        "    the final coordinated limitations via a GPT call.\n",
        "    \"\"\"\n",
        "    coord_prompt = (\n",
        "        COORDINATOR_PROMPT\n",
        "        + f\"**Extractor Agent**:\\n{extractor_text}\\n\\n\"\n",
        "        + f\"**Analyzer Agent**:\\n{analyzer_text}\\n\\n\"\n",
        "        + f\"**Reviewer Agent**:\\n{reviewer_text}\\n\\n\"\n",
        "        + f\"**Citation Agent**:\\n{citation_text}\\n\\n\"\n",
        "        # + f\"**Image Agent**:\\n{image_text}\\n\\n\"\n",
        "        + \"Produce a single, numbered list of final limitations, noting each source in brackets.\"\n",
        "    )\n",
        "    return azure_run_critic(coord_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bccabe7-7690-49fb-8cda-b3dec061782b"
      },
      "outputs": [],
      "source": [
        "# Example: Create a new column to store the output\n",
        "results = []\n",
        "\n",
        "df['master_agent_ext_analy_rev'] = ''\n",
        "\n",
        "for i in range(5): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    extractor_text = df.at[i, 'extractor_agent']\n",
        "    analyzer_text  = df.at[i, 'analyzer_agent']\n",
        "    reviewer_text  = df.at[i, 'reviewer_agent']\n",
        "    citation_text  = df.at[i, 'citation_agent_in_by_8']\n",
        "    # image_text     = df_image.at[i, 'image_limitations']\n",
        "\n",
        "    try:\n",
        "        result = master_agent(extractor_text, analyzer_text, reviewer_text, citation_text)\n",
        "        # result = master_agent(extractor_text, analyzer_text,reviewer_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        result = \"ERROR\"\n",
        "\n",
        "    df.at[i,'master_agent_ext_analy_rev'] = result\n",
        "    results.append(result)\n",
        "\n",
        "# Add results back to df\n"
      ]
    }
  ]
}