### LimAgents: Can LLM Agents Generate Meaningful Limitations from Scientific Articles?

- Scientific Article Limitation Generation with LLM Agents

- This repository contains the code, prompts, datasets, and evaluation scripts for our paper:
ðŸ“„ "LimAgents: Can LLM Agents Generate Meaningful Limitations from Scientific Articles?"

Our work investigates whether LLM agentsâ€”specialized roles such as Extractor, Analyzer, Reviewer, and Citationâ€”can generate more meaningful, contextually grounded, and peer-reviewâ€“ready limitations than traditional zero-shot LLMs. 

## ðŸ“‚ Repository Structure

```bash

â”œâ”€â”€ ablation_study/
â”‚   â”œâ”€â”€ LimAgents_Llama_9_Agents.ipynb 
â”‚   â”œâ”€â”€ LimAgents_ablation_study_3_Agent_GPT4o_mini.ipynb 
â”‚   â”œâ”€â”€ LimAgents_ablation_study_evulation_each_agents.ipynb 
â”‚   â””â”€â”€ LimAgents_ablation_study_with_various_agents.ipynb
â”‚
â”œâ”€â”€ citation_agent/
â”‚   â”œâ”€â”€ LimAgents_Citation_Agent.ipynb 
â”‚   â”œâ”€â”€ LimAgents_LLM_Reranker_in_RAG_Retriever.ipynb
â”‚   â”œâ”€â”€ LimAgents_citation_agent_RAG_settings.ipynb
â”‚   â””â”€â”€ LimAgents_cited_by_data_collection.ipynb
â”‚
â”œâ”€â”€ end_to_end_llm_agents/
â”‚   â”œâ”€â”€ LimAgents_4_Agents_GPT_4o_mini_(end_to_end).ipynb
â”‚   â”œâ”€â”€ LimAgents_Llama_3_Agents_(end_to_end).ipynb
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ LimAgents_Evaluation.ipynb.ipynb
â”‚
â”œâ”€â”€ experiment_with_other_agents/
â”‚   â”œâ”€â”€ LimAgents_Gemini.ipynb
â”‚   â”œâ”€â”€ LimAgents_ablation_study_Graph_Agent.ipynb
â”‚   â””â”€â”€ LimAgents_image_agent.ipynb
â”‚
â”œâ”€â”€ ground_truth_extraction/
â”‚   â”œâ”€â”€ LimAgents_ground_truth_extraction_by_LLM.ipynb
â”‚
â”œâ”€â”€ judge_and_self_feedback/
â”‚   â”œâ”€â”€ LimAgents_Judge_and_Self_Feedback.ipynb
â”‚
â”œâ”€â”€ llm_agents_gpt4o_mini/
â”‚   â”œâ”€â”€ LimAgents_4_Agents_GPT_4o_mini.ipynb
â”‚
â”œâ”€â”€ limagents_prompt/
â”‚   â”œâ”€â”€ LimAgents_Prompt.ipynb
â”‚
â”œâ”€â”€ zero_shot_setting/
â”‚   â”œâ”€â”€ LimAgents_zero_shot.ipynb
â”‚
â””â”€â”€ README.md

```

### 1. Ablation Study

Experiments testing different agent configurations:

9-agent full system

3-agent minimal setup (Extractor, Analyzer, Reviewer) with GPT-4o mini

Individual agent runs and evaluations

Comparative results showing the effect of agent decomposition

### 2. Citation Agent

Implements and evaluates the Citation Agent, grounding limitation generation in broader scholarly context:

Retrieval-augmented generation (RAG) settings

Cited-in and cited-by data collection

LLM re-ranking for retrieved passages

### 3. End-to-End LLM Agents

Runs the full LimAgents pipeline:

Tested with GPT-4o mini and LLaMA-3 (8B)

Generates limitations using the optimal agent setup

Includes evaluation scripts for end-to-end assessment

### 4. Evaluation

Code for evaluating generated limitations:

Coverage: how many ground-truth limitations are matched

Quality: LLM-based pairwise matching of generated vs. ground-truth limitations

Supports both coverage metrics and LLM-as-a-judge evaluation

### 5. Experiment with Other Agents

Exploratory experiments with non-standard agents:

Gemini Agent (multi-modal reasoning)

Graph Agent (graph-based representations)

Image Agent (handling visual content in papers)

### 6. Ground Truth Extraction

Scripts for building high-quality ground truth datasets:

Extract author-stated limitations

Collect peer-review limitations from OpenReview

Merge both sources into a richer gold-standard set

### 7. Judge and Self-Feedback

Implements the Judge Agent and self-refinement loops:

Judge Agent scores limitations (depth, originality, actionability, coverage)

Underperforming outputs trigger re-generation via self-feedback

### 8. LLM Agents â€“ GPT-4o Mini

Experiments using GPT-4o mini with the optimal 4-agent setup (Extractor, Analyzer, Reviewer, Citation):

Demonstrates effectiveness of multi-agent orchestration

Provides baseline vs. LLaMA and zero-shot settings

### 9. LimAgents Prompt

Collection of prompts used in experiments:

Worker agents (Extractor, Analyzer, Reviewer, Citation)

Judge Agent and Master Agent

Chain-of-limitations step-by-step workflow

### 10. Zero-Shot Setting

Baselines with direct prompting (no agents):

Shows shortcomings of zero-shot methods (vague, generic outputs)

Provides baseline for comparison against LimAgents

### ðŸ”‘ Key Contributions in Code

Multi-agent orchestration: Explicit roles for Extractor, Analyzer, Reviewer, Citation

Retrieval integration (RAG): Citation Agent retrieves cited & citing papers

Judge + Self-feedback: Iterative refinement for higher-quality limitations

Pointwise evaluation protocol: LLM-as-a-judge coverage assessment beyond n-gram metrics

Ground-truth construction: Combining author-stated + OpenReview limitations for benchmarking
