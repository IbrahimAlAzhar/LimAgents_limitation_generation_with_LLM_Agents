{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e5be7c-8a1d-456b-8670-043161049358"
      },
      "source": [
        "### LLM agents to generate limitations (Agents: Extractor, Analyzer, Reviewer, Synthesizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e7f586f4-f366-4329-a26e-70232b6994cb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Define which sections to pull in for every row\n",
        "SECTIONS = [\"df_Abstract\", \"df_Introduction\", \"df_Related_Work\",\n",
        "            \"df_Methodology\", \"df_Dataset\", \"df_Experiment_and_Results\", \"df_Conclusion\"]\n",
        "\n",
        "# Define a helper that streams one prompt and returns the full text\n",
        "def run_critic(prompt: str) -> str:\n",
        "    summary = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary += chunk.choices[0].delta.content or \"\"\n",
        "    return summary.strip()\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for idx in range(len(df_lim)):\n",
        "    # build a list of “Section:\\ncontent” strings\n",
        "    pieces = []\n",
        "    for sec in SECTIONS:\n",
        "        col = sec.replace(\" \", \"_\")    # e.g. \"Related Work\" → \"Related_Work\"\n",
        "        if pd.notna(df_lim.loc[idx, col]):\n",
        "            pieces.append(f\"{sec}:\\n{df_lim.loc[idx, col]}\")\n",
        "\n",
        "    text_blob = \"\\n\\n\".join(pieces)\n",
        "\n",
        "    extractor_agent = run_critic(\n",
        "        '''You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article and\n",
        "        extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as the Discussion, Conclusion.\n",
        "        List each limitation verbatim, including direct quotes where possible, and provide\n",
        "        a brief context (e.g., what aspect of the study the limitation pertains to). Ensure accuracy and avoid inferring or adding\n",
        "        limitations not explicitly stated. If no limitations are mentioned, state this clearly. Output your findings in a structured\n",
        "        format with bullet points.\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    analyzer_agent = run_critic(\n",
        "        '''You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze the\n",
        "        provided scientific article and identify potential limitations that are not explicitly stated by the authors. Focus on aspects\n",
        "        such as study design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions.\n",
        "        For each inferred limitation, provide a clear explanation of why it is a limitation and how it impacts the study’s validity,\n",
        "        reliability, or generalizability. Ensure your inferences are grounded in the article’s content and avoid speculative assumptions.\n",
        "        Output your findings in a structured format with bullet points, including a brief justification for each limitation.\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    reviewer_agent = run_critic(\n",
        "        '''You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research. Your task\n",
        "        is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "        that might be raised in an open review process, considering common critiques such as reproducibility, transparency,\n",
        "        generalizability, or ethical considerations. If possible, leverage insights from similar studies or common methodological\n",
        "        issues in the field (search the web or X posts if needed for context). For each limitation, explain why it would be a\n",
        "        concern in an open review and how it aligns with peer review standards. Output your findings in a structured format with\n",
        "        bullet points, ensuring each limitation is relevant to the article’s content.:\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "\n",
        "    # 3) master coordinator to fuse them\n",
        "    coord_prompt = (\n",
        "    '''You are a **Master Coordinator**. You are an expert in scientific communication and synthesis.\n",
        "    Your task is to integrate the limitations provided by three other agents:\n",
        "    1. The **Extractor** (explicit limitations from the article),\n",
        "    2. The **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. The **Reviewer** (limitations from an open review perspective).\n",
        "\n",
        "    Your goals are to:\n",
        "    1. Combine all limitations into a cohesive list, removing redundancies.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize limitations explicitly mentioned by the authors, supplementing them with inferred or peer-review-based limitations only if they add value.\n",
        "    4. Highlight discrepancies between the agents’ outputs and resolve them by cross-referencing the article.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for inclusion in a scientific review or report.\n",
        "\n",
        "    Provide a brief justification for each limitation, noting its source:\n",
        "    - \"Author-stated\" (from Extractor),\n",
        "    - \"Inferred\" (from Analyzer),\n",
        "    - \"Peer-review-derived\" (from Reviewer).\n",
        "\n",
        "    You will be given three lists of sub-limitations:\\n\\n'''\n",
        "    f\"**Extractor Agent**:\\n{extractor_agent}\\n\\n\"\n",
        "    f\"**Analyzer Agent**:\\n{analyzer_agent}\\n\\n\"\n",
        "    f\"**Reviewer Agent**:\\n{reviewer_agent}\\n\\n\"\n",
        "    )\n",
        "    final_lim = run_critic(coord_prompt)\n",
        "\n",
        "    generated_limitations.append({\n",
        "        # \"Experiment\": experiments_lim,\n",
        "        # \"Clarity\": clarity_lim,\n",
        "        # \"Impact\": impact_lim,\n",
        "        \"final\": final_lim\n",
        "    })\n",
        "    # time.sleep(1)  # to avoid hitting rate limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f108a283-801c-449b-aa99-8d73b6fbe1b6"
      },
      "outputs": [],
      "source": [
        "# Extract the 'final' value from each dict and build a DataFrame\n",
        "final_values = [d.get(\"final\", \"\") for d in generated_limitations]\n",
        "df_generated_limitations_2 = pd.DataFrame(final_values, columns=[\"generated_limitations_1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "376e3bb3-c3c0-4669-9b09-a91c580aa696"
      },
      "outputs": [],
      "source": [
        "# convert list to string and split\n",
        "def process_single_limitation(limitation_text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text on '**' and return the segments\n",
        "    that occur before each '**'.\n",
        "    \"\"\"\n",
        "    parts = limitation_text.split(\"**\")\n",
        "    # parts at even indices (0,2,4,…) are the “previous” segments\n",
        "    prev_texts = [\n",
        "        part.strip()\n",
        "        for idx, part in enumerate(parts)\n",
        "        if idx % 2 == 0    # even indices\n",
        "           and part.strip()  # non-empty\n",
        "    ]\n",
        "    return prev_texts\n",
        "\n",
        "# Apply to your DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = (\n",
        "    df_generated_limitations_2[\"generated_limitations_1\"]\n",
        "    .apply(process_single_limitation)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "540f7e5f-1aef-4508-adf2-8e949d3a491f"
      },
      "outputs": [],
      "source": [
        "# convert string to list\n",
        "import ast\n",
        "\n",
        "# This will parse the string \"[...]\" into a real list object\n",
        "df_generated_limitations_2['generated_limitations_1'] = (\n",
        "    df_generated_limitations_2['generated_limitations_1']\n",
        "      .astype(str)               # ensure it’s a string\n",
        "      .apply(ast.literal_eval)   # safely evaluate Python literal\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac51da97-4343-4226-a5b2-13875119b3bf"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "def enumerate_and_filter(cell):\n",
        "    \"\"\"\n",
        "    Given a cell that is either:\n",
        "      - A Python list of strings, or\n",
        "      - A string repr of such a list,\n",
        "    this will:\n",
        "      1. turn it into a list of sublists,\n",
        "      2. remove any sublist equal to ['-'],\n",
        "      3. prefix each remaining sublist's string with its 1-based index,\n",
        "      4. return a new list-of-lists.\n",
        "    \"\"\"\n",
        "    # 1) Parse string repr if necessary\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            lst = ast.literal_eval(cell)\n",
        "        except Exception:\n",
        "            # not a literal list → treat the entire cell as one string\n",
        "            lst = [cell]\n",
        "    else:\n",
        "        lst = cell\n",
        "\n",
        "    # 2) Ensure list-of-lists\n",
        "    lol = []\n",
        "    for item in lst:\n",
        "        if isinstance(item, list):\n",
        "            lol.append(item)\n",
        "        else:\n",
        "            # assume it's a bare string\n",
        "            lol.append([str(item)])\n",
        "\n",
        "    # 3) Filter out ['-'] sublists\n",
        "    filtered = [sub for sub in lol if not (len(sub) == 1 and sub[0].strip() == \"-\")]\n",
        "\n",
        "    # 4) Enumerate: prefix each sublist’s only element with \"i. \"\n",
        "    enumerated = [[f\"{i+1}. {sub[0]}\"] for i, sub in enumerate(filtered)]\n",
        "\n",
        "    return enumerated\n",
        "\n",
        "# Example usage on your DataFrame\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1'].apply(enumerate_and_filter))\n",
        "\n",
        "# Remove the first sublist in each list-of-lists\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1']\n",
        "    .apply(lambda lol: lol[1:] if isinstance(lol, list) and len(lol) > 0 else [])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee611cff-39bd-42fd-800f-6374806bb4bc"
      },
      "outputs": [],
      "source": [
        "def process_single_limitation(limitation_text):\n",
        "    # Split into different limitations (separated by \\n\\n)\n",
        "    limitations = limitation_text.split('\\n\\n')\n",
        "    processed_limitations = []\n",
        "\n",
        "    for limitation in limitations:\n",
        "        # Remove numbering (e.g., \"1. **Limited Literature Review**\" → \"**Limited Literature Review**\")\n",
        "        cleaned_limitation = limitation.split('. ', 1)[-1] if '. ' in limitation else limitation\n",
        "\n",
        "        # Split into sentences (using '.')\n",
        "        sentences = [s.strip() for s in cleaned_limitation.split('.') if s.strip()]\n",
        "\n",
        "        if sentences:\n",
        "            processed_limitations.append(sentences)\n",
        "\n",
        "    return processed_limitations\n",
        "\n",
        "# df_generated_limitations_2['generated_limitations_1'] = df_generated_limitations_2['generated_limitations_1'].apply(process_single_limitation)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(process_single_limitation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4ad0b49b-0a18-4345-8e84-5d35bd6c0083"
      },
      "outputs": [],
      "source": [
        "# add numbering of LLM generated limitations\n",
        "def add_numbering_to_limitations(list_of_lists):\n",
        "    if not isinstance(list_of_lists, list):\n",
        "        return list_of_lists  # Skip if not a list\n",
        "\n",
        "    numbered_list = []\n",
        "    for idx, sublist in enumerate(list_of_lists, start=1):\n",
        "        if sublist:  # Ensure sublist is not empty\n",
        "            # Add numbering to the first element of the sublist\n",
        "            numbered_sublist = [f\"{idx}. {sublist[0]}\"] + sublist[1:]\n",
        "            numbered_list.append(numbered_sublist)\n",
        "    return numbered_list\n",
        "\n",
        "# Apply to the column (modifies existing column)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(add_numbering_to_limitations)\n",
        "# df_generated_limitations_2['generated_limitations_1']  = df_generated_limitations_2['generated_limitations_1'] .apply(add_numbering_to_limitations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0f19c5b-2084-4ae1-9ed6-d08468a45031"
      },
      "outputs": [],
      "source": [
        "# organize numbers\n",
        "def renumber_limitations(limitations_list):\n",
        "    \"\"\"\n",
        "    Reorganizes numbered limitations to be sequential (1, 2, 3, ...)\n",
        "    while preserving all other content.\n",
        "    \"\"\"\n",
        "    if not isinstance(limitations_list, list):\n",
        "        return limitations_list\n",
        "\n",
        "    renumbered = []\n",
        "    for i, sublist in enumerate(limitations_list, start=1):\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            # Process the first item in the sublist (where the number appears)\n",
        "            first_item = sublist[0]\n",
        "\n",
        "            # Remove existing numbering (e.g., \"2. :\" → \":\")\n",
        "            content = re.sub(r'^\\d+\\.\\s*:\\s*', '', first_item)\n",
        "\n",
        "            # Add new numbering\n",
        "            renumbered_item = f\"{i}. : {content}\"\n",
        "\n",
        "            # Reconstruct the sublist with the renumbered first item\n",
        "            new_sublist = [renumbered_item] + sublist[1:]\n",
        "            renumbered.append(new_sublist)\n",
        "        else:\n",
        "            renumbered.append(sublist)  # Keep non-list or empty entries\n",
        "\n",
        "    return renumbered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: renumber_limitations(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a971b4fe-4a82-48f9-92f8-a82be04d5973"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_source_entries(entries):\n",
        "    \"\"\"\n",
        "    Remove sublists where any string in the sublist contains\n",
        "    'author-stated', 'inferred', or 'peer-review-derived' (case-insensitive).\n",
        "    \"\"\"\n",
        "    if not isinstance(entries, list):\n",
        "        return entries  # Return non-list entries as-is\n",
        "\n",
        "    filtered = []\n",
        "    keywords = ['author-stated', 'inferred', 'peer-review-derived']  # Lowercase for matching\n",
        "\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list):\n",
        "            # Convert entire sublist to lowercase string for keyword search\n",
        "            sublist_text = ' '.join(map(str, sublist)).lower()\n",
        "            # Check if ANY keyword exists in the sublist text\n",
        "            if not any(keyword in sublist_text for keyword in keywords):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list items\n",
        "\n",
        "    return filtered\n",
        "\n",
        "# Apply to DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: remove_source_entries(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "76198a81-f296-433d-a785-225aa8558bb8"
      },
      "outputs": [],
      "source": [
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "\n",
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "def remove_here_are_the(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting any sub-list that starts with \"1. Here are the\"\n",
        "    (case-insensitive and whitespace-tolerant).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0].strip().lower()  # Clean whitespace and lowercase\n",
        "            if not first_item.startswith(\"1. here are the\"):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to every row in the DataFrame\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_here_are_the(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "187559bb-3769-4504-a97d-5c9b1a80efb2"
      },
      "outputs": [],
      "source": [
        "# Initialize the 'combined' column with empty lists\n",
        "df_lim['combined3'] = [[] for _ in range(len(df_lim))]  # Match the DataFrame's length\n",
        "\n",
        "# Generate combinations for each row (up to 100 pairs per row)\n",
        "for i in range(len(df_lim)):\n",
        "    combined_list = []\n",
        "    list1 = df_lim[\"Lim_and_OR_ground_truth_final\"][i]\n",
        "    list2 = df_generated_limitations_2[\"generated_limitations_1\"][i]\n",
        "\n",
        "    # Generate all possible combinations (item1 from list1, item2 from list2)\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store only the first 100 combinations (or all if fewer than 100)\n",
        "    df_lim.at[i, 'combined3'] = combined_list  # Truncate to 100 max"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# at first, generate summary (index 0 to 412) (done)\n",
        "import time\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "start_time = time.time()\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "all_generated_summary = []\n",
        "\n",
        "for i in range(len(df_lim)): # len(df_lim)\n",
        "    generated_summary = []\n",
        "    for description1, description2 in df_lim['combined3'][i]:\n",
        "      prompt = '''Check whether 'list2' contains a topic or limitation from 'list1' or 'list1' contains a topic or limitation from 'list2'.\n",
        "      Your answer should be \"Yes\" or \"No\" \\n. List 1:''' + str(description1) + \"List2: \" + str(description2)\n",
        "      summary_text = \"\"  # Initialize an empty string to collect the limitation text\n",
        "      stream = client.chat.completions.create(\n",
        "          # model=\"gpt-3.5-turbo\",\n",
        "          model=\"gpt-4o-mini\",\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": prompt,\n",
        "              }\n",
        "          ],\n",
        "          stream=True,\n",
        "          temperature=0  # Adjust the temperature as needed, max_tokens=150\n",
        "      )\n",
        "\n",
        "      for chunk in stream:\n",
        "          summary_chunk = chunk.choices[0].delta.content or \"\"\n",
        "          # print(limitation_chunk, end=\"\")\n",
        "          summary_text += summary_chunk  # Append each chunk to the limitation_text\n",
        "\n",
        "      # print(\"\\n\")  # Print a newline for readability\n",
        "      summary_chunks = []\n",
        "      summary_chunks.append(summary_text)\n",
        "\n",
        "      generated_summary.append((summary_chunks, \"list1\", description1, \"list2\", description2))\n",
        "    all_generated_summary.append(generated_summary)\n",
        "    # time.sleep(1)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total runtime: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "HOPRHFVt5osV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b33e9d0d-6e7a-40e2-83f3-9935aca5a6b9"
      },
      "outputs": [],
      "source": [
        "# ground truth coverage\n",
        "data = []\n",
        "row_num = 1  # Start row_num from 1, increment for each sublist\n",
        "\n",
        "# Extract data from nested_list2\n",
        "for sublist in all_generated_summary:\n",
        "    for is_match, list1_label, ground_truth, list2_label, llm_generated in sublist:\n",
        "        # Each tup is in the form of (list1, s1, s2, s3, s4)\n",
        "        # Append data to list as a dictionary to maintain column order\n",
        "        data.append({\n",
        "            'row_num': row_num,\n",
        "            'is_match': is_match[0],\n",
        "            'ground_truth': ground_truth,\n",
        "            'llm_generated': llm_generated\n",
        "        })\n",
        "    row_num += 1  # Increment row_num for each new sublist\n",
        "\n",
        "# Create DataFrame from the list of dictionaries\n",
        "df4 = pd.DataFrame(data)\n",
        "df4.to_csv(\"df_neurips_llm_agents.csv\",index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fb424a3-70f0-428d-b9e3-65de97f3095f",
        "outputId": "83409c7e-205f-4b6f-bad8-ed2a44f8ad6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3195\n",
            "Number of unique consecutive 'ground_truth' texts and their counts:\n",
            "              count\n",
            "ground_truth       \n",
            "1                17\n",
            "2                17\n",
            "3                17\n",
            "4                17\n",
            "5                17\n",
            "...             ...\n",
            "5518             20\n",
            "5519             20\n",
            "5520             20\n",
            "5521             20\n",
            "5522             20\n",
            "\n",
            "[5522 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Update the function to handle lists in each row\n",
        "def extract_first_number_from_list(row):\n",
        "    for text in row:  # Iterate through each string in the list\n",
        "        match = re.match(r'^(\\d+)', text)\n",
        "        if match:\n",
        "            return int(match.group(1))\n",
        "    return None  # Return None if no number is found\n",
        "\n",
        "# Apply the updated function to the 'ground_truth' column\n",
        "df4['section'] = df4['ground_truth'].apply(extract_first_number_from_list)\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "ck = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in df4.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['section'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            # print(\"row_num\",row[\"row_num\"])\n",
        "            # print(\"is_match\", row[\"is_match\"])\n",
        "            # print(\"section\", row['section'])\n",
        "            # print(\"order\", row['order'])\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            ck += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['section']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    ck += 1\n",
        "print(ck)\n",
        "\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = df4['ground_truth'].ne(df4['ground_truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "group_counts = df4.groupby(unique_blocks)['ground_truth'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(group_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "175e9f98-5c70-4f95-a4b6-3f219d21edbf",
        "outputId": "13a7d446-9537-4437-ac5c-f1f295e9de9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5785947120608476"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "3195/5522"
      ]
    }
  ]
}