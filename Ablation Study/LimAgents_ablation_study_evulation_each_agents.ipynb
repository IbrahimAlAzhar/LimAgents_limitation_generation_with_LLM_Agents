{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "178117b2-5fa7-4edd-a9f8-3d2338b0e2d1"
      },
      "source": [
        "### Evaluation of Extractor Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aafd3c5-c1c4-41f8-ba75-2e82a49c817e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Make sure the output column exists\n",
        "df['extractor_agent_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"extractor_agent\"]\n",
        "\n",
        "    # Skip if missing or not a string\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'extractor_agent_list'] = []\n",
        "        continue\n",
        "\n",
        "    # Split based on bullet points (each limitation starts with \"- **\")\n",
        "    parts = re.split(r'\\n\\s*-\\s+\\*\\*', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        part = part.strip()\n",
        "        if not part:\n",
        "            continue\n",
        "\n",
        "        # Add back the initial \"**\" since we removed it during split\n",
        "        part = \"**\" + part\n",
        "\n",
        "        # Extract title\n",
        "        title_match = re.search(r'\\*\\*(.*?)\\*\\*:', part)\n",
        "        title = title_match.group(1).strip() if title_match else \"Unknown Title\"\n",
        "\n",
        "        # Extract quote\n",
        "        quote_match = re.search(r'\\*\\*Quote\\*\\*:\\s*\"([^\"]+)\"', part)\n",
        "        quote = quote_match.group(1).strip() if quote_match else \"No quote found\"\n",
        "\n",
        "        # Extract context\n",
        "        context_match = re.search(r'\\*\\*Context\\*\\*:\\s*(.*?)(?:\\n|$)', part, re.S)\n",
        "        context = context_match.group(1).strip() if context_match else \"No context found\"\n",
        "\n",
        "        lim_list.append([title, quote, context])\n",
        "\n",
        "    df.at[row_idx, 'extractor_agent_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01b56f70-035b-44e3-88ed-1a00fa78114c"
      },
      "outputs": [],
      "source": [
        "for i in range(len(df)):\n",
        "    sublists = df.at[i, 'extractor_agent_list']\n",
        "\n",
        "    if isinstance(sublists, list) and len(sublists) > 0:\n",
        "        df.at[i, 'extractor_agent_list'] = sublists[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42e3b7e-a7a4-4561-b45c-d4c2247f9f47"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# ensure the output column exists\n",
        "df['Ground_Truth_Lim_OPR_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"Ground_Truth_Lim_OPR\"]\n",
        "\n",
        "    # skip non-strings\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'Ground_Truth_Lim_OPR_list'] = []\n",
        "        continue\n",
        "\n",
        "    # Remove \"Merged Limitations:\" header if present\n",
        "    raw = raw.replace(\"Merged Limitations:\", \"\").strip()\n",
        "\n",
        "    # Split by \\n where numbered items begin\n",
        "    parts = re.split(r'\\n(?=\\d+\\.\\s)', raw)\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part.strip(), flags=re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'Ground_Truth_Lim_OPR_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eb42a1a-ec1d-469c-bae4-355c2eb33196"
      },
      "outputs": [],
      "source": [
        "# making combinations from 'ground truth' and llm generated text'\n",
        "\n",
        "df['combined'] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Generate combinations for each row\n",
        "for i in range(len(df)):\n",
        "    combined_list = []\n",
        "    list1 = df[\"Ground_Truth_Lim_OPR_list\"][i]\n",
        "    list2 = df[\"extractor_agent_list\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df.at[i, 'combined'] = combined_list  # Truncate if needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "import json\n",
        "\n",
        "llm_results = []\n",
        "df['extractor_eval_new_gt'] = ''\n",
        "for idx in range(len(df)): # len(df)\n",
        "    print(\"idx is\",idx)\n",
        "    pairs = df.at[idx, 'combined']   # assume this is List[Tuple[list, list]]\n",
        "    if not isinstance(pairs, list) or not pairs:\n",
        "        llm_results.append(None)\n",
        "        continue\n",
        "\n",
        "    # build the named-pairs block in one go\n",
        "    formatted = \"\\n\".join(\n",
        "        f\"Pair {i+1}:\\n  List1: {first}\\n  List2: {second}\"\n",
        "        for i, (first, second) in enumerate(pairs)\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"For each of the following pairs, answer “Yes” if List1 contains a topic or limitation\\n\"\n",
        "        \"from List2, or List2 contains a topic or limitation from from List1; otherwise answer “No”.\\n\"\n",
        "        \"Respond *only* with a JSON object mapping each Pair name to “Yes” or “No”.\\n\\n\"\n",
        "        \"Pairs:\\n\"\n",
        "        f\"{formatted}\"\n",
        "    )\n",
        "\n",
        "    # single call per row\n",
        "    resp_text = run_critic_openai(prompt)\n",
        "    llm_results.append(resp_text)\n",
        "\n",
        "    df.at[idx, 'extractor_eval_new_gt'] = resp_text\n",
        "\n"
      ],
      "metadata": {
        "id": "n21qIc7E2-8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "488a3f8f-6cd2-4ef6-8da2-a672842e2c57"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"df_neruips_21_22_final.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87f2ec5-a894-4824-ad14-f2e20faba157"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# extract all 'Yes', 'No'\n",
        "pattern = r'\"Pair\\s*\\d+\"\\s*:\\s*\"(Yes|No)\"'\n",
        "\n",
        "all_matches = []\n",
        "for idx in range(len(df)):\n",
        "    raw = df.at[idx, 'extractor_eval_new_gt']\n",
        "    if not isinstance(raw, str):\n",
        "        all_matches.append([])\n",
        "        continue\n",
        "    matches = re.findall(pattern, raw)\n",
        "    all_matches.append(matches)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb5aa3c1-1aea-4ac2-af58-a99932fe118d",
        "outputId": "e0defb6a-cc1c-4e38-c384-43a159ccc2f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "325"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86ceb916-cc46-45af-9091-496c2bfde93d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for idx, tuples in df['combined'].items():\n",
        "    if not isinstance(tuples, list):\n",
        "        continue\n",
        "    # get the matching list for this row\n",
        "    matches = all_matches[idx] if idx < len(all_matches) else []\n",
        "\n",
        "    for j, (list1, list2) in enumerate(tuples):\n",
        "        # grab the j-th match or None if out of range\n",
        "        is_match = matches[j] if j < len(matches) else None\n",
        "\n",
        "        rows.append({\n",
        "            'source_row': idx,\n",
        "            'List1':      list1,\n",
        "            'List2':      list2,\n",
        "            'is_match':   is_match\n",
        "        })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "\n",
        "result_df.rename(\n",
        "    columns={\n",
        "        'List1': 'Ground_Truth',\n",
        "        'List2': 'LLM_generated'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "e1341858-ff8e-48db-98ca-ca6bbaf5d713"
      },
      "outputs": [],
      "source": [
        "# Initialize a counter and a variable to track the current gt_number\n",
        "current_gt = None\n",
        "counter = 1\n",
        "\n",
        "# We'll update the values in a new list first\n",
        "llm_gen_numbers = []\n",
        "\n",
        "for idx, row in result_df.iterrows():\n",
        "    gt = row['gt_number']\n",
        "\n",
        "    if gt != current_gt:\n",
        "        # New gt_number group, reset counter\n",
        "        current_gt = gt\n",
        "        counter = 1\n",
        "    llm_gen_numbers.append(counter)\n",
        "    counter += 1\n",
        "\n",
        "# Assign the list back to the DataFrame\n",
        "result_df['llm_gen_number'] = llm_gen_numbers\n",
        "# Now result_df['gt_number_consecutive'] will be identical for any consecutive rows\n",
        "# that share the same Ground_Truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca340db3-e8d7-4983-aff7-f6798dc9fbbf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def extract_leading_number(x):\n",
        "    \"\"\"\n",
        "    If x is a list, grab its first element; then:\n",
        "    • If it’s an int, return it.\n",
        "    • If it’s a string starting with digits (with or without a dot), return those digits.\n",
        "    Otherwise return None.\n",
        "    \"\"\"\n",
        "    # step 1: if it’s a list, pull out the first item\n",
        "    val = x[0] if isinstance(x, list) and x else x\n",
        "\n",
        "    # step 2: if it’s already an int, just return it\n",
        "    if isinstance(val, int):\n",
        "        return val\n",
        "\n",
        "    # step 3: if it’s a string, regex for leading digits\n",
        "    if isinstance(val, str):\n",
        "        # match “123.” or just “123”\n",
        "        m = re.match(r'^\\s*(\\d+)(?:\\.)?', val)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "# extract into new columns\n",
        "result_df['gt_number']        = result_df['Ground_Truth'].apply(extract_leading_number)\n",
        "result_df['llm_gen_number']   = result_df['LLM_generated'].apply(extract_leading_number)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8213215-fdb3-4203-83d4-c7c49b11ab41"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv(\"df_llm_as_a_judge_extactor_lim_or.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth coverage\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "match = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in result_df.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['gt_number'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            match += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['gt_number']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    match += 1\n",
        "print(match)\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = result_df['Ground_Truth'].ne(result_df['Ground_Truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "ck = result_df.groupby(unique_blocks)['gt_number'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(ck)\n"
      ],
      "metadata": {
        "id": "qQ2FKszg3Lxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5ab55935-daa3-4520-b6d8-e6f8beab35d2"
      },
      "source": [
        "### Evaluation (faithfulness, soundness, importance), ext+anly+rev+cit (rel 8 then rel text with input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b64c2f7b-bdc8-4c4c-b915-1614b027ba23"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3e31da0-aeaa-4ebf-94a5-5b378e7e1ef1"
      },
      "outputs": [],
      "source": [
        "evaluation_prompt_template = '''\n",
        "You are an expert reviewer. Evaluate the quality of the generated limitations based on the following three criteria: Faithfulness,\n",
        "Soundness, and Importance. For each criterion, assign a score between 1 and 5 and provide a short justification.\n",
        "\n",
        "Faithfulness = The generated limitations should accurately represent the paper’s content and findings, avoiding any introduction\n",
        "of misinformation or contradictions to the original concepts, methodologies or results presented.\n",
        "– 5 points: Perfect alignment with the original content and findings, with no misinformation or contradictions. Fully reflects the\n",
        "paper’s concepts, methodologies, and results\n",
        "accurately.\n",
        "– 4 points: Mostly aligns with the original content but contains minor inaccuracies or slight\n",
        "misinterpretations. These do not significantly\n",
        "affect the overall understanding of the paper’s\n",
        "concepts or results.\n",
        "– 3 points: Generally aligns with the original\n",
        "content but includes several minor inaccuracies or contradictions. Some elements may\n",
        "not fully reflect the paper’s concepts or results,\n",
        "though the overall understanding is mostly intact.\n",
        "– 2 points: Noticeable misalignment with the\n",
        "original content, with multiple inaccuracies\n",
        "or contradictions that could mislead readers.\n",
        "Some key aspects of the paper’s concepts or\n",
        "results are misrepresented.\n",
        "– 1 point: Introduces significant misalignment\n",
        "by misrepresenting issues that do not exist in\n",
        "the paper. Creates considerable misinformation and contradictions that distort the original\n",
        "content, concepts, or results.\n",
        "\n",
        "Soundness = The generated limitations should be detailed and specific, with suggestions or critiques that are practical, logically\n",
        "coherent, and purposeful. It should clearly address relevant aspects of the paper and offer insights that can genuinely improve the\n",
        "research.\n",
        "– 5 points: Highly detailed and specific, with\n",
        "practical, logically coherent, and purposeful\n",
        "suggestions. Clearly addresses relevant aspects and offers insights that substantially improve the research.\n",
        "– 4 points: Detailed and mostly specific, with\n",
        "generally practical and logically sound suggestions. Addresses relevant aspects well but may\n",
        "lack depth or novelty in some areas.\n",
        "– 3 points: Detailed and specific but with some\n",
        "issues in practicality or logical coherence. Suggestions are somewhat relevant and offer partial improvements.\n",
        "– 2 points: Somewhat vague or lacking in specificity, with suggestions that have limited practicality or logical coherence. Addresses\n",
        "relevant aspects only partially and provides minimal improvement.\n",
        "– 1 point: Lacks detail and specificity, with impractical or incoherent suggestions. Fails to\n",
        "effectively address relevant aspects or offer\n",
        "constructive insights for improvement.\n",
        "\n",
        "Importance =  The generated limitations should\n",
        "address the most significant issues that impact the\n",
        "paper’s main findings and contributions. They\n",
        "should highlight key areas where improvements\n",
        "or further research are needed, emphasizing their\n",
        "potential to enhance the research’s relevance and\n",
        "overall impact.\n",
        "– 5 points: Addresses critical issues that substantially impact the paper’s findings and contributions. Clearly identifies major areas for\n",
        "significant improvement or further research,\n",
        "enhancing the research’s relevance and overall\n",
        "impact.\n",
        "– 4 points: Identifies meaningful issues that contribute to refining the paper’s findings and\n",
        "methodology. While the impact is notable,\n",
        "it does not reach the level of fundamentally\n",
        "shaping future research directions.\n",
        "– 3 points: Highlights important issues that offer some improvement to the current work but\n",
        "do not significantly impact future research directions. Provides useful insights for refining\n",
        "the paper but lacks broader implications for\n",
        "further study.\n",
        "– 2 points: Points out limitations with limited\n",
        "relevance to the paper’s overall findings and\n",
        "contributions. Suggestions offer marginal improvements but fail to address more substantial\n",
        "gaps in the research.\n",
        "– 1 point: Focuses on trivial issues, such as minor errors or overly detailed aspects. Does not\n",
        "address substantive issues affecting the paper’s\n",
        "findings or contributions, limiting its overall\n",
        "relevance and impact.\n",
        "\n",
        "Input:\n",
        "Input Paper: [Input Paper]\n",
        "LLM Generated Limitations: [LLM Generated Limitations]\n",
        "\n",
        "Please evaluate the **Generated Limitations** based on the **Input Paper Content** and return your response strictly in the following JSON format:\n",
        "\n",
        "Faithfulness: rating: , explanation:,\n",
        "Soundness:    rating: explanation: ,\n",
        "Importance:   rating: , explanation:\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results = []\n",
        "\n",
        "df['ext_analy_result'] = ''\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    input_text = df.at[i, 'response_string_neurips']\n",
        "    generated_limitations = df.at[i, 'master_agent_ext_analy_rev']\n",
        "\n",
        "    if pd.isna(input_text) or pd.isna(generated_limitations):\n",
        "        results.append(None)\n",
        "        continue\n",
        "\n",
        "    prompt = evaluation_prompt_template.format(\n",
        "        input_text=input_text.strip(),\n",
        "        generated_limitations=generated_limitations.strip()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = azure_run_critic(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        result = None\n",
        "\n",
        "    df.at[i, \"ext_analy_result\"] = result\n",
        "    # results.append(result)\n",
        "\n",
        "# df.to_csv(\"/media/ibrahim/Extreme SSD/Limitations Data/NeurIPS_new/evaluations/df_neurips_21_22_eval.csv\",index=False)"
      ],
      "metadata": {
        "id": "Wt4GMW_o3Zxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01f83c59-64eb-44ef-9ad5-128306c08530",
        "outputId": "0412b08c-0ab9-4a95-e0f6-e65a26b67a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Faithfulness: 4.00\n",
            "Average Soundness:   3.00\n",
            "Average Importance:  3.99\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize empty columns\n",
        "df['faithfulness_score'] = None\n",
        "df['soundness_score'] = None\n",
        "df['importance_score'] = None\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    val = row['ext_analy_result']\n",
        "\n",
        "    if pd.isna(val):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Clean and parse JSON\n",
        "        clean_json = re.sub(r'```json|```', '', val).strip()\n",
        "        parsed = json.loads(clean_json)\n",
        "\n",
        "        # Store ratings into new columns\n",
        "        df.at[idx, 'faithfulness_score'] = parsed['Faithfulness']['rating']\n",
        "        df.at[idx, 'soundness_score'] = parsed['Soundness']['rating']\n",
        "        df.at[idx, 'importance_score'] = parsed['Importance']['rating']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {idx} failed to parse:\", e)\n",
        "        continue\n",
        "\n",
        "avg_faith = df['faithfulness_score'].mean()\n",
        "avg_sound = df['soundness_score'].mean()\n",
        "avg_imp = df['importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c594fccb-59ea-44e5-8009-33e44c346e00"
      },
      "outputs": [],
      "source": [
        "evaluation_prompt_template = '''\n",
        "You are an expert reviewer. I am providing the LLM-generated limitations, the ground truth limitations, and a score from an initial assessment.\n",
        "\n",
        "Your job is to **review the initial rating**, compare the **LLM-generated limitations** against the **ground truth**, and adjust the\n",
        "score **if necessary** for each of the following criteria: Faithfulness, Soundness, and Importance.\n",
        "\n",
        "Each criterion should have a rating from 1 to 5 (see scale below), along with a short justification.\n",
        "\n",
        "Faithfulness = The generated limitations should accurately represent the paper’s content and findings, avoiding any introduction\n",
        "of misinformation or contradictions to the original concepts, methodologies or results presented.\n",
        "– 5 points: Perfect alignment with the original content and findings, with no misinformation or contradictions. Fully reflects the\n",
        "paper’s concepts, methodologies, and results\n",
        "accurately.\n",
        "– 4 points: Mostly aligns with the original content but contains minor inaccuracies or slight\n",
        "misinterpretations. These do not significantly\n",
        "affect the overall understanding of the paper’s\n",
        "concepts or results.\n",
        "– 3 points: Generally aligns with the original\n",
        "content but includes several minor inaccuracies or contradictions. Some elements may\n",
        "not fully reflect the paper’s concepts or results,\n",
        "though the overall understanding is mostly intact.\n",
        "– 2 points: Noticeable misalignment with the\n",
        "original content, with multiple inaccuracies\n",
        "or contradictions that could mislead readers.\n",
        "Some key aspects of the paper’s concepts or\n",
        "results are misrepresented.\n",
        "– 1 point: Introduces significant misalignment\n",
        "by misrepresenting issues that do not exist in\n",
        "the paper. Creates considerable misinformation and contradictions that distort the original\n",
        "content, concepts, or results.\n",
        "\n",
        "Soundness = The generated limitations should be detailed and specific, with suggestions or critiques that are practical, logically\n",
        "coherent, and purposeful. It should clearly address relevant aspects of the paper and offer insights that can genuinely improve the\n",
        "research.\n",
        "– 5 points: Highly detailed and specific, with\n",
        "practical, logically coherent, and purposeful\n",
        "suggestions. Clearly addresses relevant aspects and offers insights that substantially improve the research.\n",
        "– 4 points: Detailed and mostly specific, with\n",
        "generally practical and logically sound suggestions. Addresses relevant aspects well but may\n",
        "lack depth or novelty in some areas.\n",
        "– 3 points: Detailed and specific but with some\n",
        "issues in practicality or logical coherence. Suggestions are somewhat relevant and offer partial improvements.\n",
        "– 2 points: Somewhat vague or lacking in specificity, with suggestions that have limited practicality or logical coherence. Addresses relevant\n",
        "aspects only partially and provides minimal\n",
        "improvement.\n",
        "– 1 point: Lacks detail and specificity, with impractical or incoherent suggestions. Fails to\n",
        "effectively address relevant aspects or offer\n",
        "constructive insights for improvement.\n",
        "\n",
        "Importance = The generated limitations should\n",
        "address the most significant issues that impact the\n",
        "paper’s main findings and contributions. They\n",
        "should highlight key areas where improvements\n",
        "or further research are needed, emphasizing their\n",
        "potential to enhance the research’s relevance and\n",
        "overall impact.\n",
        "– 5 points: Addresses critical issues that substantially impact the paper’s findings and contributions. Clearly identifies major areas for\n",
        "significant improvement or further research,\n",
        "enhancing the research’s relevance and overall\n",
        "impact.\n",
        "– 4 points: Identifies meaningful issues that contribute to refining the paper’s findings and\n",
        "methodology. While the impact is notable,\n",
        "it does not reach the level of fundamentally\n",
        "shaping future research directions.\n",
        "– 3 points: Highlights important issues that offer some improvement to the current work but\n",
        "do not significantly impact future research directions. Provides useful insights for refining\n",
        "the paper but lacks broader implications for\n",
        "further study.\n",
        "– 2 points: Points out limitations with limited\n",
        "relevance to the paper’s overall findings and\n",
        "contributions. Suggestions offer marginal improvements but fail to address more substantial\n",
        "gaps in the research.\n",
        "– 1 point: Focuses on trivial issues, such as minor errors or overly detailed aspects. Does not\n",
        "address substantive issues affecting the paper’s\n",
        "findings or contributions, limiting its overall\n",
        "relevance and impact.\n",
        "\n",
        "Input:\n",
        "Ground Truth Limitations: {input_text}\n",
        "\n",
        "LLM Generated Limitations: {generated_limitations}\n",
        "\n",
        "Initial Scores:\n",
        "- Faithfulness: {faith}\n",
        "- Soundness: {sound}\n",
        "- Importance: {imp}\n",
        "\n",
        "Please evaluate the **LLM Generated Limitations** based on the **Ground Truth** and return your response strictly in the following JSON format:\n",
        "\n",
        "Faithfulness: rating: , explanation:,\n",
        "Soundness:    rating: , explanation:,\n",
        "Importance:   rating: , explanation:\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize result columns\n",
        "df['adjusted_score_ext_analy_rev_cit_with_rel_json'] = None  # Optional: for storing raw JSON string\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    try:\n",
        "        input_text = df.at[i, 'Ground_Truth_Lim_OPR']\n",
        "        generated = df.at[i, 'master_agent_ext_analy_rev']\n",
        "        faith = df.at[i, 'faithfulness_score']\n",
        "        sound = df.at[i, 'soundness_score']\n",
        "        imp = df.at[i, 'importance_score']\n",
        "\n",
        "        if pd.isna(input_text) or pd.isna(generated) or pd.isna(faith) or pd.isna(sound) or pd.isna(imp):\n",
        "            continue\n",
        "\n",
        "        prompt = evaluation_prompt_template.format(\n",
        "            input_text=input_text.strip(),\n",
        "            generated_limitations=generated.strip(),\n",
        "            faith=int(faith),\n",
        "            sound=int(sound),\n",
        "            imp=int(imp)\n",
        "        )\n",
        "\n",
        "        result = azure_run_critic(prompt)\n",
        "        df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json'] = result  # Optional: Store full JSON output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {i} failed: {e}\")\n",
        "        continue\n"
      ],
      "metadata": {
        "id": "WXDyvzzd3cqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cb59d10-d59c-4f82-a888-9a99c39b6792"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Initialize new columns\n",
        "df['adjusted_faithfulness_score'] = None\n",
        "df['adjusted_soundness_score'] = None\n",
        "df['adjusted_importance_score'] = None\n",
        "\n",
        "# Define regex patterns for each score\n",
        "faith_re = re.compile(r'\"Faithfulness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "sound_re = re.compile(r'\"Soundness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "imp_re   = re.compile(r'\"Importance\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "\n",
        "# Apply regex extraction row-wise\n",
        "for i in range(len(df)):\n",
        "    row = df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json']\n",
        "    if pd.isna(row):\n",
        "        continue\n",
        "\n",
        "    # Clean text from triple backticks and newline artifacts\n",
        "    cleaned = re.sub(r\"```json|```\", \"\", row).strip()\n",
        "\n",
        "    # Extract values using regex\n",
        "    faith_match = faith_re.search(cleaned)\n",
        "    sound_match = sound_re.search(cleaned)\n",
        "    imp_match   = imp_re.search(cleaned)\n",
        "\n",
        "    if faith_match:\n",
        "        df.at[i, 'adjusted_faithfulness_score'] = int(faith_match.group(1))\n",
        "    if sound_match:\n",
        "        df.at[i, 'adjusted_soundness_score'] = int(sound_match.group(1))\n",
        "    if imp_match:\n",
        "        df.at[i, 'adjusted_importance_score'] = int(imp_match.group(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe66d8a-d230-404c-ad7c-b8752f730732",
        "outputId": "13cf2820-40a5-4c92-b422-72c4f7a7fcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Faithfulness: 3.10\n",
            "Average Soundness:   3.15\n",
            "Average Importance:  3.07\n"
          ]
        }
      ],
      "source": [
        "avg_faith = df['adjusted_faithfulness_score'].mean()\n",
        "avg_sound = df['adjusted_soundness_score'].mean()\n",
        "avg_imp = df['adjusted_importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4934a9a4-d70d-4269-b0a1-b18fbaa0aa85"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"df_neurips_21_22_eval.csv\",index=False)"
      ]
    }
  ]
}