{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c896d5a-bf1e-47f0-9d99-6bf244f9d81a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neurips_limitation_and_OR_with_cited_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dc82303-9a6d-4ab8-a066-8c9bd1cd8d6f",
        "outputId": "db3c666f-7b00-4e26-fe54-4e91b7a71602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (25.1.1)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bc4ecd8-accf-4a67-a583-f06864b35aad"
      },
      "outputs": [],
      "source": [
        "!pip3 -q install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32f0761c-9c9f-45a6-bc44-bae8f6d00c00"
      },
      "outputs": [],
      "source": [
        "# another openai account (original gmail)\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Methodology critic, data critic, assumption critic, master cooridinator"
      ],
      "metadata": {
        "id": "o13WIApj6KaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import asyncio\n",
        "from agents import Agent, Runner\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "await main()\n",
        "\n",
        "\n",
        "# 1) Define your agents once at module-scope\n",
        "methodology_agent = Agent(\n",
        "    name=\"MethodologyCritic\",\n",
        "    instructions=\"\"\"\n",
        "You are a methodology critic.\n",
        "Analyze the METHODOLOGY of this scientific article and list 3 key limitations:\n",
        "Focus on sample size, control groups, reproducibility, and bias.\n",
        "Return bullet points.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "data_agent = Agent(\n",
        "    name=\"DataCritic\",\n",
        "    instructions=\"\"\"\n",
        "You are a data critic.\n",
        "Analyze the DATA in this article and list 3 limitations:\n",
        "Focus on dataset size, missing variables, measurement errors.\n",
        "Return bullet points.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "assumptions_agent = Agent(\n",
        "    name=\"AssumptionsCritic\",\n",
        "    instructions=\"\"\"\n",
        "You are an assumptions critic.\n",
        "Critique the ASSUMPTIONS in this article:\n",
        "Focus on untested premises and generalization issues.\n",
        "Return bullet points.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "master_agent = Agent(\n",
        "    name=\"MasterCoordinator\",\n",
        "    instructions=\"\"\"\n",
        "You are an expert synthesizer.\n",
        "Combine these three lists into a cohesive 'Limitations' section for a scientific paper:\n",
        "1) Methodology\n",
        "\n",
        "Use an academic tone and avoid redundancy.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "\n",
        "# 2) Helper to run all agents for one row\n",
        "async def generate_for_row(row):\n",
        "    # Build the full text from your six columns\n",
        "    sections = [\n",
        "        row[\"df_Abstract\"],\n",
        "        row[\"df_Introduction\"],\n",
        "        row[\"df_Related_Work\"],\n",
        "        row[\"df_Methodology\"],\n",
        "        row[\"df_Dataset\"],\n",
        "        row[\"df_Conclusion\"],\n",
        "    ]\n",
        "    article_text = \"\\n\\n\".join([s for s in sections if isinstance(s, str) and s.strip()])\n",
        "\n",
        "    # 2a) Run the three critics in parallel\n",
        "    meth_task  = Runner.run(starting_agent=methodology_agent, input=article_text)\n",
        "    data_task  = Runner.run(starting_agent=data_agent,       input=article_text)\n",
        "    assum_task = Runner.run(starting_agent=assumptions_agent, input=article_text)\n",
        "\n",
        "    meth_res, data_res, assum_res = await asyncio.gather(meth_task, data_task, assum_task)\n",
        "\n",
        "    # 2b) Prepare the input for the master synthesizer\n",
        "    combined = (\n",
        "        f\"Methodology:\\n{meth_res.final_output}\\n\\n\"\n",
        "        f\"Data:\\n{data_res.final_output}\\n\\n\"\n",
        "        f\"Assumptions:\\n{assum_res.final_output}\"\n",
        "    )\n",
        "    master_res = await Runner.run(starting_agent=master_agent, input=combined)\n",
        "\n",
        "    # 2c) Return both final text and the four input-lists\n",
        "    return {\n",
        "        \"limitations\":      master_res.final_output,\n",
        "        \"input_logs\": {\n",
        "            \"methodology\": meth_res.to_input_list(),\n",
        "            \"data\":        data_res.to_input_list(),\n",
        "            \"assumptions\": assum_res.to_input_list(),\n",
        "            \"master\":      master_res.to_input_list(),\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "# 3) Main entrypoint: load df, process each row, write back to df\n",
        "async def main():\n",
        "    # df = pd.read_csv(\"your_dataset.csv\")  # or however you already have it in memory\n",
        "\n",
        "    all_results = []\n",
        "    for _, row in df2.iterrows():\n",
        "        print(\"yes\")\n",
        "        result = await generate_for_row(row)\n",
        "        print(\"result\", result)\n",
        "        all_results.append(result)\n",
        "\n",
        "    # unpack into new columns\n",
        "    df2[\"LLM_Limitations\"] = [r[\"limitations\"] for r in all_results]\n",
        "    df2[\"LLM_InputLogs\"]   = [r[\"input_logs\"]  for r in all_results]\n",
        "\n",
        "    # df2.to_csv(\"with_limitations.csv\", index=False)\n",
        "    print(\"Done! Wrote with LLM_Limitations and LLM_InputLogs.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YiUWvDmg6QSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dfcb40d-40f7-4559-aab3-1fc8e56eb40f"
      },
      "outputs": [],
      "source": [
        "# 1) Define your agents once at module-scope\n",
        "methodology_agent = Agent(\n",
        "    name=\"MethodologyCritic\",\n",
        "    instructions=\"\"\"\n",
        "You are a methodology critic.\n",
        "Analyze the METHODOLOGY of this scientific article and list 3 key limitations:\n",
        "Focus on sample size, control groups, reproducibility, and bias.\n",
        "Return bullet points.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "master_agent = Agent(\n",
        "    name=\"MasterCoordinator\",\n",
        "    instructions=\"\"\"\n",
        "You are an expert synthesizer.\n",
        "Combine these three lists into a cohesive 'Limitations' section for a scientific paper:\n",
        "1) Methodology\n",
        "\n",
        "Use an academic tone and avoid redundancy.\n",
        "\"\"\",\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Methodology Critic Function\n",
        "def critique_methodology(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # Your model name\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a introduction critic. Analyze the introduction and list 3-5 limitations. Focus on sample size, controls, and bias. Return bullet points.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ],\n",
        "        temperature=0.3  # Less creative, more factual\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Process each row\n",
        "for index, row in df2.iterrows():\n",
        "    # print(f\"\\nAnalyzing Paper {row['paper_id']}:\")\n",
        "    print(row['df_Introduction'])\n",
        "    limitations = critique_methodology(row['df_Introduction'][index])\n",
        "    df2.at[index, 'limitations'] = limitations\n",
        "    print(\"--------------------\")\n",
        "    print(limitations)\n"
      ],
      "metadata": {
        "id": "A-RW0bGs6d6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "17e41efc-2a7c-4833-9946-a8165f6c27a1"
      },
      "source": [
        "### LLM agents to generate limitations (MARG: Agents: Experiment, Clarity, and Impact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "673365e7-f088-480a-b45c-413ff36f6cbe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Define which sections to pull in for every row\n",
        "SECTIONS = [\"df_Abstract\", \"df_Introduction\", \"df_Related_Work\",\n",
        "            \"df_Methodology\", \"df_Dataset\", \"df_Experiment_and_Results\", \"df_Conclusion\"]\n",
        "\n",
        "# Define a helper that streams one prompt and returns the full text\n",
        "def run_critic(prompt: str) -> str:\n",
        "    summary = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary += chunk.choices[0].delta.content or \"\"\n",
        "    return summary.strip()\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for idx in range(len(df_lim)):\n",
        "    # build a list of “Section:\\ncontent” strings\n",
        "    pieces = []\n",
        "    for sec in SECTIONS:\n",
        "        col = sec.replace(\" \", \"_\")    # e.g. \"Related Work\" → \"Related_Work\"\n",
        "        if pd.notna(df_lim.loc[idx, col]):\n",
        "            pieces.append(f\"{sec}:\\n{df_lim.loc[idx, col]}\")\n",
        "\n",
        "    text_blob = \"\\n\\n\".join(pieces)\n",
        "\n",
        "    experiments_lim = run_critic(\n",
        "        '''Critically analyze the experimental methodology in this study. Highlight any potential biases, uncontrolled variables,\n",
        "        or limitations in the experimental design. Suggest improvements to enhance reproducibility, statistical power, or alignment\n",
        "        with best practices in [your field, e.g., 'neuroscience' or 'nanomaterials']. Focus on rigor without undermining the study’s\n",
        "        core findings.\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    clarity_lim = run_critic(\n",
        "        '''Rewrite the [Results/Discussion] section to improve clarity for a multidisciplinary audience. Avoid jargon, use active voice,\n",
        "        and emphasize logical flow. Ensure each paragraph transitions smoothly, with clear topic sentences and concise takeaways.\n",
        "        Maintain technical accuracy while prioritizing accessibility. \\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    impact_lim = run_critic(\n",
        "        '''Articulate the broader implications of these findings for [field/industry/society]. Draft a compelling 'Impact Statement'\n",
        "        (3–5 sentences) that highlights: (1) How this work advances current knowledge, (2) Potential real-world applications,\n",
        "        and (3) Future research directions. Avoid hyperbole; anchor claims in the data.:\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "\n",
        "    # 3) master coordinator to fuse them\n",
        "    coord_prompt = (\n",
        "        \"You are a **Master Coordinator**.  You will be given three lists of sub-limitations.\\n\\n\"\n",
        "        f\"**Experiment Critic**:\\n{experiments_lim}\\n\\n\"\n",
        "        f\"**Clarity Critic**:\\n{clarity_lim}\\n\\n\"\n",
        "        f\"**Impact Critic**:\\n{impact_lim}\\n\\n\"\n",
        "        \"Please synthesize these into a single, coherent **final** set of limitations, \"\n",
        "        \"organizing them thematically and removing duplicates.\"\n",
        "    )\n",
        "    final_lim = run_critic(coord_prompt)\n",
        "\n",
        "    generated_limitations.append({\n",
        "        # \"Experiment\": experiments_lim,\n",
        "        # \"Clarity\": clarity_lim,\n",
        "        # \"Impact\": impact_lim,\n",
        "        \"final\": final_lim\n",
        "    })\n",
        "    time.sleep(1)  # to avoid hitting rate limits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35ae6ea0-19ac-4521-9eaf-0ddce9339030"
      },
      "outputs": [],
      "source": [
        "# Extract the 'final' value from each dict and build a DataFrame\n",
        "final_values = [d.get(\"final\", \"\") for d in generated_limitations]\n",
        "df_generated_limitations_2 = pd.DataFrame(final_values, columns=[\"generated_limitations_1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b200fbb4-407f-4916-9208-44892f18e3b6"
      },
      "outputs": [],
      "source": [
        "# convert list to string and split\n",
        "def process_single_limitation(limitation_text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Split the text on '**' and return the segments\n",
        "    that occur before each '**'.\n",
        "    \"\"\"\n",
        "    parts = limitation_text.split(\"**\")\n",
        "    # parts at even indices (0,2,4,…) are the “previous” segments\n",
        "    prev_texts = [\n",
        "        part.strip()\n",
        "        for idx, part in enumerate(parts)\n",
        "        if idx % 2 == 0    # even indices\n",
        "           and part.strip()  # non-empty\n",
        "    ]\n",
        "    return prev_texts\n",
        "\n",
        "# Apply to your DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = (\n",
        "    df_generated_limitations_2[\"generated_limitations_1\"]\n",
        "    .apply(process_single_limitation)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3db2cd40-de7d-4c0a-b317-fc83c252d32d"
      },
      "outputs": [],
      "source": [
        "# convert string to list\n",
        "import ast\n",
        "\n",
        "# This will parse the string \"[...]\" into a real list object\n",
        "df_generated_limitations_2['generated_limitations_1'] = (\n",
        "    df_generated_limitations_2['generated_limitations_1']\n",
        "      .astype(str)               # ensure it’s a string\n",
        "      .apply(ast.literal_eval)   # safely evaluate Python literal\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6308dd91-c761-4914-b2e7-d7c8ba578e8c"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "def enumerate_and_filter(cell):\n",
        "    \"\"\"\n",
        "    Given a cell that is either:\n",
        "      - A Python list of strings, or\n",
        "      - A string repr of such a list,\n",
        "    this will:\n",
        "      1. turn it into a list of sublists,\n",
        "      2. remove any sublist equal to ['-'],\n",
        "      3. prefix each remaining sublist's string with its 1-based index,\n",
        "      4. return a new list-of-lists.\n",
        "    \"\"\"\n",
        "    # 1) Parse string repr if necessary\n",
        "    if isinstance(cell, str):\n",
        "        try:\n",
        "            lst = ast.literal_eval(cell)\n",
        "        except Exception:\n",
        "            # not a literal list → treat the entire cell as one string\n",
        "            lst = [cell]\n",
        "    else:\n",
        "        lst = cell\n",
        "\n",
        "    # 2) Ensure list-of-lists\n",
        "    lol = []\n",
        "    for item in lst:\n",
        "        if isinstance(item, list):\n",
        "            lol.append(item)\n",
        "        else:\n",
        "            # assume it's a bare string\n",
        "            lol.append([str(item)])\n",
        "\n",
        "    # 3) Filter out ['-'] sublists\n",
        "    filtered = [sub for sub in lol if not (len(sub) == 1 and sub[0].strip() == \"-\")]\n",
        "\n",
        "    # 4) Enumerate: prefix each sublist’s only element with \"i. \"\n",
        "    enumerated = [[f\"{i+1}. {sub[0]}\"] for i, sub in enumerate(filtered)]\n",
        "\n",
        "    return enumerated\n",
        "\n",
        "# Example usage on your DataFrame\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1'].apply(enumerate_and_filter))\n",
        "\n",
        "# Remove the first sublist in each list-of-lists\n",
        "df_generated_limitations_2['generated_limitations_1'] = (df_generated_limitations_2['generated_limitations_1']\n",
        "    .apply(lambda lol: lol[1:] if isinstance(lol, list) and len(lol) > 0 else [])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_limitation(limitation_text):\n",
        "    # Split into different limitations (separated by \\n\\n)\n",
        "    limitations = limitation_text.split('\\n\\n')\n",
        "    processed_limitations = []\n",
        "\n",
        "    for limitation in limitations:\n",
        "        # Remove numbering (e.g., \"1. **Limited Literature Review**\" → \"**Limited Literature Review**\")\n",
        "        cleaned_limitation = limitation.split('. ', 1)[-1] if '. ' in limitation else limitation\n",
        "\n",
        "        # Split into sentences (using '.')\n",
        "        sentences = [s.strip() for s in cleaned_limitation.split('.') if s.strip()]\n",
        "\n",
        "        if sentences:\n",
        "            processed_limitations.append(sentences)\n",
        "\n",
        "    return processed_limitations\n",
        "\n",
        "# df_generated_limitations_2['generated_limitations_1'] = df_generated_limitations_2['generated_limitations_1'].apply(process_single_limitation)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(process_single_limitation)"
      ],
      "metadata": {
        "id": "M_x9Sqsp6n7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add numbering of LLM generated limitations\n",
        "def add_numbering_to_limitations(list_of_lists):\n",
        "    if not isinstance(list_of_lists, list):\n",
        "        return list_of_lists  # Skip if not a list\n",
        "\n",
        "    numbered_list = []\n",
        "    for idx, sublist in enumerate(list_of_lists, start=1):\n",
        "        if sublist:  # Ensure sublist is not empty\n",
        "            # Add numbering to the first element of the sublist\n",
        "            numbered_sublist = [f\"{idx}. {sublist[0]}\"] + sublist[1:]\n",
        "            numbered_list.append(numbered_sublist)\n",
        "    return numbered_list\n",
        "\n",
        "# Apply to the column (modifies existing column)\n",
        "df_lim['Lim_and_OR_ground_truth_final'] = df_lim['Lim_and_OR_ground_truth_final'].apply(add_numbering_to_limitations)\n",
        "# df_generated_limitations_2['generated_limitations_1']  = df_generated_limitations_2['generated_limitations_1'] .apply(add_numbering_to_limitations)"
      ],
      "metadata": {
        "id": "nYwDCLUE6mAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "def remove_here_are_the(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting any sub-list that starts with \"1. Here are the\"\n",
        "    (case-insensitive and whitespace-tolerant).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0].strip().lower()  # Clean whitespace and lowercase\n",
        "            if not first_item.startswith(\"1. here are the\"):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to every row in the DataFrame\n",
        "df_lim[\"Lim_and_OR_ground_truth_final\"] = df_lim[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_here_are_the(lst) if isinstance(lst, list) else lst\n",
        ")"
      ],
      "metadata": {
        "id": "loV5tXg46kGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}