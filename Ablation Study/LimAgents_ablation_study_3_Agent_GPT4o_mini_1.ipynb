{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f535af8-3d65-4e4d-891c-2211824df7c6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neurips_limitation_and_OR_with_cited_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3de412ad-59d5-454e-a10b-d1151c2cdac6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install langchain\n",
        "# For the chat-style client\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# If you also need the OpenAI LLM wrapper\n",
        "from langchain.llms import OpenAI\n",
        "!pip install --upgrade langchain\n",
        "import time\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "import pandas as pd\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import torch\n",
        "import io\n",
        "import sys\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from langchain.schema import Document as LCDocument\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# New imports for FAISS and BM25 from LangChain\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.schema import Document as LCDocument\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.text_splitter    import CharacterTextSplitter\n",
        "from langchain.embeddings       import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores     import FAISS\n",
        "from langchain.retrievers       import BM25Retriever\n",
        "from sklearn.metrics.pairwise  import cosine_similarity\n",
        "import numpy as np\n",
        "# from llama_index import Document as LIDoc, VectorStoreIndex\n",
        "from llama_index.core.schema import Document as LIDoc\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from llama_index.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import io\n",
        "import sys\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores    import FAISS\n",
        "from langchain.retrievers      import BM25Retriever, EnsembleRetriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "# 2) Import the predictor and context from their true locations\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n"
      ],
      "metadata": {
        "id": "i8EQzW1SomA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bda8c69-def3-4707-9b54-5d26d2eccbf2"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# 1) choose your model’s max tokens\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME         = \"gpt-4o-mini\"\n",
        "\n",
        "# 2) helper to count tokens of any string\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text))\n",
        "\n",
        "# 3) helper to truncate a list of passages so that\n",
        "#    len(query) + sum(len(passages)) <= MAX_CONTEXT_TOKENS\n",
        "def truncate_for_context(\n",
        "    query: str,\n",
        "    passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    enc        = tiktoken.encoding_for_model(model)\n",
        "    q_tokens   = enc.encode(query)\n",
        "    remaining  = max_tokens - len(q_tokens)\n",
        "    kept, used = [], 0\n",
        "\n",
        "    for p in passages:\n",
        "        p_tokens = enc.encode(p)\n",
        "        if used + len(p_tokens) > remaining:\n",
        "            # only take as many tokens as fit\n",
        "            allowed = remaining - used\n",
        "            if allowed > 0:\n",
        "                kept.append(enc.decode(p_tokens[:allowed]))\n",
        "            break\n",
        "        kept.append(p)\n",
        "        used += len(p_tokens)\n",
        "\n",
        "    return kept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8bea9ff-fc86-4722-8d43-44c5434ba596",
        "outputId": "082b8b9c-63dc-44f3-eba8-b38c1070dfa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_652548/2005548234.py:25: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n"
          ]
        }
      ],
      "source": [
        "# single embeddings object\n",
        "# hf_emb = HuggingFaceEmbeddings(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def make_retriever_for_docs(docs, k=3):\n",
        "    # FAISS\n",
        "    faiss_store = FAISS.from_documents(docs, hf_emb)\n",
        "    faiss_r     = faiss_store.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # BM25\n",
        "    bm25_r      = BM25Retriever.from_documents(docs)\n",
        "    bm25_r.k    = k\n",
        "\n",
        "    # ensemble (50/50 weight)\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "\n",
        "generated_limitations = []\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains      import RetrievalQA\n",
        "\n",
        "# your GPT-4o-mini chat model\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def generate_limitations(question, retriever):\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=chat_llm,\n",
        "        chain_type=\"stuff\",        # simplest “stuff the docs into the prompt”\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "    return qa.run(question).strip()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b25cced4-5207-408c-9d34-7b7b79b1ec4a"
      },
      "outputs": [],
      "source": [
        "# define the (label, column_name) pairs in order\n",
        "fields = [\n",
        "    (\"Abstract\",                \"df_Abstract\"),\n",
        "    (\"Introduction\",            \"df_Introduction\"),\n",
        "    (\"Related Work\",            \"df_Related_Work\"),\n",
        "    (\"Methodology\",             \"df_Methodology\"),\n",
        "    (\"Dataset\",                 \"df_Dataset\"),\n",
        "    (\"Conclusion\",              \"df_Conclusion\"),\n",
        "    (\"Experiment and Results\",  \"df_Experiment_and_Results\"),\n",
        "]\n",
        "\n",
        "def make_response_string(row):\n",
        "    parts = []\n",
        "    for label, col in fields:\n",
        "        text = row.get(col, \"\")\n",
        "        if isinstance(text, str) and text.strip():\n",
        "            parts.append(f\"{label}: {text.strip()}\")\n",
        "    # join with a separator of your choice\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "# create the new column\n",
        "df[\"response_string_all\"] = df.apply(make_response_string, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57328261-fe33-4e3d-8593-a52d9ffa1b09"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "\n",
        "\n",
        "system_prompt = '''You are a helpful, respectful, and honest assistant for generating limitations or shortcomings of a research paper.\n",
        " Generate limitations or shortcomings for the following passages from the scientific paper.'''\n",
        "\n",
        "# ─── your existing setup ─────────────────────────────────────────────────────\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# ─── new unified helper ───────────────────────────────────────────────────────\n",
        "def run_critic(prompt: str,*,system_prompt: str | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Wraps ChatOpenAI to (optionally) send a system prompt + your user prompt,\n",
        "    and returns the assistant's reply as a stripped string.\n",
        "    \"\"\"\n",
        "    messages: list[SystemMessage | HumanMessage] = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(SystemMessage(content=system_prompt))\n",
        "\n",
        "    messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "    # this will wait for the full response (no streaming)\n",
        "    response = chat_llm.invoke(messages)\n",
        "    return response.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1695ef91-da3c-4542-9a7e-6291f6a2fcf6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "def truncate_for_context(query: str,passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    enc       = tiktoken.encoding_for_model(model)\n",
        "    # allow all specials\n",
        "    q_tokens  = enc.encode(query, disallowed_special=())\n",
        "    budget    = max_tokens - len(q_tokens)\n",
        "    kept, used = [], 0\n",
        "    for p in passages:\n",
        "        p_toks = enc.encode(p, disallowed_special=())\n",
        "        if used + len(p_toks) > budget:\n",
        "            if budget - used > 0:\n",
        "                kept.append(enc.decode(p_toks[:(budget - used)]))\n",
        "            break\n",
        "        kept.append(p)\n",
        "        used += len(p_toks)\n",
        "    return kept\n",
        "\n",
        "\n",
        "def ensure_passages_within_budget(\n",
        "    query: str,\n",
        "    passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Returns `passages` truncated so that\n",
        "    count_tokens(query + concat(passages)) ≤ max_tokens.\n",
        "    \"\"\"\n",
        "    # count full size\n",
        "    total = count_tokens(query + \"\\n\\n\".join(passages), model=model)\n",
        "    if total <= max_tokens:\n",
        "        return passages\n",
        "\n",
        "    print(f\"Truncating context ({total} tokens)…\")\n",
        "    # truncate_for_context only returns the shorter passages list\n",
        "    return truncate_for_context(query, passages, max_tokens=max_tokens, model=model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e348a86-52e0-4fec-9ddd-419b5356d67f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Define which sections to pull in for every row\n",
        "SECTIONS = [\"df_Abstract\", \"df_Introduction\", \"df_Related_Work\",\n",
        "            \"df_Methodology\", \"df_Dataset\", \"df_Experiment_and_Results\", \"df_Conclusion\"]\n",
        "\n",
        "# Define a helper that streams one prompt and returns the full text\n",
        "def run_critic(prompt: str) -> str:\n",
        "    summary = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary += chunk.choices[0].delta.content or \"\"\n",
        "    return summary.strip()\n",
        "\n",
        "generated_limitations = []\n",
        "\n",
        "for idx in range(10): # len(df_lim)\n",
        "    # build a list of “Section:\\ncontent” strings\n",
        "    pieces = []\n",
        "    for sec in SECTIONS:\n",
        "        col = sec.replace(\" \", \"_\")    # e.g. \"Related Work\" → \"Related_Work\"\n",
        "        if pd.notna(df.loc[idx, col]):\n",
        "            pieces.append(f\"{sec}:\\n{df.loc[idx, col]}\")\n",
        "\n",
        "    text_blob = \"\\n\\n\".join(pieces)\n",
        "\n",
        "    extractor_agent = run_critic(\n",
        "        '''You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article and\n",
        "        extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as the Discussion, Conclusion.\n",
        "        List each limitation verbatim, including direct quotes where possible, and provide\n",
        "        a brief context (e.g., what aspect of the study the limitation pertains to). Ensure accuracy and avoid inferring or adding\n",
        "        limitations not explicitly stated. If no limitations are mentioned, state this clearly. Output your findings in a structured\n",
        "        format with bullet points.\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    analyzer_agent = run_critic(\n",
        "        '''You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze the\n",
        "        provided scientific article and identify potential limitations that are not explicitly stated by the authors. Focus on aspects\n",
        "        such as study design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions.\n",
        "        For each inferred limitation, provide a clear explanation of why it is a limitation and how it impacts the study’s validity,\n",
        "        reliability, or generalizability. Ensure your inferences are grounded in the article’s content and avoid speculative assumptions.\n",
        "        Output your findings in a structured format with bullet points, including a brief justification for each limitation.\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "    reviewer_agent = run_critic(\n",
        "        '''You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research. Your task\n",
        "        is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "        that might be raised in an open review process, considering common critiques such as reproducibility, transparency,\n",
        "        generalizability, or ethical considerations. If possible, leverage insights from similar studies or common methodological\n",
        "        issues in the field (search the web or X posts if needed for context). For each limitation, explain why it would be a\n",
        "        concern in an open review and how it aligns with peer review standards. Output your findings in a structured format with\n",
        "        bullet points, ensuring each limitation is relevant to the article’s content.:\\n\\n'''\n",
        "        + text_blob\n",
        "    )\n",
        "\n",
        "    # 3) master coordinator to fuse them\n",
        "    coord_prompt = (\n",
        "    '''You are a **Master Coordinator**. You are an expert in scientific communication and synthesis.\n",
        "    Your task is to integrate the limitations provided by three other agents:\n",
        "    1. The **Extractor** (explicit limitations from the article),\n",
        "    2. The **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. The **Reviewer** (limitations from an open review perspective).\n",
        "\n",
        "    Your goals are to:\n",
        "    1. Combine all limitations into a cohesive list, removing redundancies.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize limitations explicitly mentioned by the authors, supplementing them with inferred or peer-review-based limitations only if they add value.\n",
        "    4. Highlight discrepancies between the agents’ outputs and resolve them by cross-referencing the article.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for inclusion in a scientific review or report.\n",
        "\n",
        "    Provide a brief justification for each limitation, noting its source:\n",
        "    - \"Author-stated\" (from Extractor),\n",
        "    - \"Inferred\" (from Analyzer),\n",
        "    - \"Peer-review-derived\" (from Reviewer).\n",
        "\n",
        "    You will be given three lists of sub-limitations:\\n\\n'''\n",
        "    f\"**Extractor Agent**:\\n{extractor_agent}\\n\\n\"\n",
        "    f\"**Analyzer Agent**:\\n{analyzer_agent}\\n\\n\"\n",
        "    f\"**Reviewer Agent**:\\n{reviewer_agent}\\n\\n\"\n",
        "    )\n",
        "    final_lim = run_critic(coord_prompt)\n",
        "\n",
        "    generated_limitations.append({\n",
        "        \"Extractor\": extractor_agent,\n",
        "        \"Analzyer\": analyzer_agent,\n",
        "        \"Reviewer\": reviewer_agent,\n",
        "        \"final\": final_lim\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "536d86c5-7ed3-4e29-8093-577d1261ced9"
      },
      "outputs": [],
      "source": [
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df_generated_limitations_2[\"generated_limitations_1\"] = df_generated_limitations_2[\"generated_limitations_1\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "\n",
        "# remove future work\n",
        "import re\n",
        "\n",
        "def remove_future_entries(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting sublists where the first item contains\n",
        "    'future' inside **double asterisks** (case-insensitive).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0]\n",
        "            # Check if 'future' appears inside **...** (case-insensitive)\n",
        "            if not re.search(r'\\*\\*.*future.*\\*\\*', first_item, re.IGNORECASE):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to the DataFrame column\n",
        "df[\"Lim_and_OR_ground_truth_final\"] = df[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_future_entries(lst) if isinstance(lst, list) else lst\n",
        ")\n",
        "\n",
        "def remove_here_are_the(entries):\n",
        "    \"\"\"\n",
        "    Given a list of lists (where each sub-list contains strings),\n",
        "    return a new list omitting any sub-list that starts with \"1. Here are the\"\n",
        "    (case-insensitive and whitespace-tolerant).\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "    for sublist in entries:\n",
        "        if isinstance(sublist, list) and len(sublist) > 0:\n",
        "            first_item = sublist[0].strip().lower()  # Clean whitespace and lowercase\n",
        "            if not first_item.startswith(\"1. here are the\"):\n",
        "                filtered.append(sublist)\n",
        "        else:\n",
        "            filtered.append(sublist)  # Keep non-list entries as-is\n",
        "    return filtered\n",
        "\n",
        "# Apply to every row in the DataFrame\n",
        "df[\"Lim_and_OR_ground_truth_final\"] = df[\"Lim_and_OR_ground_truth_final\"].apply(\n",
        "    lambda lst: remove_here_are_the(lst) if isinstance(lst, list) else lst\n",
        ")"
      ]
    }
  ]
}