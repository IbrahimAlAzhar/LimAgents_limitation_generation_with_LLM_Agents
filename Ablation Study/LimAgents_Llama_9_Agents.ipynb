{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Limitation Generation with 9 Agents by Llama 3 8B"
      ],
      "metadata": {
        "id": "kHgsX6WEGLXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import signal\n",
        "import sys\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Global variable to store DataFrame\n",
        "global_df = None\n",
        "global_current_row = 0\n",
        "\n",
        "def signal_handler(signum, frame):\n",
        "    \"\"\"Handle termination signals to save progress\"\"\"\n",
        "    print(f\"\\n⚠️ Received signal {signum}. Saving progress before termination...\")\n",
        "    if global_df is not None:\n",
        "        # Save current progress\n",
        "        emergency_file = f\"emergency_save_{global_current_row}.csv\"\n",
        "        global_df.to_csv(emergency_file, index=False)\n",
        "        print(f\"  🚨 Emergency save completed: {emergency_file}\")\n",
        "\n",
        "        # Also save to final output\n",
        "        output_file = \"df_neurips_limitations_multi_agent.csv\"\n",
        "        global_df.to_csv(output_file, index=False)\n",
        "        print(f\"  🚨 Final output updated: {output_file}\")\n",
        "\n",
        "    print(\"  📊 Progress saved. Exiting gracefully...\")\n",
        "    sys.exit(0)\n",
        "\n",
        "# Register signal handlers\n",
        "signal.signal(signal.SIGTERM, signal_handler)  # PBS termination\n",
        "signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C\n"
      ],
      "metadata": {
        "id": "mGIo4BOKE07o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Llama 3 8B model and tokenizer (4-bit quantized)\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "cache_dir = \"llama3_8b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "print(\"Loading Llama 3 8B model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def truncate_prompt_for_model(prompt: str, max_length: int = 8000) -> str:\n",
        "    \"\"\"Truncate prompt to fit within model's context window, leaving room for generation\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # If tokens exceed max_length, truncate\n",
        "    if tokens.shape[1] > max_length:\n",
        "        print(f\"⚠️ Prompt token count = {tokens.shape[1]} exceeds limit ({max_length}). Truncating...\")\n",
        "        # Decode back to text, keeping only the first max_length tokens\n",
        "        truncated_tokens = tokens[:, :max_length]\n",
        "        return tokenizer.decode(truncated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "iuGbcbEME6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llama_generate(prompt, max_new_tokens=512):\n",
        "    \"\"\"Generate text using Llama 3 8B\"\"\"\n",
        "    # Truncate prompt to fit within model's context window\n",
        "    truncated_prompt = truncate_prompt_for_model(prompt, max_length=8000)\n",
        "\n",
        "    # Tokenize with proper padding and attention mask\n",
        "    inputs = tokenizer(\n",
        "        truncated_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=8192,\n",
        "        return_attention_mask=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DfPwA2xjE8Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent-specific prompts\n",
        "def get_extractor_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article\n",
        "and extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as Discussion, Conclusion, or\n",
        "Limitations. List each limitation verbatim, including direct quotes where possible, and provide a brief context (e.g., what aspect of\n",
        "the study the limitation pertains to). Ensure accuracy and avoid inferring or adding limitations not explicitly stated. If no limitations\n",
        "are mentioned, state this clearly.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "Plan: Outline which sections (e.g., Discussion, Conclusion, Limitations) to analyze and identify tools (e.g., text extraction) to\n",
        "access the article content. Justify the selection of sections based on their likelihood of containing limitation statements.\n",
        "\n",
        "Reasoning: Let's think step by step to ensure thorough and accurate extraction of limitations:\n",
        "Step 1: Identify all sections in the article that may contain limitations. For example, the Discussion often includes limitations as\n",
        "authors reflect on their findings, while a dedicated Limitations section is explicit.\n",
        "Step 2: Use text extraction tools to retrieve content from these sections. Verify that the content is complete and accurate.\n",
        "Step 3: Scan for explicit limitation statements, such as phrases like \"a limitation of this study\" or \"we acknowledge that.\"\n",
        "Document why each statement qualifies as a limitation.\n",
        "Step 4: For each identified limitation, extract the verbatim quote (if available) and note the context (e.g., related to sample size,\n",
        "methodology).\n",
        "Step 5: Check for completeness by reviewing other potential sections (e.g., Conclusion) to ensure no limitations are missed.\n",
        "Analyze: Use tools to extract and verify the article's content, focusing on explicit limitation statements. Cross-reference extracted\n",
        "quotes with the original text to ensure accuracy.\n",
        "\n",
        "Reflect: Verify that all relevant sections were checked and no limitations were missed. Consider whether any section might have been\n",
        "overlooked and re-evaluate if necessary.\n",
        "Continue: Do not terminate until all explicitly stated limitations are identified or confirmed absent.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Verbatim quote (if available), context (e.g., aspect of the study), and section reference.\n",
        "If none: \"No limitations explicitly stated in the article.\"\n",
        "\n",
        "Tool Use:\n",
        "Use text extraction tools to access and verify article content.\n",
        "Do not assume content; retrieve it directly from the provided article.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected the Discussion section because authors often discuss study constraints there.\"\n",
        "\"I found the phrase 'a limitation of this study' in the Limitations section, indicating an explicit limitation.\"\n",
        "\"I checked the Conclusion section to ensure no additional limitations were mentioned, confirming completeness.\"\n",
        "This narrative ensures transparency and justifies each decision in the extraction process.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Please extract and list the key limitations found in this paper. Be specific and provide clear reasoning for each limitation identified.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "v5FGdrPYE-_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analyzer_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze\n",
        "the provided scientific article and identify potential limitations not explicitly stated by the authors. Focus on aspects such as study\n",
        "design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions. For each inferred\n",
        "limitation, provide a clear explanation of why it is a limitation and how it impacts the study's validity, reliability,\n",
        "or generalizability. Ensure inferences are grounded in the article's content and avoid speculative assumptions.\n",
        "\n",
        "Workflow:\n",
        "Plan: Identify key areas (e.g., methodology, sample size, statistical analysis) to analyze and select tools (e.g., text analysis) to\n",
        "verify article details. Justify the selection based on their potential to reveal limitations.\n",
        "Reasoning: Let's think step by step to identify inferred limitations:\n",
        "\n",
        "Step 1: Review the article's methodology to identify gaps (e.g., study design flaws, sampling issues).\n",
        "Step 2: Use text analysis tools to extract relevant details (e.g., sample size, statistical methods).\n",
        "Step 3: Evaluate each area for potential limitations, such as small sample size affecting generalizability or unaddressed assumptions.\n",
        "Step 4: Document why each gap qualifies as a limitation and its impact on the study.\n",
        "Step 5: Ensure all key areas are covered to avoid missing potential limitations.\n",
        "Analyze: Critically evaluate the article, using tools to confirm content, and infer limitations based on methodological or analytical gaps.\n",
        "Reflect: Assess whether inferred limitations are grounded in the article and relevant to its validity, reliability, or generalizability.\n",
        "Re-evaluate overlooked areas if necessary.\n",
        "\n",
        "\n",
        "Continue: Iterate until all potential inferred limitations are identified.\n",
        "Output Format:\n",
        "Bullet points listing each inferred limitation.\n",
        "For each: Description, explanation, and impact on the study.\n",
        "\n",
        "Tool Use:\n",
        "Use text analysis tools to verify article content (e.g., methodology, results).\n",
        "Avoid assumptions; base inferences on retrieved content.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"The methodology section mentions a convenience sample, which may limit generalizability.\"\n",
        "\"The statistical analysis lacks adjustment for confounders, potentially affecting validity.\"\n",
        "\"I checked the results section to ensure no additional gaps were missed.\"\n",
        "This narrative ensures transparency and justifies each inferred limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Please provide a detailed analysis of the limitations in this research. Consider both obvious and subtle limitations that could affect the validity and applicability of the findings.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RBrJMU2BFBvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reviewer_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research.\n",
        "Your task is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "that might be raised in an open review process, considering common critiques such as reproducibility, transparency, generalizability,\n",
        "or ethical considerations. Leverage insights from similar studies or common methodological issues in the field by searching the web or\n",
        "X posts for context, if needed.\n",
        "\n",
        "Workflow:\n",
        "Plan: Identify areas for review (e.g., reproducibility, transparency, ethics) and plan searches for external context\n",
        "(e.g., similar studies, methodological critiques). Justify the selection based on peer review standards.\n",
        "Reasoning: Let's think step by step to identify peer-review limitations:\n",
        "\n",
        "Step 1: Select key areas for review (e.g., reproducibility, ethics) based on common peer review critiques.\n",
        "Step 2: Use text analysis tools to extract relevant article details (e.g., methods, data reporting).\n",
        "Step 3: Identify potential limitations, such as lack of transparency in data or ethical concerns, and justify using article content.\n",
        "Step 4: Search web/X for external context (e.g., similar studies) to support limitations, rating source relevance\n",
        "(high, medium, low, none).\n",
        "Step 5: Synthesize findings, ensuring limitations align with peer review standards and are supported by article or external context.\n",
        "Analyze: Critically review the article, integrating external context to identify limitations. Use tools to verify content and sources.\n",
        "Reflect: Verify that limitations align with peer review standards and are supported by the article or external context.\n",
        "Re-evaluate overlooked areas if necessary.\n",
        "\n",
        "Continue: Iterate until all relevant peer-review limitations are identified.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, why it's a concern, and alignment with peer review standards.\n",
        "Include citations for external sources in the format Source Name, if used.\n",
        "\n",
        "Tool Use:\n",
        "Use web/X search tools to find relevant literature or methodological critiques.\n",
        "Use text analysis tools to verify article content.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected reproducibility because peer reviewers often critique data availability.\"\n",
        "\"The article lacks a data sharing statement, which limits reproducibility.\"\n",
        "\"A web search revealed similar studies provide data openly, supporting this limitation.\"\n",
        "This narrative ensures transparency and justifies each identified limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Please provide a critical review identifying the limitations and areas of concern in this research. Consider what a peer reviewer would highlight as weaknesses or areas needing improvement.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DcnmDLYJFEcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_citation_prompt(cited_papers: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert scientific research assistant tasked with inferring potential limitations for an unspecified\n",
        "current scientific article based solely on its cited papers.\n",
        "You are given information from multiple cited papers, which are assumed to be referenced by the current article.\n",
        "Your goal is to analyze these cited works and identify possible limitations that the current paper may have, by\n",
        "comparing its presumed scope, methods, or results against the cited literature.\n",
        "Because the input paper itself is not provided, you must reason from the cited papers alone, identifying what\n",
        "gaps, stronger methods, broader coverage, or alternative results the cited works might expose in the hypothetical\n",
        "current paper that cites them.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Generate a list of scientifically grounded limitations that the current article might have, assuming it builds upon or is informed by the provided cited papers.\n",
        "\n",
        "Each limitation should:\n",
        "\n",
        "Be concise\n",
        "\n",
        "Reference the relevant cited paper(s) by title\n",
        "\n",
        "Clearly explain how the cited paper exposes a potential limitation\n",
        "\n",
        "Be plausible and insightful based on common scientific reasoning\n",
        "\n",
        "Workflow:\n",
        "Plan:\n",
        "Identify key insights, strengths, and scopes of the cited papers that could set a high bar or reveal blind spots\n",
        "in a hypothetical citing article.\n",
        "\n",
        "Reasoning: Let's think step by step to infer limitations:\n",
        "Review each cited paper to extract its methodology, findings, and scope.\n",
        "Ask: If a paper cited this work but did not adopt or address its insights, what limitation might arise?\n",
        "Identify where the cited paper offers better methodology, broader scope, or contradicting findings.\n",
        "Formulate each limitation as a plausible shortcoming of a hypothetical article that builds on—but possibly\n",
        "underutilizes—these cited works.\n",
        "\n",
        "Justify each limitation based on specific attributes of the cited paper (e.g., \"more comprehensive dataset\",\n",
        "\"stronger evaluation metric\", etc.)\n",
        "\n",
        "Analyze:\n",
        "Develop a set of inferred limitations, each tied to specific cited paper(s) and grounded in logical comparison.\n",
        "\n",
        "Reflect:\n",
        "Ensure coverage of all relevant cited papers and validate that each limitation is scientifically plausible in\n",
        "context.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, explanation, and reference to the cited paper(s) in the format Paper Title.\n",
        "\n",
        "Tool Use (if applicable):\n",
        "\n",
        "Use citation lookup tools or document content to extract accurate summaries.\n",
        "Do not assume details about the input paper—focus only on drawing limitations based on differences, omissions,\n",
        "or underuse of the cited works.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected [Paper X] because it uses a more robust method than the current article.\"\n",
        "\"The current article's simpler method may limit accuracy compared to [Paper X].\"\n",
        "\"I reviewed all cited papers to ensure no relevant gaps were missed.\"\n",
        "This narrative ensures transparency and justifies each identified limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please identify limitations that would be relevant for researchers who might cite this paper in future work.\n",
        "Consider what limitations future authors might mention when discussing this paper's contribution to the field,\n",
        "based on the cited papers context.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_HaaXzZqFHbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_theory_assumptions_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert auditor specializing in mathematical rigor, theoretical validity, and assumption scrutiny. Your tasks are to:\n",
        "\n",
        "Trace each formal claim from theoretical foundation to objective to algorithm, ensuring logical consistency.\n",
        "Verify definitions, assumptions, and properties (e.g., convexity, smoothness, identifiability) for correctness and applicability.\n",
        "Identify overly restrictive or unrealistic assumptions (e.g., applying finite-dimensional results to infinite-dimensional settings, VC bounds on large-scale models).\n",
        "Flag proof gaps, undefined notation, missing terms, or invalid generalizations (e.g., 2D to 3D, finite to infinite).\n",
        "\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Provide limitations about THEORY & ASSUMPTIONS for the following content. Use only facts from the text.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yhGTeL7EFJZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_baselines_novelty_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert auditor evaluating baseline comparisons, novelty, and comparative fairness. Your tasks are to:\n",
        "\n",
        "Assess baseline coverage and fairness, verifying equivalence in parameters, data, tuning, random seeds, and metrics.\n",
        "Ensure comparisons include state-of-the-art (SOTA) or standard methods relevant to the task (e.g., rate-distortion for codecs, SODA for specific settings).\n",
        "Detect discrepancies or outdated results compared to cited prior work.\n",
        "Evaluate novelty, identifying whether contributions are incremental, rebranded prior art, or truly novel, with evidence.\n",
        "For qualitative comparisons, verify like-for-like evidence and fairness.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Provide limitations about BASELINES, NOVELTY & COMPARABILITY for the following content.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZzcAz24fFJUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_robustness_failure_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert auditor focused on robustness, failure modes, and ablation studies. Your tasks are to:\n",
        "\n",
        "Identify missing tests for edge cases (e.g., noise, adversarial inputs, distribution shifts, extreme parameter limits, long sequences, or 3D leakage).\n",
        "Verify sensitivity analyses for normalization choices (e.g., batch normalization with small batches), key hyperparameters, and model components.\n",
        "Ensure inclusion of simple heuristic baselines in selector or scheduler settings.\n",
        "Confirm the presence of detailed failure case analyses with examples.\n",
        "\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Provide limitations about ROBUSTNESS, FAILURE MODES & ABLATIONS for the following content.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NqxlYsBRFLuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reproducibility_compute_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert auditor verifying reproducibility, computational claims, and scalability. Your tasks are to:\n",
        "\n",
        "Flag missing details on training/inference time, FLOPs, memory usage, hardware, batch sizes, random seeds, confidence intervals, dataset sizes, or code/data availability.\n",
        "Compare pipeline overheads (e.g., video-specific training, pruning) against simple baselines for efficiency claims.\n",
        "Evaluate scalability, checking if results are limited to small instances or include a cost model for larger scales.\n",
        "Flag unsubstantiated claims of “efficiency” or “faster” performance without quantitative evidence.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Provide limitations about REPRODUCIBILITY, COMPUTE & SCALABILITY for the following content.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PqasXWZpFLoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_presentation_clarity_prompt(paper_content: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert auditor ensuring clarity, presentation integrity, and impact discussion. Your tasks are to:\n",
        "\n",
        "Identify undefined terms, inconsistent notation, or mislabeled equations, figures, or tables.\n",
        "Verify the inclusion of sufficient qualitative evidence (e.g., multi-view 3D visualizations, long-sequence demonstrations).\n",
        "Flag issues in captions or legends (e.g., ambiguous labels, incorrect dataset references).\n",
        "Ensure discussion of societal, ethical, or fairness impacts when relevant to the task or application.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Provide limitations about PRESENTATION, CLARITY & IMPACT for the following content.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gIbK4EocFJOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_merger_prompt(extractor_output: str, analyzer_output: str, reviewer_output: str, citation_output: str,\n",
        "                     theory_output: str, baselines_output: str, robustness_output: str, reproducibility_output: str, presentation_output: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided by nine specialized agents:\n",
        "\n",
        "**Original Agents:**\n",
        "1. **Extractor** (explicit limitations from the article),\n",
        "2. **Analyzer** (inferred limitations from critical analysis),\n",
        "3. **Reviewer** (limitations from an open review perspective),\n",
        "4. **Citation** (limitations based on cited papers).\n",
        "5. **Theory & Assumptions** (mathematical correctness, assumptions, scope),\n",
        "6. **Baselines & Novelty** (comparative evaluation, novelty assessment),\n",
        "7. **Robustness & Failure Modes** (edge conditions, ablation studies),\n",
        "8. **Reproducibility & Compute** (reproducibility claims, scalability),\n",
        "9. **Presentation & Clarity** (clarity, integrity, impact discussion).\n",
        "\n",
        "**Goals**:\n",
        "1. Combine all limitations into a cohesive, non-redundant list.\n",
        "2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article's content.\n",
        "3. Prioritize critical limitations that affect the paper's validity and reproducibility.\n",
        "5. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report.\n",
        "\n",
        "**Workflow**:\n",
        "1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "2. **Analyze**: Combine limitations, prioritizing critical ones, and verify alignment with the article.\n",
        "3. **Reflect**: Check for completeness, scientific rigor, and clarity.\n",
        "4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "**Output Format**:\n",
        "- Numbered list of final limitations.\n",
        "\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Extractor Agent Analysis:\n",
        "{extractor_output}\n",
        "\n",
        "Analyzer Agent Analysis:\n",
        "{analyzer_output}\n",
        "\n",
        "Reviewer Agent Analysis:\n",
        "{reviewer_output}\n",
        "\n",
        "Citation Agent Analysis:\n",
        "{citation_output}\n",
        "\n",
        "Theory & Assumptions Agent Analysis:\n",
        "{theory_output}\n",
        "\n",
        "Baselines & Novelty Agent Analysis:\n",
        "{baselines_output}\n",
        "\n",
        "Robustness & Failure Modes Agent Analysis:\n",
        "{robustness_output}\n",
        "\n",
        "Reproducibility & Compute Agent Analysis:\n",
        "{reproducibility_output}\n",
        "\n",
        "Presentation & Clarity Agent Analysis:\n",
        "{presentation_output}\n",
        "\n",
        "Please merge these nine different perspectives on the paper's limitations into a comprehensive, well-organized analysis. Synthesize the insights, resolve any contradictions, and provide a unified view of the paper's limitations.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "XkVSC5A6FRim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run a specific agent\n",
        "def run_agent(agent_name: str, paper_content: str, agent_prompt_func, cited_papers: str = \"\") -> str:\n",
        "    \"\"\"Run a specific agent and return its output\"\"\"\n",
        "    print(f\"  Running {agent_name} agent...\")\n",
        "    try:\n",
        "        if agent_name == \"Citation\":\n",
        "            # For Citation agent, use only cited_papers, not paper_content\n",
        "            prompt = agent_prompt_func(cited_papers)\n",
        "        else:\n",
        "            prompt = agent_prompt_func(paper_content)\n",
        "        response = llama_generate(prompt, max_new_tokens=512)\n",
        "        print(f\"  {agent_name} agent completed\")\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"  Error in {agent_name} agent: {e}\")\n",
        "        return f\"ERROR in {agent_name} agent: {str(e)}\"\n",
        "\n",
        "# Load the DataFrame\n",
        "output_file1 = \"df_neurips_limitations_multi_agent_1.csv\"\n",
        "df1 = pd.read_csv(output_file1)\n",
        "\n",
        "\n",
        "print(\"Loading CSV file...\")\n",
        "try:\n",
        "    df = pd.read_csv(\"df_neruips_21_22_final.csv\")\n",
        "    print(f\"Successfully loaded CSV file with shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV file not found. Please check the file path.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV file: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Check if required columns exist\n",
        "required_columns = ['df_Abstract', 'df_Introduction', 'df_Related_Work', 'df_Methodology',\n",
        "                   'df_Dataset', 'df_Conclusion', 'df_Experiment_and_Results',\n",
        "                   'LLM_extracted_future_work', 'relevance_8_cited_in', 'relevance_8_cited_by']\n",
        "\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "# if missing_columns:\n",
        "#     print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "#     print(f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Create combined content column\n",
        "df['combined'] = df.apply(\n",
        "    lambda row: (\n",
        "        f\"Abstract: {row.get('df_Abstract', '')}\\n\"\n",
        "        f\"Introduction: {row.get('df_Introduction', '')}\\n\"\n",
        "        f\"Related_Work: {row.get('df_Related_Work', '')}\\n\"\n",
        "        f\"Methodology: {row.get('df_Methodology', '')}\\n\"\n",
        "        f\"Dataset: {row.get('df_Dataset', '')}\\n\"\n",
        "        f\"Conclusion: {row.get('df_Conclusion', '')}\\n\"\n",
        "        f\"Experiment_and_Results: {row.get('df_Experiment_and_Results', '')}\\n\"\n",
        "        f\"LLM_extracted_future_work: {row.get('LLM_extracted_future_work', '')}\\n\"\n",
        "        f\"Extra1: {row.get('df_col_2', '')}\\n\"\n",
        "        f\"Extra2: {row.get('df_col_3', '')}\\n\"\n",
        "        f\"Extra3: {row.get('df_col_4', '')}\\n\"\n",
        "        f\"Extra4: {row.get('df_col_5', '')}\\n\"\n",
        "        f\"Extra5: {row.get('df_col_6', '')}\\n\"\n",
        "        f\"Extra6: {row.get('df_col_7', '')}\\n\"\n",
        "        f\"Extra7: {row.get('df_col_8', '')}\"\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(\"Processing all samples with multiple agents...\")\n",
        "\n",
        "# Initialize new columns for each agent\n",
        "\n",
        "df['limitations_extractor'] = \"\"\n",
        "df['limitations_analyzer'] = \"\"\n",
        "df['limitations_reviewer'] = \"\"\n",
        "df['limitations_citation_only'] = \"\"\n",
        "df['limitations_theory_assumptions'] = \"\"\n",
        "df['limitations_baselines_novelty'] = \"\"\n",
        "df['limitations_robustness_failure'] = \"\"\n",
        "df['limitations_reproducibility_compute'] = \"\"\n",
        "df['limitations_presentation_clarity'] = \"\"\n",
        "df['limitations_merged_final'] = \"\""
      ],
      "metadata": {
        "id": "cnD3tORcFRcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY6T6TEPEcWk"
      },
      "outputs": [],
      "source": [
        "# Process each row with all agents\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    global_current_row = i + 1  # Update global counter\n",
        "    global_df = df  # Update global DataFrame reference\n",
        "\n",
        "    print(f\"\\n=== Processing row {i+1}/{len(df)} ===\")\n",
        "    row = df.iloc[i]\n",
        "    paper_content = row['combined']\n",
        "\n",
        "    # Get cited papers information for citation agent\n",
        "    cited_in = row.get('relevance_8_cited_in', '')\n",
        "    # print(\"cited_in\", cited_in)\n",
        "    cited_by = row.get('relevance_8_cited_by', '')\n",
        "    # print(\"cited_by\", cited_by)\n",
        "    cited_papers = f\"Papers cited by this article:\\n{cited_in}\\n\\nPapers that cited this article:\\n{cited_by}\"\n",
        "\n",
        "    # Run existing agents (using stored results)\n",
        "    extractor_output = run_agent(\"Extractor\", paper_content, get_extractor_prompt)\n",
        "    analyzer_output = run_agent(\"Analyzer\", paper_content, get_analyzer_prompt)\n",
        "    reviewer_output = run_agent(\"Reviewer\", paper_content, get_reviewer_prompt)\n",
        "    citation_output = run_agent(\"Citation\", \"\", get_citation_prompt, cited_papers)\n",
        "    theory_output = run_agent(\"Theory\", paper_content, get_theory_assumptions_prompt)\n",
        "    baselines_output = run_agent(\"Baselines\", paper_content, get_baselines_novelty_prompt)\n",
        "    robustness_output = run_agent(\"Robustness\", paper_content, get_robustness_failure_prompt)\n",
        "    reproducibility_output = run_agent(\"Reproducibility\", paper_content, get_reproducibility_compute_prompt)\n",
        "    presentation_output = run_agent(\"Presentation\", paper_content, get_presentation_clarity_prompt)\n",
        "\n",
        "    # Store individual agent outputs\n",
        "    df.at[i,'limitations_extractor'] = extractor_output\n",
        "    df.at[i, 'limitations_analyzer'] = analyzer_output\n",
        "    df.at[i, 'limitations_reviewer'] = reviewer_output\n",
        "    df.at[i, 'limitations_citation_only'] = citation_output\n",
        "    df.at[i, 'limitations_theory_assumptions'] = theory_output\n",
        "    df.at[i, 'limitations_baselines_novelty'] = baselines_output\n",
        "    df.at[i, 'limitations_robustness_failure'] = robustness_output\n",
        "    df.at[i, 'limitations_reproducibility_compute'] = reproducibility_output\n",
        "    df.at[i, 'limitations_presentation_clarity'] = presentation_output\n",
        "\n",
        "    # Merge all agent outputs\n",
        "    print(f\"  Running Master Coordinator agent...\")\n",
        "    try:\n",
        "        merger_prompt = get_merger_prompt(extractor_output, analyzer_output, reviewer_output, citation_output,\n",
        "                                         theory_output, baselines_output, robustness_output, reproducibility_output, presentation_output)\n",
        "        merged_output = llama_generate(merger_prompt, max_new_tokens=512)\n",
        "        df.at[i, 'limitations_merged_final'] = merged_output.strip()\n",
        "        print(\"limitations_merged_final\", df.at[i, 'limitations_merged_final'])\n",
        "        print(f\"  Master Coordinator agent completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error in Master Coordinator agent: {e}\")\n",
        "        df.at[i, 'limitations_merged_final'] = f\"ERROR in Master Coordinator agent: {str(e)}\"\n",
        "\n",
        "    # print(\"limitations_merged_final\", df.at[i, 'limitations_merged_final'])\n",
        "    print(f\"  Row {i+1} completed\")\n",
        "\n",
        "    # Save progress every 5 rows to prevent data loss\n",
        "    if i % 5 == 0:\n",
        "        output_file = \"df_neurips_limitations_multi_agent_9_agents.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"  ✅ Checkpoint saved at row {i+1}\")\n",
        "\n",
        "# Save results\n",
        "output_file = \"df_neurips_limitations_multi_agent_9_agents.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "print(f\"\\nScript started at: {time.ctime(start_time)}\")\n",
        "print(f\"Script ended at:   {time.ctime(end_time)}\")\n",
        "print(f\"Total elapsed time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "cfeKV06TDX2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth Coverage"
      ],
      "metadata": {
        "id": "xS-ZREZpGicO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making lists of list 'master_agent' text\n",
        "import re\n",
        "\n",
        "# make sure the output column exists\n",
        "df['limitations_merged_final_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"limitations_merged_final\"]\n",
        "    # skip if missing or not a string\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'limitations_merged_final_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'limitations_merged_final_list'] = lim_list\n"
      ],
      "metadata": {
        "id": "i-YZE2sB8ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5_JG1h8Zg6"
      },
      "outputs": [],
      "source": [
        "# making lists of list 'ground truth' text\n",
        "\n",
        "import re\n",
        "\n",
        "# ensure the output column exists\n",
        "df['Lim_and_OR_ground_truth_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"Lim_and_OR_ground_truth_final\"]\n",
        "    # skip non-strings\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, flags=re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making combinations from 'ground truth' and llm generated text'\n",
        "\n",
        "df['combined'] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Generate combinations for each row\n",
        "for i in range(len(df)):\n",
        "    combined_list = []\n",
        "    list1 = df[\"Lim_and_OR_ground_truth_list\"][i]\n",
        "    list2 = df[\"limitations_merged_final_list\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df.at[i, 'combined'] = combined_list  # Truncate if needed"
      ],
      "metadata": {
        "id": "Q4BL139r8w3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "# from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "import json\n",
        "\n",
        "llm_results = []\n",
        "df['LLM_eval'] = ''\n",
        "for idx in range(len(df)): # len(df)\n",
        "    print(\"idx is\",idx)\n",
        "    pairs = df.at[idx, 'combined']   # assume this is List[Tuple[list, list]]\n",
        "    if not isinstance(pairs, list) or not pairs:\n",
        "        llm_results.append(None)\n",
        "        continue\n",
        "\n",
        "    # build the named-pairs block in one go\n",
        "    formatted = \"\\n\".join(\n",
        "        f\"Pair {i+1}:\\n  List1: {first}\\n  List2: {second}\"\n",
        "        for i, (first, second) in enumerate(pairs)\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"For each of the following pairs, answer “Yes” if List1 contains a topic or limitation\\n\"\n",
        "        \"from List2, or List2 contains a topic or limitation from from List1; otherwise answer “No”.\\n\"\n",
        "        \"Respond *only* with a JSON object mapping each Pair name to “Yes” or “No”.\\n\\n\"\n",
        "        \"Pairs:\\n\"\n",
        "        f\"{formatted}\"\n",
        "    )\n",
        "\n",
        "    # single call per row\n",
        "    resp_text = run_critic_openai(prompt)\n",
        "    llm_results.append(resp_text)\n",
        "\n",
        "    df.at[idx, 'LLM_eval'] = resp_text\n"
      ],
      "metadata": {
        "id": "vSA2C5Q58wxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# extract all 'Yes', 'No'\n",
        "pattern = r'\"Pair\\s*\\d+\"\\s*:\\s*\"(Yes|No)\"'\n",
        "\n",
        "all_matches = []\n",
        "for idx in range(len(df)):\n",
        "    raw = df.at[idx, 'LLM_eval']\n",
        "    if not isinstance(raw, str):\n",
        "        all_matches.append([])\n",
        "        continue\n",
        "    matches = re.findall(pattern, raw)\n",
        "    all_matches.append(matches)\n"
      ],
      "metadata": {
        "id": "-QPgh-ML8wrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for idx, tuples in df['combined'].items():\n",
        "    if not isinstance(tuples, list):\n",
        "        continue\n",
        "    # get the matching list for this row\n",
        "    matches = all_matches[idx] if idx < len(all_matches) else []\n",
        "\n",
        "    for j, (list1, list2) in enumerate(tuples):\n",
        "        # grab the j-th match or None if out of range\n",
        "        is_match = matches[j] if j < len(matches) else None\n",
        "\n",
        "        rows.append({\n",
        "            'source_row': idx,\n",
        "            'List1':      list1,\n",
        "            'List2':      list2,\n",
        "            'is_match':   is_match\n",
        "        })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "\n",
        "result_df.rename(\n",
        "    columns={\n",
        "        'List1': 'Ground_Truth',\n",
        "        'List2': 'LLM_generated'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "coYkymR69RH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_leading_number(x):\n",
        "    \"\"\"\n",
        "    If x is a list, grab its first element; then:\n",
        "    • If it’s an int, return it.\n",
        "    • If it’s a string starting with digits (with or without a dot), return those digits.\n",
        "    Otherwise return None.\n",
        "    \"\"\"\n",
        "    # step 1: if it’s a list, pull out the first item\n",
        "    val = x[0] if isinstance(x, list) and x else x\n",
        "\n",
        "    # step 2: if it’s already an int, just return it\n",
        "    if isinstance(val, int):\n",
        "        return val\n",
        "\n",
        "    # step 3: if it’s a string, regex for leading digits\n",
        "    if isinstance(val, str):\n",
        "        # match “123.” or just “123”\n",
        "        m = re.match(r'^\\s*(\\d+)(?:\\.)?', val)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "# extract into new columns\n",
        "result_df['gt_number']        = result_df['Ground_Truth'].apply(extract_leading_number)\n",
        "result_df['llm_gen_number']   = result_df['LLM_generated'].apply(extract_leading_number)\n"
      ],
      "metadata": {
        "id": "Qzxu5do09RCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth coverage\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "match = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in result_df.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['gt_number'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            match += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['gt_number']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    match += 1\n",
        "print(match)\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = result_df['Ground_Truth'].ne(result_df['Ground_Truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "ck = result_df.groupby(unique_blocks)['gt_number'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(ck)\n"
      ],
      "metadata": {
        "id": "sTscp5vO9Q7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Quality bewtween matched pairs (NLP based metrics)"
      ],
      "metadata": {
        "id": "jCYma4Kk-9fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: ground_truth || LLM_Generated limitation: llm_generated"
      ],
      "metadata": {
        "id": "3oeLB_NN_lpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# say you want to rename 'oldA'→'newA' and 'oldB'→'newB'\n",
        "df.rename(columns={\n",
        "    'Ground_Truth': 'ground_truth',\n",
        "    'LLM_generated': 'llm_generated',\n",
        "    # 'Is_same': 'is_match',\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "iBF9CAs3Tn-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the column 'is_match' is 'no'\n",
        "df_filtered = df[df['is_match'].str.lower() != 'no']"
      ],
      "metadata": {
        "id": "3kCqlOU0Tn-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WQOUxnfuTn-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore (all)\n",
        "!pip3 -q install bert-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d89e97-82cc-4988-8a2f-0aa4714969ab",
        "id": "oH3IU2CYTn-P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTScor for whole texts"
      ],
      "metadata": {
        "id": "hF9cjY0TTn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row using one loop\n",
        "def calculate_bertscore(row):\n",
        "    # Calculate BERT Scores directly for the ground_truth and llm_generated of the row\n",
        "    _, _, F1 = scorer.score([row['ground_truth']], [row['llm_generated']])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df_filtered['bert_score'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ],
      "metadata": {
        "id": "fWW1VgFcENca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the 'bert_score' column in df_highest_score\n",
        "average_bert_score = df_filtered['bert_score'].mean()\n",
        "\n",
        "# Display the average\n",
        "average_bert_score\n"
      ],
      "metadata": {
        "id": "DS-k2i6hEPgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rouge_score"
      ],
      "metadata": {
        "id": "emuvZfNmERVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate similarity metrics for each row\n",
        "def calculate_metrics(row):\n",
        "    metrics = {}\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(row['ground_truth'], row['llm_generated'])\n",
        "    metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
        "    metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
        "    metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Cosine Similarity\n",
        "    vectorizer = CountVectorizer().fit_transform([row['ground_truth'], row['llm_generated']])\n",
        "    vectors = vectorizer.toarray()\n",
        "    metrics['cosine_similarity'] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set1 = set(row['ground_truth'].split())\n",
        "    set2 = set(row['llm_generated'].split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    metrics['jaccard_similarity'] = intersection / union if union > 0 else 0\n",
        "\n",
        "    # BLEU Score\n",
        "    metrics['bleu_score'] = sentence_bleu([row['ground_truth'].split()], row['llm_generated'].split())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Apply the function to each row in the DataFrame and store results in new columns\n",
        "metric_results = df_filtered.apply(calculate_metrics, axis=1)\n",
        "\n",
        "# Expand the dictionary into separate columns\n",
        "metric_results_df = pd.DataFrame(metric_results.tolist())\n",
        "df_filtered = pd.concat([df_filtered, metric_results_df], axis=1)\n"
      ],
      "metadata": {
        "id": "_y2fbOzaA1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of each metric\n",
        "average_metrics = {\n",
        "    'Average ROUGE-1': df_filtered['rouge1'].mean(),\n",
        "    'Average ROUGE-2': df_filtered['rouge2'].mean(),\n",
        "    'Average ROUGE-L': df_filtered['rougeL'].mean(),\n",
        "    'Average Cosine Similarity': df_filtered['cosine_similarity'].mean(),\n",
        "    'Average Jaccard Similarity': df_filtered['jaccard_similarity'].mean(),\n",
        "    'Average BLEU Score': df_filtered['bleu_score'].mean()\n",
        "}\n",
        "\n",
        "# Print the average metrics\n",
        "average_metrics\n"
      ],
      "metadata": {
        "id": "x_JC28oCA3-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic similarity"
      ],
      "metadata": {
        "id": "qY1r5p35Tn-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install keybert\n",
        "\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Ensure all entries are strings (even if NaN)\n",
        "df_filtered['ground_truth'] = df_filtered['ground_truth'].fillna(\"\").astype(str)\n",
        "df_filtered['llm_generated'] = df_filtered['llm_generated'].fillna(\"\").astype(str)\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    return [kw[0] for kw in keywords]  # Extract just the keywords\n",
        "\n",
        "# Apply KeyBERT to 'ground_truth' and 'LLM_generated' columns\n",
        "df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "id": "D_un-kmDBCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard Similarity\n",
        "def jaccard_similarity(row):\n",
        "    set1 = set(row['ground_truth_words'])\n",
        "    set2 = set(row['LLM_generated_words'])\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Apply Jaccard Similarity to each row\n",
        "df_filtered['jaccard_similarity_topic'] = df_filtered.apply(jaccard_similarity, axis=1)\n",
        "df_filtered['jaccard_similarity_topic'].mean()"
      ],
      "metadata": {
        "id": "FZ9I8AWzTn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute Cosine Similarity with empty check\n",
        "def cosine_sim(row):\n",
        "    gt = ' '.join(row['ground_truth_words'])\n",
        "    llm = ' '.join(row['LLM_generated_words'])\n",
        "\n",
        "    # If either is empty, return 0 similarity\n",
        "    if not gt.strip() or not llm.strip():\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer().fit_transform([gt, llm])\n",
        "        vectors = vectorizer.toarray()\n",
        "        return cosine_similarity(vectors)[0, 1]\n",
        "    except ValueError:\n",
        "        return 0.0  # fallback if vocabulary is still empty\n",
        "\n",
        "df_filtered['cosine_similarity_topic'] = df_filtered.apply(cosine_sim, axis=1)\n",
        "mean_sim = df_filtered['cosine_similarity_topic'].mean()\n",
        "print(mean_sim)\n"
      ],
      "metadata": {
        "id": "AC4rW-7VA-Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract text between double asterisks\n",
        "def extract_text_between_asterisks(text):\n",
        "    matches = re.findall(r'\\*\\*(.*?)\\*\\*', text)\n",
        "    return matches\n",
        "\n",
        "# Apply the function to both columns and store results in new columns\n",
        "df_filtered['ground_truth_extracted'] = df_filtered['ground_truth'].apply(extract_text_between_asterisks)\n",
        "df_filtered['llm_generated_extracted'] = df_filtered['llm_generated'].apply(extract_text_between_asterisks)\n"
      ],
      "metadata": {
        "id": "cq4Ekh7-Tn-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}