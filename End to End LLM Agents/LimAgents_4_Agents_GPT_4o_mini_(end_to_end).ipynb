{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRNwv0JYdHa4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"df_neruips_21_22_final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the column names to concatenate\n",
        "cols_to_concat = [\n",
        "    \"neurips_Abstract\",\n",
        "    \"neurips_Introduction\",\n",
        "    \"neurips_Related_Work\",\n",
        "    \"neurips_Methodology\",\n",
        "    \"neurips_Dataset\",\n",
        "    \"neurips_Conclusion\",\n",
        "    \"neurips_Experiment_and_Results\",\n",
        "    \"neurips_Extra\"\n",
        "]\n",
        "\n",
        "# Create a new column 'response_string_neurips' with labeled concatenation\n",
        "def concat_with_labels(row):\n",
        "    parts = []\n",
        "    for col in cols_to_concat:\n",
        "        if isinstance(row.get(col), str) and row[col].strip():\n",
        "            label = col.replace(\"neurips_\", \"\").replace(\"_\", \" \")\n",
        "            parts.append(f\"{label}: {row[col].strip()}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "df[\"response_string_neurips\"] = df.apply(concat_with_labels, axis=1)"
      ],
      "metadata": {
        "id": "bbYaCKCDt6rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "import time\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "p-34FDlCt6oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "# Tokenization setup\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "max_tokens = 128000\n",
        "\n",
        "def truncate_to_max_tokens(text: str, max_length: int) -> str:\n",
        "    tokens = encoding.encode(text)\n",
        "    return encoding.decode(tokens[:max_length]) if len(tokens) > max_length else text"
      ],
      "metadata": {
        "id": "u6apKIbruAbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Extractor = '''You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article and\n",
        "extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as the Discussion, Conclusion.\n",
        "List each limitation verbatim, including direct quotes where possible, and provide\n",
        "a brief context (e.g., what aspect of the study the limitation pertains to). Ensure accuracy and avoid inferring or adding\n",
        "limitations not explicitly stated. If no limitations are mentioned, state this clearly. Output your findings in a structured\n",
        "format with bullet points.\\n\\n'''\n",
        "Analyzer = '''You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze the\n",
        "        provided scientific article and identify potential limitations that are not explicitly stated by the authors. Focus on aspects\n",
        "        such as study design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions.\n",
        "        For each inferred limitation, provide a clear explanation of why it is a limitation and how it impacts the study’s validity,\n",
        "        reliability, or generalizability. Ensure your inferences are grounded in the article’s content and avoid speculative assumptions.\n",
        "        Output your findings in a structured format with bullet points, including a brief justification for each limitation.\\n\\n'''\n",
        "\n",
        "Reviewer = '''You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research. Your task\n",
        "        is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "        that might be raised in an open review process, considering common critiques such as reproducibility, transparency,\n",
        "        generalizability, or ethical considerations. If possible, leverage insights from similar studies or common methodological\n",
        "        issues in the field (search the web or X posts if needed for context). For each limitation, explain why it would be a\n",
        "        concern in an open review and how it aligns with peer review standards. Output your findings in a structured format with\n",
        "        bullet points, ensuring each limitation is relevant to the article’s content.:\\n\\n'''\n",
        "\n",
        "Citation = ''' You are an expert scientific research assistant tasked with generating limitations for a scientific article based on information from its cited papers. Use the provided cited paper information as broader context to inform your analysis. Your goal is to identify limitations for the current paper, focusing on shortcomings that align with the findings, methods, or scope of the cited papers. Prioritize limitations that could strengthen the current paper’s discussion or guide future research. Ensure limitations are concise, scientifically grounded, and tied to the cited papers’ context.\n",
        "\n",
        "Workflow:\n",
        "Plan: Identify relevant cited papers and plan how their findings/methods relate to the current article. Justify the selection based on potential gaps or differences.\n",
        "Reasoning: Let’s think step by step to identify limitations:\n",
        "\n",
        "Step 1: Review the cited papers to determine their relevance to the current article’s findings, methods, or scope.\n",
        "Step 2: Use citation lookup tools to extract key details from cited papers (e.g., methodology, results).\n",
        "Step 3: Identify gaps or differences between cited papers and the current article (e.g., advanced methods not adopted).\n",
        "Step 4: Document each limitation, explaining how it stems from the cited papers and its relevance to the current study.\n",
        "Step 5: Ensure all relevant cited papers are analyzed to capture potential limitations.\n",
        "Analyze: Identify limitations based on gaps or differences between the cited papers and the current article, using tools to verify content.\n",
        "Reflect: Ensure limitations are grounded in cited paper content and relevant to the current study. Re-evaluate overlooked papers if necessary.\n",
        "\n",
        "Continue: Iterate until all relevant limitations are identified.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, explanation, and reference to the cited paper(s) in the format Paper Title.\n",
        "\n",
        "Tool Use:\n",
        "Use citation lookup tools to access and verify cited paper content.\n",
        "Avoid assumptions; base limitations on retrieved cited paper information.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "“I selected [Paper X] because it uses a more robust method than the current article.”\n",
        "“The current article’s simpler method may limit accuracy compared to [Paper X].”\n",
        "“I reviewed all cited papers to ensure no relevant gaps were missed.”\n",
        "This narrative ensures transparency and justifies each identified limitation. '''"
      ],
      "metadata": {
        "id": "gW1DD_x-uAWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# regeenrate\n",
        "Regenerate_PROMPT = '''\n",
        "\n",
        "You are tasked with generating limitations based on feedback from the Judge Agent.\n",
        "Feedback Structure:\n",
        "Strengths: [strengths]\n",
        "Issues: [issues]\n",
        "Suggestions: [suggestions]\n",
        "Task: Create a set of limitations that:\n",
        "\n",
        "Builds upon the identified strengths to reinforce positive aspects.\n",
        "Minimizes the impact of issues by addressing them constructively.\n",
        "Incorporates suggestions to ensure actionable improvements.\n",
        "Ensure the limitations are clear, concise, and aligned with your role as [specify role, e.g., a content generator, analyst, etc.].'''\n",
        "\n"
      ],
      "metadata": {
        "id": "RfE6khHtuwLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lim gen\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"\\nProcessing row {i}\")\n",
        "    extractor_input = extractor_prompt + df['response_string_neurips'][i]\n",
        "    analyzer_input = analyzer_prompt + df['response_string_neurips'][i]\n",
        "    reviewer_input = reviewer_prompt + df['response_string_neurips'][i]\n",
        "\n",
        "    extractor_agent = azure_run_critic(extractor_input)\n",
        "    analyzer_agent = azure_run_critic(analyzer_input)\n",
        "    reviewer_agent = azure_run_critic(reviewer_input)\n",
        "\n",
        "    df.at[i, \"extractor_agent\"]  = extractor_agent\n",
        "    df.at[i, \"analyzer_agent\"]  = analyzer_agent\n",
        "    df.at[i, \"reviewer_agent\"]  = reviewer_agent\n"
      ],
      "metadata": {
        "id": "YfzUd39proRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# citation agent\n",
        "import re\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Tokenization setup\n",
        "encoding   = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "max_tokens = 128000\n",
        "\n",
        "def truncate_to_max_tokens(text: str, max_length: int) -> str:\n",
        "    tokens = encoding.encode(text)\n",
        "    return encoding.decode(tokens[:max_length]) if len(tokens) > max_length else text\n",
        "\n",
        "# Make sure the output column exists\n",
        "df['citation_agent_in_by_8'] = ''\n",
        "\n",
        "# Process each row\n",
        "for i in range(len(df)):\n",
        "    print(f\"Processing row {i}...\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # 1) Collect all items from relevance_8_cited_in\n",
        "    cited_in_list = row.get('relevance_8_cited_in', []) or []\n",
        "    cited_in_texts = []\n",
        "    for itm in cited_in_list:\n",
        "        # if it's a dict with 'text' key, grab that, otherwise str(itm)\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_in_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_in_texts.append(str(itm))\n",
        "\n",
        "    # 2) Collect all items from relevance_8_cited_by\n",
        "    cited_by_list = row.get('relevance_8_cited_by', []) or []\n",
        "    cited_by_texts = []\n",
        "    for itm in cited_by_list:\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_by_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_by_texts.append(str(itm))\n",
        "\n",
        "    # 3) Build the combined prompt section\n",
        "    cited_in_block = \"\\n\".join(cited_in_texts)\n",
        "    cited_by_block = \"\\n\".join(cited_by_texts)\n",
        "\n",
        "    combined_cited_input = (\n",
        "        \"Referenced papers:\\n\" + cited_in_block +\n",
        "        \"\\n\\nPapers who cited this paper:\\n\" + cited_by_block\n",
        "    )\n",
        "\n",
        "    input_paper = df['response_string_neurips'][i]\n",
        "    prompt = (\n",
        "        \"You are an assistant tasked to generate limitations or shortcomings \"\n",
        "        \"in a scientific article. Below is the input paper:\\n\"\n",
        "        f\"{input_paper}\\n\\n\"\n",
        "        \" Below is the relevant text from both the papers \"\n",
        "        \"that this article cites and those that cite it.\\n\\n\"\n",
        "        f\"{combined_cited_input}\\n\\n\"\n",
        "        \"Please generate limitations based on this information.\"\n",
        "    )\n",
        "\n",
        "    # 5) Truncate and call LLM\n",
        "    truncated = truncate_to_max_tokens(prompt, max_tokens)\n",
        "    try:\n",
        "        llm_summary = azure_run_critic(truncated)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        llm_summary = \"ERROR\"\n",
        "\n",
        "    df.at[i, \"citation_agent_in_by_8\"] = llm_summary\n",
        "\n",
        "df.to_csv(\"df_neruips_21_22_final.csv\",index=False)"
      ],
      "metadata": {
        "id": "ZfwCmhk4uKMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n"
      ],
      "metadata": {
        "id": "WI6kQLpQroL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COORDINATOR_PROMPT = '''\n",
        "    You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided\n",
        "    by four agents:\n",
        "    1. **Extractor** (explicit limitations from the article),\n",
        "    2. **Analyzer** (inferred limitations from critical analysis),\n",
        "    3. **Reviewer** (limitations from an open review perspective),\n",
        "    4. **Citation** (limitations based on cited papers).\n",
        "\n",
        "    **Goals**:\n",
        "    1. Combine all limitations into a cohesive, non-redundant list.\n",
        "    2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article’s content.\n",
        "    3. Prioritize author-stated limitations, supplementing with inferred, peer-review, or citation-based limitations if they add value.\n",
        "    4. Resolve discrepancies between agents’ outputs by cross-referencing the article and cited papers, using tools to verify content.\n",
        "    5. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report, with citations for external sources.\n",
        "\n",
        "    **Workflow** (inspired by SYS_PROMPT_SWEBENCH):\n",
        "    1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "    2. **Analyze**: Combine limitations, prioritizing author-stated ones, and verify alignment with the article.\n",
        "    3. **Reflect**: Check for completeness, scientific rigor, and clarity; resolve discrepancies using article content or tools.\n",
        "    4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "    **Output Format**:\n",
        "    - Numbered list of final limitations.\n",
        "    - For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived], [Cited-papers]).\n",
        "    - Include citations for external sources (e.g., web/X posts, cited papers) in the format [Source Name](ID).\n",
        "    **Tool Use**:\n",
        "    - Use text extraction tools to verify article content.\n",
        "    - Use citation lookup tools to cross-reference cited papers.\n",
        "    - Use web/X search tools to resolve discrepancies involving external context.\n",
        "\n",
        "    **Input**: '''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# taking the self-feedback if it exists otherwise acutal one\n",
        "# def master_agent(extractor_text, analyzer_text, reviewer_text, citation_text):\n",
        "def master_agent(extractor_text, analyzer_text,reviewer_text,citation_text):\n",
        "    \"\"\"\n",
        "    Takes the outputs of the four specialized agents and produces\n",
        "    the final coordinated limitations via a GPT call.\n",
        "    \"\"\"\n",
        "    coord_prompt = (\n",
        "        COORDINATOR_PROMPT\n",
        "        + f\"**Extractor Agent**:\\n{extractor_text}\\n\\n\"\n",
        "        + f\"**Analyzer Agent**:\\n{analyzer_text}\\n\\n\"\n",
        "        + f\"**Reviewer Agent**:\\n{reviewer_text}\\n\\n\"\n",
        "        + f\"**Citation Agent**:\\n{citation_text}\\n\\n\"\n",
        "        # + f\"**Image Agent**:\\n{image_text}\\n\\n\"\n",
        "        + \"Produce a single, numbered list of final limitations, noting each source in brackets.\"\n",
        "    )\n",
        "    return azure_run_critic(coord_prompt)\n",
        "\n",
        "# Example: Create a new column to store the output\n",
        "results = []\n",
        "\n",
        "df['master_agent_ext_analy_rev_cit_with_rel'] = ''\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    extractor_text = df.at[i, 'extractor_agent']\n",
        "    analyzer_text  = df.at[i, 'analyzer_agent']\n",
        "    reviewer_text  = df.at[i, 'reviewer_agent']\n",
        "    citation_text  = df.at[i, 'selected_cited_text_related_with_input_lim']\n",
        "    # image_text     = df_image.at[i, 'image_limitations']\n",
        "\n",
        "    try:\n",
        "        result = master_agent(extractor_text, analyzer_text, reviewer_text, citation_text)\n",
        "        # result = master_agent(extractor_text, analyzer_text,reviewer_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        result = \"ERROR\"\n",
        "\n",
        "    df.at[i,'master_agent_ext_analy_rev_cit_with_rel'] = result\n",
        "    results.append(result)\n",
        "\n",
        "# Add results back to df\n",
        "df.to_csv(\"df_neruips_21_22_final.csv\",index=False)"
      ],
      "metadata": {
        "id": "eXn6g9gEnRCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Judge and Self Feedback"
      ],
      "metadata": {
        "id": "FvxH03sNvKYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JUDGE_PROMPT = ''' You are a Judge Agent, an expert in evaluating scientific text quality with a focus on limitation generation for\n",
        "scientific articles. Your task is to assess the outputs of four agents—Extractor (explicit limitations from the article), Analyzer\n",
        "(inferred limitations from critical analysis), Reviewer (peer-review limitations), and Citation (limitations based on cited papers).\n",
        "For each agent’s output, assign a numerical score (0–100) and provide specific feedback based on defined criteria. The evaluation is\n",
        "reference-free, relying on the output’s inherent quality and alignment with each agent’s role.\n",
        "\n",
        "Evaluation Criteria:\n",
        "Depth: How critical and insightful is the limitation? Does it reveal significant issues in the study’s design, findings, or implications?\n",
        "(20% weight)\n",
        "\n",
        "\n",
        "\n",
        "Originality: Is the limitation a generic critique or a novel, context-specific insight? (20% weight)\n",
        "\n",
        "Actionability: Can researchers realistically address the limitation in future work? Does it provide clear paths for improvement?\n",
        "(30% weight)\n",
        "\n",
        "Topic Coverage: How broadly does the set of limitations cover relevant aspects (e.g., methodology, scope for Extractor/Analyzer; peer\n",
        "review standards for Reviewer; cited paper gaps for Citation)? (30% weight)\n",
        "\n",
        "Workflow: Plan: Review each agent’s role and expected output (Extractor: explicit limitations; Analyzer: inferred methodological gaps;\n",
        "Reviewer: peer-review critiques; Citation: cited paper gaps). Identify tools (e.g., text analysis, citation lookup) to verify content\n",
        "if needed.\n",
        "\n",
        "Reasoning: Let’s think step by step to evaluate each output: Step 1: Read the agent’s output and confirm its alignment with the agent’s\n",
        "role. Step 2: Assess each criterion (Depth, Originality, Actionability, Topic Coverage), noting strengths and weaknesses. Step 3: Assign\n",
        "a score (0–10) for each criterion based on quality, then calculate the weighted total (0–100). Step 4: Generate feedback for each\n",
        "criterion, specifying what was done well and what needs improvement. Step 5: Verify the evaluation by cross-checking with the article\n",
        "or cited papers using tools, if necessary.\n",
        "\n",
        "Analyze: Use tools to verify article or cited paper content to ensure accurate evaluation (e.g., confirm Extractor’s quotes, Citation’s\n",
        "references). Reflect: Ensure the score and feedback are fair, consistent, and actionable. Re-evaluate if any criterion seems misjudged.\n",
        "Continue: Iterate until the evaluation is complete for all agents.\n",
        "\n",
        "Tool Use: Use text analysis tools to verify article content (e.g., Extractor’s quotes, Analyzer’s methodology). Use citation lookup\n",
        "tools to confirm cited paper details (e.g., Citation’s references). Use web/X search tools to validate Reviewer’s external context,\n",
        "if needed.\n",
        "\n",
        "Chain of Thoughts: Document the evaluation process explicitly. For example: “The Extractor’s output identifies a limitation but\n",
        "lacks critical insight, reducing Depth.” “The Analyzer’s limitation is generic, affecting Originality.” “The Reviewer’s output is\n",
        "actionable but misses ethical considerations, limiting Topic Coverage.” This narrative ensures transparency and justifies the score\n",
        "and feedback.\n",
        "\n",
        "Scoring: For each criterion, assign a score (0–10) based on quality: 0–3: Poor (major issues, e.g., superficial, generic, not actionable,\n",
        "narrow coverage). 4–6: Fair (moderate issues, e.g., somewhat insightful, partially actionable, incomplete coverage). 7–8: Good\n",
        "(minor issues, e.g., mostly critical, slightly generic, broadly actionable). 9–10: Excellent (no issues, e.g., highly insightful,\n",
        "novel, clearly actionable, comprehensive coverage).\n",
        "\n",
        "Calculate the total score: Sum (criterion score × weight), where weights are Depth (0.2), Originality (0.2), Actionability (0.3),\n",
        "Topic Coverage (0.3).\n",
        "\n",
        "Example: Depth (8 × 0.2 = 1.6), Originality (7 × 0.2 = 1.4), Actionability (9 × 0.3 = 2.7), Topic Coverage (6 × 0.3 = 1.8),\n",
        "Total = (1.6 + 1.4 + 2.7 + 1.8) × 10 = 75.\n",
        "\n",
        "Input:\n",
        "Extractor Agent: [extractor_agent output]\n",
        "Analyzer Agent: [analyzer_agent output]\n",
        "Reviewer Agent: [reviewer_agent output]\n",
        "Citation Agent: [citation_agent output]\n",
        "\n",
        "Output Format: The output must strictly be in JSON format, starting with ```json\\n{...}.\n",
        "For each agent (Extractor, Analyzer, Reviewer, Citation), provide a JSON object with the following structure:\n",
        "\n",
        "{\n",
        "  \"agent\": \"[Agent Name]\",\n",
        "  \"total_score\": [Numerical score, 0–100],\n",
        "  \"evaluation\": {\n",
        "    \"Depth\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Originality\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Actionability\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    },\n",
        "    \"Topic_Coverage\": {\n",
        "      \"score\": [0–10],\n",
        "      \"strengths\": \"[What was done well]\",\n",
        "      \"issues\": \"[Problems identified]\",\n",
        "      \"suggestions\": \"[How to improve]\"\n",
        "    }\n",
        "  }\n",
        "}'''"
      ],
      "metadata": {
        "id": "E6Hths9_nQ9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def llm_assessment(agent_texts: dict,\n",
        "                   agent_prompts: dict,\n",
        "                   metrics=None):\n",
        "    \"\"\"\n",
        "    Performs the LLM assessment (collective judge) to generate scores and feedback for each agent.\n",
        "    If parsing fails, returns the raw LLM response in a third return value.\n",
        "\n",
        "    Returns:\n",
        "      combined: dict mapping agent name to parsed JSON evaluation data (empty if parse failed)\n",
        "      row_scores: dict of per-agent score keys (scores or None)\n",
        "      raw_response: the unparsed LLM output string\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['Depth','Originality','Actionability','Topic_Coverage']\n",
        "\n",
        "    # 1) Fire off the collective judge prompt\n",
        "    raw_response = run_critic(\n",
        "        JUDGE_PROMPT +\n",
        "        \"\".join(f\"**{agent} Agent**:\\n{agent_texts[agent]}\\n\\n\"\n",
        "                for agent in agent_prompts) +\n",
        "        JUDGE_PROMPT\n",
        "    )\n",
        "    # print(\"collective judge response:\\n\", raw_response)\n",
        "\n",
        "    # 2) Extract JSON-fenced blocks\n",
        "    blocks = re.findall(r\"```json\\n(.*?)```\", raw_response, re.DOTALL)\n",
        "\n",
        "    if not blocks:\n",
        "        # No JSON blocks found at all → return empty combined and scores, plus raw text\n",
        "        # print(\"⚠️ Warning: No JSON-fenced sections found in collective_judge.\")\n",
        "        return {}, {f\"{agent}_score\": None for agent in agent_prompts}, raw_response\n",
        "\n",
        "    combined = {}\n",
        "    for b in blocks:\n",
        "        try:\n",
        "            parsed = json.loads(b)\n",
        "            agent_name = parsed.get(\"agent\")\n",
        "            if agent_name:\n",
        "                combined[agent_name] = parsed\n",
        "            # else:\n",
        "            #     print(\"⚠️ Warning: JSON block missing 'agent' field:\", b)\n",
        "        except json.JSONDecodeError:\n",
        "            # print(\"⚠️ Warning: Failed to parse JSON block:\", b)\n",
        "            # skip it\n",
        "            pass\n",
        "\n",
        "    # If combined is still empty, parsing failed entirely\n",
        "    if not combined:\n",
        "        # print(\"⚠️ Warning: Parsed no valid agent entries. Returning empty scores.\")\n",
        "        return {}, {f\"{agent}_score\": None for agent in agent_prompts}, raw_response\n",
        "\n",
        "    # 3) Build row_scores from combined\n",
        "    row_scores = {}\n",
        "    for agent, data in combined.items():\n",
        "        row_scores[f\"{agent}_score\"] = data.get(\"total_score\")\n",
        "\n",
        "    return combined, row_scores, raw_response\n"
      ],
      "metadata": {
        "id": "btBlDcmsnQ4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = ['extractor_agent', 'analyzer_agent', 'reviewer_agent', 'citation_agent']\n",
        "all_generated_summary = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    print(\"idx\",idx)\n",
        "    extractor_output = row.get(\"extractor_agent\", \"\")\n",
        "    analyzer_output = row.get(\"analyzer_agent\", \"\")\n",
        "    reviewer_output = row.get(\"reviewer_agent\", \"\")\n",
        "    citation_output = row.get(\"citation_agent\", \"\")\n",
        "\n",
        "    # Prepare prompt with row-specific agent outputs\n",
        "    prompt_filled = JUDGE_PROMPT.replace(\n",
        "        \"[extractor_agent output]\", extractor_output\n",
        "    ).replace(\n",
        "        \"[analyzer_agent output]\", analyzer_output\n",
        "    ).replace(\n",
        "        \"[reviewer_agent output]\", reviewer_output\n",
        "    ).replace(\n",
        "        \"[citation_agent output]\", reviewer_output\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = azure_run_critic(prompt_filled)\n",
        "    except Exception as e:\n",
        "        result = f\"ERROR: {e}\"\n",
        "\n",
        "    # df['LLM_feedback_resp'] = all_generated_summary\n",
        "    df.at[idx, \"LLM_feedback_resp\"]  = result\n",
        "\n",
        "    all_generated_summary.append(result)\n"
      ],
      "metadata": {
        "id": "MpRHyo8FuQSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "flattened_rows = []\n",
        "\n",
        "for idx, raw_text in df['LLM_feedback_resp'].items():\n",
        "    # 1. Extract JSON block inside triple backticks\n",
        "    blocks = re.findall(r\"```json\\n(.*?)```\", str(raw_text), re.DOTALL)\n",
        "    if not blocks:\n",
        "        continue\n",
        "\n",
        "    json_str = blocks[0].strip()\n",
        "\n",
        "    try:\n",
        "        # Try loading the JSON string\n",
        "        parsed_json = json.loads(json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        continue  # skip if parsing fails\n",
        "\n",
        "    row_data = {\"row\": idx}\n",
        "\n",
        "    # 2. Handle list of agents\n",
        "    if isinstance(parsed_json, list):\n",
        "        agents = parsed_json\n",
        "    elif isinstance(parsed_json, dict):\n",
        "        # dict with agent names as keys\n",
        "        agents = parsed_json.values()\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    # 3. Extract all fields from each agent\n",
        "    for entry in agents:\n",
        "        if not isinstance(entry, dict):\n",
        "            continue\n",
        "\n",
        "        agent = entry.get(\"agent\")\n",
        "        if not agent:\n",
        "            continue\n",
        "\n",
        "        row_data[f\"{agent}_total_score\"] = entry.get(\"total_score\")\n",
        "\n",
        "        evaluation = entry.get(\"evaluation\", {})\n",
        "        for metric, values in evaluation.items():\n",
        "            for aspect in [\"score\", \"strengths\", \"issues\", \"suggestions\"]:\n",
        "                col_name = f\"{agent}_{metric}_{aspect}\"\n",
        "                row_data[col_name] = values.get(aspect)\n",
        "\n",
        "    flattened_rows.append(row_data)\n",
        "\n",
        "# Final DataFrame\n",
        "df_flattened = pd.DataFrame(flattened_rows)\n"
      ],
      "metadata": {
        "id": "1e0LHF7puQLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ─── 1. Definitions ───\n",
        "\n",
        "metrics = [\"Depth\", \"Originality\", \"Actionability\", \"Topic_Coverage\"]\n",
        "agents  = [\"Extractor\", \"Analyzer\", \"Reviewer\", \"Citation\"]\n",
        "\n",
        "\n",
        "AGENT_BASE_PROMPTS = {\n",
        "    \"Extractor\": Extractor,\n",
        "    \"Analyzer\":  Analyzer,\n",
        "    \"Reviewer\":  Reviewer,\n",
        "    \"Citation\":  Citation\n",
        "}\n",
        "\n",
        "# ─── 2. Ensure score columns are numeric ───\n",
        "for agent in agents:\n",
        "    for metric in metrics:\n",
        "        col = f\"{agent}_{metric}_score\"\n",
        "        df_flattened[col] = pd.to_numeric(df_flattened[col], errors=\"coerce\")\n",
        "\n",
        "# ─── 3. Initialize empty regenerated columns ───\n",
        "for agent in agents:\n",
        "    df_flattened[f\"{agent}_regenerated\"] = None\n",
        "\n",
        "# ─── 4. Iterate over rows ───\n",
        "for i, row in df_flattened.iterrows():\n",
        "    print(f\"Processing row {i}...\")\n",
        "\n",
        "    input_text = df[\"response_string_neurips\"][i]\n",
        "    # print(\"input text\",input_text)\n",
        "    # retrieved  = row.get(\"retrieved_text\", \"\")\n",
        "    # print(\"input text\",input_text)\n",
        "\n",
        "    for agent in agents:\n",
        "        # print(\"yes\")\n",
        "        feedback_parts = []\n",
        "\n",
        "        for metric in metrics:\n",
        "            score_col = f\"{agent}_{metric}_score\"\n",
        "            score = row.get(score_col, None)\n",
        "\n",
        "            if pd.notna(score) and score < 5:\n",
        "                strengths   = row.get(f\"{agent}_{metric}_strengths\", \"\")\n",
        "                issues      = row.get(f\"{agent}_{metric}_issues\", \"\")\n",
        "                suggestions = row.get(f\"{agent}_{metric}_suggestions\", \"\")\n",
        "\n",
        "                feedback_parts.append(\n",
        "                    f\"{metric} Feedback:\\n\"\n",
        "                    f\"  Strengths: {strengths}\\n\"\n",
        "                    f\"  Issues: {issues}\\n\"\n",
        "                    f\"  Suggestions: {suggestions}\"\n",
        "                )\n",
        "        # print(\"feedback_parts\",feedback_parts)\n",
        "        if feedback_parts:\n",
        "            feedback_blob = \"\\n\\n\".join(feedback_parts)\n",
        "\n",
        "            seed_text = retrieved + system_prompt if agent == \"Citation\" else input_text\n",
        "\n",
        "            full_prompt = (\n",
        "                AGENT_BASE_PROMPTS[agent]\n",
        "                + seed_text\n",
        "                + \"\\n\\n\"\n",
        "                + Regenerate_PROMPT\n",
        "                + \"\\n\\n\"\n",
        "                + feedback_blob\n",
        "            )\n",
        "            # print(\"full prompt\",full_prompt)\n",
        "            try:\n",
        "                regenerated = azure_run_critic(full_prompt)\n",
        "                df.at[i, f\"{agent}_regenerated\"] = regenerated\n",
        "                print(\"regenerated\",regenerated)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error on row {i}, agent {agent}: {e}\")\n",
        "                df.at[i, f\"{agent}_regenerated\"] = f\"ERROR: {e}\"\n"
      ],
      "metadata": {
        "id": "kblyaUkGunIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ground Truth Coverage"
      ],
      "metadata": {
        "id": "n0wcbptK8l5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: Lim_and_OR_ground_truth_final ||\n",
        "LLM Generated Limitations: master_agent_ext_analy_rev_cit"
      ],
      "metadata": {
        "id": "bSxlNYDC-mor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making lists of list 'master_agent' text\n",
        "import re\n",
        "\n",
        "# make sure the output column exists\n",
        "df['master_agent_ext_analy_rev_cit_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"master_agent_ext_analy_rev_cit\"]\n",
        "    # skip if missing or not a string\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'master_agent_ext_analy_rev_cit_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'master_agent_ext_analy_rev_cit_list'] = lim_list\n"
      ],
      "metadata": {
        "id": "i-YZE2sB8ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5_JG1h8Zg6"
      },
      "outputs": [],
      "source": [
        "# making lists of list 'ground truth' text\n",
        "\n",
        "import re\n",
        "\n",
        "# ensure the output column exists\n",
        "df['Lim_and_OR_ground_truth_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"Lim_and_OR_ground_truth_final\"]\n",
        "    # skip non-strings\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, flags=re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making combinations from 'ground truth' and llm generated text'\n",
        "\n",
        "df['combined'] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Generate combinations for each row\n",
        "for i in range(len(df)):\n",
        "    combined_list = []\n",
        "    list1 = df[\"Lim_and_OR_ground_truth_list\"][i]\n",
        "    list2 = df[\"master_agent_ext_analy_rev_cit_list\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df.at[i, 'combined'] = combined_list  # Truncate if needed"
      ],
      "metadata": {
        "id": "Q4BL139r8w3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "# from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "import json\n",
        "\n",
        "llm_results = []\n",
        "df['LLM_eval_master_agent_ext_analy_rev_cit_list'] = ''\n",
        "for idx in range(len(df)): # len(df)\n",
        "    print(\"idx is\",idx)\n",
        "    pairs = df.at[idx, 'combined']   # assume this is List[Tuple[list, list]]\n",
        "    if not isinstance(pairs, list) or not pairs:\n",
        "        llm_results.append(None)\n",
        "        continue\n",
        "\n",
        "    # build the named-pairs block in one go\n",
        "    formatted = \"\\n\".join(\n",
        "        f\"Pair {i+1}:\\n  List1: {first}\\n  List2: {second}\"\n",
        "        for i, (first, second) in enumerate(pairs)\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"For each of the following pairs, answer “Yes” if List1 contains a topic or limitation\\n\"\n",
        "        \"from List2, or List2 contains a topic or limitation from from List1; otherwise answer “No”.\\n\"\n",
        "        \"Respond *only* with a JSON object mapping each Pair name to “Yes” or “No”.\\n\\n\"\n",
        "        \"Pairs:\\n\"\n",
        "        f\"{formatted}\"\n",
        "    )\n",
        "\n",
        "    # single call per row\n",
        "    resp_text = run_critic_openai(prompt)\n",
        "    llm_results.append(resp_text)\n",
        "\n",
        "    df.at[idx, 'LLM_eval_master_agent_ext_analy_rev_cit_list'] = resp_text\n"
      ],
      "metadata": {
        "id": "vSA2C5Q58wxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# extract all 'Yes', 'No'\n",
        "pattern = r'\"Pair\\s*\\d+\"\\s*:\\s*\"(Yes|No)\"'\n",
        "\n",
        "all_matches = []\n",
        "for idx in range(len(df)):\n",
        "    raw = df.at[idx, 'LLM_eval_master_agent_ext_analy_rev_cit_list']\n",
        "    if not isinstance(raw, str):\n",
        "        all_matches.append([])\n",
        "        continue\n",
        "    matches = re.findall(pattern, raw)\n",
        "    all_matches.append(matches)\n"
      ],
      "metadata": {
        "id": "-QPgh-ML8wrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for idx, tuples in df['combined'].items():\n",
        "    if not isinstance(tuples, list):\n",
        "        continue\n",
        "    # get the matching list for this row\n",
        "    matches = all_matches[idx] if idx < len(all_matches) else []\n",
        "\n",
        "    for j, (list1, list2) in enumerate(tuples):\n",
        "        # grab the j-th match or None if out of range\n",
        "        is_match = matches[j] if j < len(matches) else None\n",
        "\n",
        "        rows.append({\n",
        "            'source_row': idx,\n",
        "            'List1':      list1,\n",
        "            'List2':      list2,\n",
        "            'is_match':   is_match\n",
        "        })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "\n",
        "result_df.rename(\n",
        "    columns={\n",
        "        'List1': 'Ground_Truth',\n",
        "        'List2': 'LLM_generated'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "coYkymR69RH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_leading_number(x):\n",
        "    \"\"\"\n",
        "    If x is a list, grab its first element; then:\n",
        "    • If it’s an int, return it.\n",
        "    • If it’s a string starting with digits (with or without a dot), return those digits.\n",
        "    Otherwise return None.\n",
        "    \"\"\"\n",
        "    # step 1: if it’s a list, pull out the first item\n",
        "    val = x[0] if isinstance(x, list) and x else x\n",
        "\n",
        "    # step 2: if it’s already an int, just return it\n",
        "    if isinstance(val, int):\n",
        "        return val\n",
        "\n",
        "    # step 3: if it’s a string, regex for leading digits\n",
        "    if isinstance(val, str):\n",
        "        # match “123.” or just “123”\n",
        "        m = re.match(r'^\\s*(\\d+)(?:\\.)?', val)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "# extract into new columns\n",
        "result_df['gt_number']        = result_df['Ground_Truth'].apply(extract_leading_number)\n",
        "result_df['llm_gen_number']   = result_df['LLM_generated'].apply(extract_leading_number)\n"
      ],
      "metadata": {
        "id": "Qzxu5do09RCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth coverage\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "match = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in result_df.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['gt_number'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            match += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['gt_number']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    match += 1\n",
        "print(match)\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = result_df['Ground_Truth'].ne(result_df['Ground_Truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "ck = result_df.groupby(unique_blocks)['gt_number'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(ck)\n"
      ],
      "metadata": {
        "id": "sTscp5vO9Q7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Quality bewtween matched pairs (NLP based metrics)"
      ],
      "metadata": {
        "id": "jCYma4Kk-9fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: ground_truth || LLM_Generated limitation: llm_generated"
      ],
      "metadata": {
        "id": "3oeLB_NN_lpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# say you want to rename 'oldA'→'newA' and 'oldB'→'newB'\n",
        "df.rename(columns={\n",
        "    'Ground_Truth': 'ground_truth',\n",
        "    'LLM_generated': 'llm_generated',\n",
        "    # 'Is_same': 'is_match',\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "iBF9CAs3Tn-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the column 'is_match' is 'no'\n",
        "df_filtered = df[df['is_match'].str.lower() != 'no']"
      ],
      "metadata": {
        "id": "3kCqlOU0Tn-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WQOUxnfuTn-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore (all)\n",
        "!pip3 -q install bert-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d89e97-82cc-4988-8a2f-0aa4714969ab",
        "id": "oH3IU2CYTn-P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTScor for whole texts"
      ],
      "metadata": {
        "id": "hF9cjY0TTn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row using one loop\n",
        "def calculate_bertscore(row):\n",
        "    # Calculate BERT Scores directly for the ground_truth and llm_generated of the row\n",
        "    _, _, F1 = scorer.score([row['ground_truth']], [row['llm_generated']])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df_filtered['bert_score'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "3c71198f0e664124afc47473567b5c7a",
            "4a2b155dd49a4665bb6c17a9f5e2c897",
            "46431161f79d4133b0e9626f15d16d1b",
            "709e4d670c8d4be1aadd41a3707db2bd",
            "216d57cf081f413ea5d73b930a5ce928",
            "9a89c927429240aba5bbc46c244f0ca9",
            "cad2d872c4334e1fb3e2f5743d01b2d0",
            "1f1072c5671e4e4c80af4697626fc2ce",
            "cfbafbdc20e645fa98ce3c756b83264a",
            "1c5cedb4617d415eb809d260a5faeb1f",
            "1c392a13dd7b46e0a62787c17e38f6f6",
            "1e315aeefc844d129827e305f35b62ea",
            "5810d6dc0d5f4c0cbc05b8333383ee4e",
            "128271b8f3594010813411e064d5664f",
            "ae3660d5c79e4bda8d65778eb6df33c1",
            "b8506a22de2c4f8387946838f89f1123",
            "33bf8a8277a94066b0ef57b346ba0d1b",
            "fe25938c285a477a9202247f53e5237a",
            "82370adfac8c46b3aee660d53989c276",
            "793de62a29d74245b0af7c2e1cfbb2af",
            "54acb0f9329d4ed5b165919d61c6890f",
            "b090beec7ce94d0485d107c2b2bc05dd",
            "7e016666a1de4333aa74fc7584a3d13c",
            "5a8dfff12b1d440a9a3d6d118f5da1be",
            "b94fbeaa201f41fe9ff5c72233bec98b",
            "09f1c2b4f9d042fd8702d6798e23440b",
            "c883587700ce4c5abdc80f11b7c534a2",
            "220aff6f3714437a8b4b75b7b4eb32da",
            "3af05e812a7249c8932df09b0b53306c",
            "fc2cb511d6a2451d9955f81cf7ee5257",
            "c4b2149e029d4241b3f0b12b88351cf3",
            "8da5dcf9d2774cb59ef13f7ed8514591",
            "8c127dbd91d64b7da16e839e386b687b",
            "2917cf2db36b4b61a87072624477fe0d",
            "62ad8f05e06847a1a2e12e6946705690",
            "6104e7963a7545da974364de4070a3e1",
            "fa9d0f0d04654132b18afbabec187fba",
            "85d2034a97bd46d7938dbc65d992004a",
            "d5586a6ca4bd4bd0a5f65aed18f3ee10",
            "49c4c1f5ee0f4a89a2ff0e21d8d8433a",
            "d091c71f3bec4e03946613cf82643417",
            "05a5fa84b2aa4d7699d6796a575cd9dc",
            "b6ff5229f29441c987deb9a9817c5062",
            "4b283526b0b54602913fc94cbb40fd6d",
            "0ca6e64c03854952a65f14dc74dafa14",
            "b9f0c45a54da452ca5f0acb93eadb483",
            "9d3a1439dacb4f7a8433437d0742ef81",
            "710b38cefec74705b660dc8e9d740717",
            "226ffb072e73472f94f5fd91f4be059b",
            "fd64cd9ef63145fbb78ed8d905badf32",
            "3df1bfcc51694873abd31ca55afa9b8c",
            "f48ea351ebfe403d87f245979441daf3",
            "5f4ac0df859d4dadb4d967a2dd625463",
            "61837e63136b4226affe1ce10e5e8d89",
            "384a6bcd98394b2fa356b6e5f121b40c",
            "81ca3b6ef8f64c37b5455e491284d849",
            "40ce7b223d0e4ba29a256cf17728b9d7",
            "b1dfc1aa16dc40f79d3f325c35400a61",
            "0bd1d66ffdfc43ef9c4c912c33901984",
            "cac14d9a288e477184e5df449fae4429",
            "1591b816fe744f1ca1097f13b8edc702",
            "5c1a00e56d3d4442a97bc1766c85c7de",
            "57087f649e9a443887468848973c3a92",
            "f1aa9f1cad4748e9898dc58905f3b3cf",
            "13d95bc774f84438b4eaf16cd6ef07dc",
            "e5287bed16cc4c21b30d91a1969829ce"
          ],
          "height": 357
        },
        "outputId": "aa3ad15c-492f-43d9-bae8-cfb62dfbccdd",
        "id": "8dNlr6IbTn-R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c71198f0e664124afc47473567b5c7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e315aeefc844d129827e305f35b62ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e016666a1de4333aa74fc7584a3d13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2917cf2db36b4b61a87072624477fe0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ca6e64c03854952a65f14dc74dafa14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81ca3b6ef8f64c37b5455e491284d849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the 'bert_score' column in df_highest_score\n",
        "average_bert_score = df_filtered['bert_score'].mean()\n",
        "\n",
        "# Display the average\n",
        "average_bert_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee158757-c545-4b03-ee05-c9a608e322b7",
        "id": "9hG55egrTn-S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.8640587552331024)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a58773-93b1-46bb-d899-d2cd0ae30095",
        "id": "_4-fkhcGTn-T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate similarity metrics for each row\n",
        "def calculate_metrics(row):\n",
        "    metrics = {}\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(row['ground_truth'], row['llm_generated'])\n",
        "    metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
        "    metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
        "    metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Cosine Similarity\n",
        "    vectorizer = CountVectorizer().fit_transform([row['ground_truth'], row['llm_generated']])\n",
        "    vectors = vectorizer.toarray()\n",
        "    metrics['cosine_similarity'] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set1 = set(row['ground_truth'].split())\n",
        "    set2 = set(row['llm_generated'].split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    metrics['jaccard_similarity'] = intersection / union if union > 0 else 0\n",
        "\n",
        "    # BLEU Score\n",
        "    metrics['bleu_score'] = sentence_bleu([row['ground_truth'].split()], row['llm_generated'].split())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Apply the function to each row in the DataFrame and store results in new columns\n",
        "metric_results = df_filtered.apply(calculate_metrics, axis=1)\n",
        "\n",
        "# Expand the dictionary into separate columns\n",
        "metric_results_df = pd.DataFrame(metric_results.tolist())\n",
        "df_filtered = pd.concat([df_filtered, metric_results_df], axis=1)\n"
      ],
      "metadata": {
        "id": "_y2fbOzaA1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of each metric\n",
        "average_metrics = {\n",
        "    'Average ROUGE-1': df_filtered['rouge1'].mean(),\n",
        "    'Average ROUGE-2': df_filtered['rouge2'].mean(),\n",
        "    'Average ROUGE-L': df_filtered['rougeL'].mean(),\n",
        "    'Average Cosine Similarity': df_filtered['cosine_similarity'].mean(),\n",
        "    'Average Jaccard Similarity': df_filtered['jaccard_similarity'].mean(),\n",
        "    'Average BLEU Score': df_filtered['bleu_score'].mean()\n",
        "}\n",
        "\n",
        "# Print the average metrics\n",
        "average_metrics\n"
      ],
      "metadata": {
        "id": "x_JC28oCA3-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install keybert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16d4d33-2042-4912-dcee-fdaab3a5fb55",
        "id": "omZk27ZkTn-U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic similarity"
      ],
      "metadata": {
        "id": "qY1r5p35Tn-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Ensure all entries are strings (even if NaN)\n",
        "df_filtered['ground_truth'] = df_filtered['ground_truth'].fillna(\"\").astype(str)\n",
        "df_filtered['llm_generated'] = df_filtered['llm_generated'].fillna(\"\").astype(str)\n",
        "\n",
        "# Now apply KeyBERT safely\n",
        "# df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "# df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n",
        "\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    return [kw[0] for kw in keywords]  # Extract just the keywords\n",
        "\n",
        "# Apply KeyBERT to 'ground_truth' and 'LLM_generated' columns\n",
        "df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "id": "D_un-kmDBCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard Similarity\n",
        "def jaccard_similarity(row):\n",
        "    set1 = set(row['ground_truth_words'])\n",
        "    set2 = set(row['LLM_generated_words'])\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Apply Jaccard Similarity to each row\n",
        "df_filtered['jaccard_similarity_topic'] = df_filtered.apply(jaccard_similarity, axis=1)\n",
        "df_filtered['jaccard_similarity_topic'].mean()"
      ],
      "metadata": {
        "id": "FZ9I8AWzTn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute Cosine Similarity with empty check\n",
        "def cosine_sim(row):\n",
        "    gt = ' '.join(row['ground_truth_words'])\n",
        "    llm = ' '.join(row['LLM_generated_words'])\n",
        "\n",
        "    # If either is empty, return 0 similarity\n",
        "    if not gt.strip() or not llm.strip():\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer().fit_transform([gt, llm])\n",
        "        vectors = vectorizer.toarray()\n",
        "        return cosine_similarity(vectors)[0, 1]\n",
        "    except ValueError:\n",
        "        return 0.0  # fallback if vocabulary is still empty\n",
        "\n",
        "df_filtered['cosine_similarity_topic'] = df_filtered.apply(cosine_sim, axis=1)\n",
        "mean_sim = df_filtered['cosine_similarity_topic'].mean()\n",
        "print(mean_sim)\n"
      ],
      "metadata": {
        "id": "AC4rW-7VA-Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract text between double asterisks\n",
        "def extract_text_between_asterisks(text):\n",
        "    matches = re.findall(r'\\*\\*(.*?)\\*\\*', text)\n",
        "    return matches\n",
        "\n",
        "# Apply the function to both columns and store results in new columns\n",
        "df_filtered['ground_truth_extracted'] = df_filtered['ground_truth'].apply(extract_text_between_asterisks)\n",
        "df_filtered['llm_generated_extracted'] = df_filtered['llm_generated'].apply(extract_text_between_asterisks)\n"
      ],
      "metadata": {
        "id": "cq4Ekh7-Tn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation: Faithfulness, Soundness, and Importance"
      ],
      "metadata": {
        "id": "lbNbk5Ayfhzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_prompt_template = '''\n",
        "You are an expert reviewer. Evaluate the quality of the generated limitations based on the following three criteria: Faithfulness,\n",
        "Soundness, and Importance. For each criterion, assign a score between 1 and 5 and provide a short justification.\n",
        "\n",
        "Faithfulness = The generated limitations should accurately represent the paper’s content and findings, avoiding any introduction\n",
        "of misinformation or contradictions to the original concepts, methodologies or results presented.\n",
        "– 5 points: Perfect alignment with the original content and findings, with no misinformation or contradictions. Fully reflects the\n",
        "paper’s concepts, methodologies, and results\n",
        "accurately.\n",
        "– 4 points: Mostly aligns with the original content but contains minor inaccuracies or slight\n",
        "misinterpretations. These do not significantly\n",
        "affect the overall understanding of the paper’s\n",
        "concepts or results.\n",
        "– 3 points: Generally aligns with the original\n",
        "content but includes several minor inaccuracies or contradictions. Some elements may\n",
        "not fully reflect the paper’s concepts or results,\n",
        "though the overall understanding is mostly intact.\n",
        "– 2 points: Noticeable misalignment with the\n",
        "original content, with multiple inaccuracies\n",
        "or contradictions that could mislead readers.\n",
        "Some key aspects of the paper’s concepts or\n",
        "results are misrepresented.\n",
        "– 1 point: Introduces significant misalignment\n",
        "by misrepresenting issues that do not exist in\n",
        "the paper. Creates considerable misinformation and contradictions that distort the original\n",
        "content, concepts, or results.\n",
        "\n",
        "Soundness = The generated limitations should be detailed and specific, with suggestions or critiques that are practical, logically\n",
        "coherent, and purposeful. It should clearly address relevant aspects of the paper and offer insights that can genuinely improve the\n",
        "research.\n",
        "– 5 points: Highly detailed and specific, with\n",
        "practical, logically coherent, and purposeful\n",
        "suggestions. Clearly addresses relevant aspects and offers insights that substantially improve the research.\n",
        "– 4 points: Detailed and mostly specific, with\n",
        "generally practical and logically sound suggestions. Addresses relevant aspects well but may\n",
        "lack depth or novelty in some areas.\n",
        "– 3 points: Detailed and specific but with some\n",
        "issues in practicality or logical coherence. Suggestions are somewhat relevant and offer partial improvements.\n",
        "– 2 points: Somewhat vague or lacking in specificity, with suggestions that have limited practicality or logical coherence. Addresses\n",
        "relevant aspects only partially and provides minimal improvement.\n",
        "– 1 point: Lacks detail and specificity, with impractical or incoherent suggestions. Fails to\n",
        "effectively address relevant aspects or offer\n",
        "constructive insights for improvement.\n",
        "\n",
        "Importance =  The generated limitations should\n",
        "address the most significant issues that impact the\n",
        "paper’s main findings and contributions. They\n",
        "should highlight key areas where improvements\n",
        "or further research are needed, emphasizing their\n",
        "potential to enhance the research’s relevance and\n",
        "overall impact.\n",
        "– 5 points: Addresses critical issues that substantially impact the paper’s findings and contributions. Clearly identifies major areas for\n",
        "significant improvement or further research,\n",
        "enhancing the research’s relevance and overall\n",
        "impact.\n",
        "– 4 points: Identifies meaningful issues that contribute to refining the paper’s findings and\n",
        "methodology. While the impact is notable,\n",
        "it does not reach the level of fundamentally\n",
        "shaping future research directions.\n",
        "– 3 points: Highlights important issues that offer some improvement to the current work but\n",
        "do not significantly impact future research directions. Provides useful insights for refining\n",
        "the paper but lacks broader implications for\n",
        "further study.\n",
        "– 2 points: Points out limitations with limited\n",
        "relevance to the paper’s overall findings and\n",
        "contributions. Suggestions offer marginal improvements but fail to address more substantial\n",
        "gaps in the research.\n",
        "– 1 point: Focuses on trivial issues, such as minor errors or overly detailed aspects. Does not\n",
        "address substantive issues affecting the paper’s\n",
        "findings or contributions, limiting its overall\n",
        "relevance and impact.\n",
        "\n",
        "Input:\n",
        "Input Paper: [Input Paper]\n",
        "LLM Generated Limitations: [LLM Generated Limitations]\n",
        "\n",
        "Please evaluate the **Generated Limitations** based on the **Input Paper Content** and return your response strictly in the following JSON format:\n",
        "\n",
        "Faithfulness: rating: , explanation:,\n",
        "Soundness:    rating: explanation: ,\n",
        "Importance:   rating: , explanation:\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "s3CF3JRvfhnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "from openai import AzureOpenAI, RateLimitError\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n"
      ],
      "metadata": {
        "id": "-uapq3btfpur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Faithfulness, Sundness, and Importance between Input text (response_string_neurips) and LLM Generated limitations (master_agent_ext_analy_rev_cit_with_rel)"
      ],
      "metadata": {
        "id": "8KIQpyWrf2AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# results = []\n",
        "\n",
        "df['ext_analy_rev_cit_with_rel_result'] = ''\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    input_text = df.at[i, 'response_string_neurips']\n",
        "    generated_limitations = df.at[i, 'master_agent_ext_analy_rev_cit_with_rel']\n",
        "\n",
        "    if pd.isna(input_text) or pd.isna(generated_limitations):\n",
        "        results.append(None)\n",
        "        continue\n",
        "\n",
        "    prompt = evaluation_prompt_template.format(\n",
        "        input_text=input_text.strip(),\n",
        "        generated_limitations=generated_limitations.strip()\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = azure_run_critic(prompt)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        result = None\n",
        "\n",
        "    df.at[i, \"ext_analy_rev_cit_with_rel_result\"] = result\n"
      ],
      "metadata": {
        "id": "iKJDOxLof0Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Faithfulness, Soundness, and Importance score"
      ],
      "metadata": {
        "id": "xFsZapKngNVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize empty columns\n",
        "df['faithfulness_score'] = None\n",
        "df['soundness_score'] = None\n",
        "df['importance_score'] = None\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    val = row['ext_analy_rev_cit_with_rel_result']\n",
        "\n",
        "    if pd.isna(val):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Clean and parse JSON\n",
        "        clean_json = re.sub(r'```json|```', '', val).strip()\n",
        "        parsed = json.loads(clean_json)\n",
        "\n",
        "        # Store ratings into new columns\n",
        "        df.at[idx, 'faithfulness_score'] = parsed['Faithfulness']['rating']\n",
        "        df.at[idx, 'soundness_score'] = parsed['Soundness']['rating']\n",
        "        df.at[idx, 'importance_score'] = parsed['Importance']['rating']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {idx} failed to parse:\", e)\n",
        "        continue\n",
        "\n",
        "avg_faith = df['faithfulness_score'].mean()\n",
        "avg_sound = df['soundness_score'].mean()\n",
        "avg_imp = df['importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ],
      "metadata": {
        "id": "dH20VLEpgMXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring score with Ground truth (Lim_and_OR_ground_truth_final) and LLM Generated response (master_agent_ext_analy_rev_cit_with_rel) by providing score from previous response (input and LLM generated text)"
      ],
      "metadata": {
        "id": "MrCLz7DbgWdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# Initialize result columns\n",
        "df['adjusted_score_ext_analy_rev_cit_with_rel_json'] = None  # Optional: for storing raw JSON string\n",
        "\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\",i)\n",
        "    try:\n",
        "        input_text = df.at[i, 'Lim_and_OR_ground_truth_final']\n",
        "        generated = df.at[i, 'master_agent_ext_analy_rev_cit_with_rel']\n",
        "        faith = df.at[i, 'faithfulness_score']\n",
        "        sound = df.at[i, 'soundness_score']\n",
        "        imp = df.at[i, 'importance_score']\n",
        "\n",
        "        if pd.isna(input_text) or pd.isna(generated) or pd.isna(faith) or pd.isna(sound) or pd.isna(imp):\n",
        "            continue\n",
        "\n",
        "        prompt = evaluation_prompt_template.format(\n",
        "            input_text=input_text.strip(),\n",
        "            generated_limitations=generated.strip(),\n",
        "            faith=int(faith),\n",
        "            sound=int(sound),\n",
        "            imp=int(imp)\n",
        "        )\n",
        "\n",
        "        result = azure_run_critic(prompt)\n",
        "        df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json'] = result  # Optional: Store full JSON output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Row {i} failed: {e}\")\n",
        "        continue\n",
        "\n"
      ],
      "metadata": {
        "id": "H52Xwlc9gV6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Initialize new columns\n",
        "df['adjusted_faithfulness_score'] = None\n",
        "df['adjusted_soundness_score'] = None\n",
        "df['adjusted_importance_score'] = None\n",
        "\n",
        "# Define regex patterns for each score\n",
        "faith_re = re.compile(r'\"Faithfulness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "sound_re = re.compile(r'\"Soundness\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "imp_re   = re.compile(r'\"Importance\"\\s*:\\s*{\\s*\"rating\"\\s*:\\s*(\\d+)', re.DOTALL)\n",
        "\n",
        "# Apply regex extraction row-wise\n",
        "for i in range(len(df)):\n",
        "    row = df.at[i, 'adjusted_score_ext_analy_rev_cit_with_rel_json']\n",
        "    if pd.isna(row):\n",
        "        continue\n",
        "\n",
        "    # Clean text from triple backticks and newline artifacts\n",
        "    cleaned = re.sub(r\"```json|```\", \"\", row).strip()\n",
        "\n",
        "    # Extract values using regex\n",
        "    faith_match = faith_re.search(cleaned)\n",
        "    sound_match = sound_re.search(cleaned)\n",
        "    imp_match   = imp_re.search(cleaned)\n",
        "\n",
        "    if faith_match:\n",
        "        df.at[i, 'adjusted_faithfulness_score'] = int(faith_match.group(1))\n",
        "    if sound_match:\n",
        "        df.at[i, 'adjusted_soundness_score'] = int(sound_match.group(1))\n",
        "    if imp_match:\n",
        "        df.at[i, 'adjusted_importance_score'] = int(imp_match.group(1))\n"
      ],
      "metadata": {
        "id": "qb24szSBgksd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_faith = df['adjusted_faithfulness_score'].mean()\n",
        "avg_sound = df['adjusted_soundness_score'].mean()\n",
        "avg_imp = df['adjusted_importance_score'].mean()\n",
        "\n",
        "print(f\"Average Faithfulness: {avg_faith:.2f}\")\n",
        "print(f\"Average Soundness:   {avg_sound:.2f}\")\n",
        "print(f\"Average Importance:  {avg_imp:.2f}\")\n"
      ],
      "metadata": {
        "id": "SwEdNrxfgkir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}