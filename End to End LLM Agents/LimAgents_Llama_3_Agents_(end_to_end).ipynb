{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Limitation Generation with 3 Agents by Llama 3 8B"
      ],
      "metadata": {
        "id": "MDMQZVp3Daty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import signal\n",
        "import sys\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Global variable to store DataFrame\n",
        "global_df = None\n",
        "global_current_row = 0\n",
        "\n",
        "def signal_handler(signum, frame):\n",
        "    \"\"\"Handle termination signals to save progress\"\"\"\n",
        "    print(f\"\\n⚠️ Received signal {signum}. Saving progress before termination...\")\n",
        "    if global_df is not None:\n",
        "        # Save current progress\n",
        "        emergency_file = f\"/emergency_save_{global_current_row}.csv\"\n",
        "        global_df.to_csv(emergency_file, index=False)\n",
        "        print(f\"  🚨 Emergency save completed: {emergency_file}\")\n",
        "\n",
        "        # Also save to final output\n",
        "        output_file = \"df_neurips_limitations_3_agents.csv\"\n",
        "        global_df.to_csv(output_file, index=False)\n",
        "        print(f\"  🚨 Final output updated: {output_file}\")\n",
        "\n",
        "    print(\"  📊 Progress saved. Exiting gracefully...\")\n",
        "    sys.exit(0)"
      ],
      "metadata": {
        "id": "sSLU3AEXCOgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register signal handlers\n",
        "signal.signal(signal.SIGTERM, signal_handler)  # PBS termination\n",
        "signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C\n",
        "\n",
        "# Load Llama 3 8B model and tokenizer (4-bit quantized)\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "cache_dir = \"llama3_8b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "print(\"Loading Llama 3 8B model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "vWg1ikpRCQlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def truncate_prompt_for_model(prompt: str, max_length: int = 8000) -> str:\n",
        "    \"\"\"Truncate prompt to fit within model's context window, leaving room for generation\"\"\"\n",
        "    # Tokenize the prompt\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # If tokens exceed max_length, truncate\n",
        "    if tokens.shape[1] > max_length:\n",
        "        print(f\"⚠️ Prompt token count = {tokens.shape[1]} exceeds limit ({max_length}). Truncating...\")\n",
        "        # Decode back to text, keeping only the first max_length tokens\n",
        "        truncated_tokens = tokens[:, :max_length]\n",
        "        return tokenizer.decode(truncated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "HmctlUEnCSES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llama_generate(prompt, max_new_tokens=512):\n",
        "    \"\"\"Generate text using Llama 3 8B\"\"\"\n",
        "    # Truncate prompt to fit within model's context window\n",
        "    truncated_prompt = truncate_prompt_for_model(prompt, max_length=8000)\n",
        "\n",
        "    # Tokenize with proper padding and attention mask\n",
        "    inputs = tokenizer(\n",
        "        truncated_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=8192,\n",
        "        return_attention_mask=True\n",
        "    ).to(model.device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "aZqVOmkbCUGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent-specific prompts - each receives both paper_content and cited_papers\n",
        "def get_extractor_prompt(paper_content: str, cited_papers: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in scientific literature analysis. Your task is to carefully read the provided scientific article\n",
        "and extract all explicitly stated limitations as mentioned by the authors. Focus on sections such as Discussion, Conclusion, or\n",
        "Limitations. List each limitation verbatim, including direct quotes where possible, and provide a brief context (e.g., what aspect of\n",
        "the study the limitation pertains to). Ensure accuracy and avoid inferring or adding limitations not explicitly stated. If no limitations\n",
        "are mentioned, state this clearly.\n",
        "\n",
        "Workflow:\n",
        "\n",
        "Plan: Outline which sections (e.g., Discussion, Conclusion, Limitations) to analyze and identify tools (e.g., text extraction) to\n",
        "access the article content. Justify the selection of sections based on their likelihood of containing limitation statements.\n",
        "\n",
        "Reasoning: Let's think step by step to ensure thorough and accurate extraction of limitations:\n",
        "Step 1: Identify all sections in the article that may contain limitations. For example, the Discussion often includes limitations as\n",
        "authors reflect on their findings, while a dedicated Limitations section is explicit.\n",
        "Step 2: Use text extraction tools to retrieve content from these sections. Verify that the content is complete and accurate.\n",
        "Step 3: Scan for explicit limitation statements, such as phrases like \"a limitation of this study\" or \"we acknowledge that.\"\n",
        "Document why each statement qualifies as a limitation.\n",
        "Step 4: For each identified limitation, extract the verbatim quote (if available) and note the context (e.g., related to sample size,\n",
        "methodology).\n",
        "Step 5: Check for completeness by reviewing other potential sections (e.g., Conclusion) to ensure no limitations are missed.\n",
        "Analyze: Use tools to extract and verify the article's content, focusing on explicit limitation statements. Cross-reference extracted\n",
        "quotes with the original text to ensure accuracy.\n",
        "\n",
        "Reflect: Verify that all relevant sections were checked and no limitations were missed. Consider whether any section might have been\n",
        "overlooked and re-evaluate if necessary.\n",
        "Continue: Do not terminate until all explicitly stated limitations are identified or confirmed absent.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Verbatim quote (if available), context (e.g., aspect of the study), and section reference.\n",
        "If none: \"No limitations explicitly stated in the article.\"\n",
        "\n",
        "Tool Use:\n",
        "Use text extraction tools to access and verify article content.\n",
        "Do not assume content; retrieve it directly from the provided article.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected the Discussion section because authors often discuss study constraints there.\"\n",
        "\"I found the phrase 'a limitation of this study' in the Limitations section, indicating an explicit limitation.\"\n",
        "\"I checked the Conclusion section to ensure no additional limitations were mentioned, confirming completeness.\"\n",
        "This narrative ensures transparency and justifies each decision in the extraction process.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please extract and list the key limitations found in this paper. Be specific and provide clear reasoning for each limitation identified.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lXcjjwqQCYaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analyzer_prompt(paper_content: str, cited_papers: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a critical scientific reviewer with expertise in research methodology and analysis. Your task is to analyze\n",
        "the provided scientific article and identify potential limitations not explicitly stated by the authors. Focus on aspects such as study\n",
        "design, sample size, data collection methods, statistical analysis, scope of findings, and underlying assumptions. For each inferred\n",
        "limitation, provide a clear explanation of why it is a limitation and how it impacts the study's validity, reliability,\n",
        "or generalizability. Ensure inferences are grounded in the article's content and avoid speculative assumptions.\n",
        "\n",
        "Workflow:\n",
        "Plan: Identify key areas (e.g., methodology, sample size, statistical analysis) to analyze and select tools (e.g., text analysis) to\n",
        "verify article details. Justify the selection based on their potential to reveal limitations.\n",
        "Reasoning: Let's think step by step to identify inferred limitations:\n",
        "\n",
        "Step 1: Review the article's methodology to identify gaps (e.g., study design flaws, sampling issues).\n",
        "Step 2: Use text analysis tools to extract relevant details (e.g., sample size, statistical methods).\n",
        "Step 3: Evaluate each area for potential limitations, such as small sample size affecting generalizability or unaddressed assumptions.\n",
        "Step 4: Document why each gap qualifies as a limitation and its impact on the study.\n",
        "Step 5: Ensure all key areas are covered to avoid missing potential limitations.\n",
        "Analyze: Critically evaluate the article, using tools to confirm content, and infer limitations based on methodological or analytical gaps.\n",
        "Reflect: Assess whether inferred limitations are grounded in the article and relevant to its validity, reliability, or generalizability.\n",
        "Re-evaluate overlooked areas if necessary.\n",
        "\n",
        "Continue: Iterate until all potential inferred limitations are identified.\n",
        "Output Format:\n",
        "Bullet points listing each inferred limitation.\n",
        "For each: Description, explanation, and impact on the study.\n",
        "\n",
        "Tool Use:\n",
        "Use text analysis tools to verify article content (e.g., methodology, results).\n",
        "Avoid assumptions; base inferences on retrieved content.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"The methodology section mentions a convenience sample, which may limit generalizability.\"\n",
        "\"The statistical analysis lacks adjustment for confounders, potentially affecting validity.\"\n",
        "\"I checked the results section to ensure no additional gaps were missed.\"\n",
        "This narrative ensures transparency and justifies each inferred limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please provide a detailed analysis of the limitations in this research. Consider both obvious and subtle limitations that could affect the validity and applicability of the findings.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "e2_gGa-sCa9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_reviewer_prompt(paper_content: str, cited_papers: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert in open peer review with a focus on transparent and critical evaluation of scientific research.\n",
        "Your task is to review the provided scientific article from the perspective of an external peer reviewer. Identify potential limitations\n",
        "that might be raised in an open review process, considering common critiques such as reproducibility, transparency, generalizability,\n",
        "or ethical considerations. Leverage insights from similar studies or common methodological issues in the field by searching the web or\n",
        "X posts for context, if needed.\n",
        "\n",
        "Workflow:\n",
        "Plan: Identify areas for review (e.g., reproducibility, transparency, ethics) and plan searches for external context\n",
        "(e.g., similar studies, methodological critiques). Justify the selection based on peer review standards.\n",
        "Reasoning: Let's think step by step to identify peer-review limitations:\n",
        "\n",
        "Step 1: Select key areas for review (e.g., reproducibility, ethics) based on common peer review critiques.\n",
        "Step 2: Use text analysis tools to extract relevant article details (e.g., methods, data reporting).\n",
        "Step 3: Identify potential limitations, such as lack of transparency in data or ethical concerns, and justify using article content.\n",
        "Step 4: Search web/X for external context (e.g., similar studies) to support limitations, rating source relevance\n",
        "(high, medium, low, none).\n",
        "Step 5: Synthesize findings, ensuring limitations align with peer review standards and are supported by article or external context.\n",
        "Analyze: Critically review the article, integrating external context to identify limitations. Use tools to verify content and sources.\n",
        "Reflect: Verify that limitations align with peer review standards and are supported by the article or external context.\n",
        "Re-evaluate overlooked areas if necessary.\n",
        "\n",
        "Continue: Iterate until all relevant peer-review limitations are identified.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, why it's a concern, and alignment with peer review standards.\n",
        "Include citations for external sources in the format Source Name, if used.\n",
        "\n",
        "Tool Use:\n",
        "Use web/X search tools to find relevant literature or methodological critiques.\n",
        "Use text analysis tools to verify article content.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected reproducibility because peer reviewers often critique data availability.\"\n",
        "\"The article lacks a data sharing statement, which limits reproducibility.\"\n",
        "\"A web search revealed similar studies provide data openly, supporting this limitation.\"\n",
        "This narrative ensures transparency and justifies each identified limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Paper Content:\n",
        "{paper_content}\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please provide a critical review identifying the limitations and areas of concern in this research. Consider what a peer reviewer would highlight as weaknesses or areas needing improvement.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vFxddIIUCbYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_merger_prompt(extractor_output: str, analyzer_output: str, reviewer_output: str) -> str:\n",
        "    return f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a **Master Coordinator**, an expert in scientific communication and synthesis. Your task is to integrate limitations provided by three specialized agents:\n",
        "\n",
        "**Agents:**\n",
        "1. **Extractor** (explicit limitations from the article),\n",
        "2. **Analyzer** (inferred limitations from critical analysis),\n",
        "3. **Reviewer** (limitations from an open review perspective).\n",
        "\n",
        "**Goals**:\n",
        "1. Combine all limitations into a cohesive, non-redundant list.\n",
        "2. Ensure each limitation is clearly stated, scientifically valid, and aligned with the article's content.\n",
        "3. Prioritize critical limitations that affect the paper's validity and reproducibility.\n",
        "4. Format the final list in a clear, concise, and professional manner, suitable for a scientific review or report.\n",
        "\n",
        "**Workflow**:\n",
        "1. **Plan**: Outline how to synthesize limitations, identify potential redundancies, and resolve discrepancies.\n",
        "2. **Analyze**: Combine limitations, prioritizing critical ones, and verify alignment with the article.\n",
        "3. **Reflect**: Check for completeness, scientific rigor, and clarity.\n",
        "4. **Continue**: Iterate until the list is comprehensive, non-redundant, and professionally formatted.\n",
        "\n",
        "**Output Format**:\n",
        "- Numbered list of final limitations.\n",
        "- For each: Clear statement, brief justification, and source in brackets (e.g., [Author-stated], [Inferred], [Peer-review-derived]).\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Extractor Agent Analysis:\n",
        "{extractor_output}\n",
        "\n",
        "Analyzer Agent Analysis:\n",
        "{analyzer_output}\n",
        "\n",
        "Reviewer Agent Analysis:\n",
        "{reviewer_output}\n",
        "\n",
        "Please merge these three different perspectives on the paper's limitations into a comprehensive, well-organized analysis. Synthesize the insights, resolve any contradictions, and provide a unified view of the paper's limitations.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Xfb-4NIFCbSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to run a specific agent\n",
        "def run_agent(agent_name: str, paper_content: str, cited_papers: str, agent_prompt_func) -> str:\n",
        "    \"\"\"Run a specific agent and return its output\"\"\"\n",
        "    print(f\"  Running {agent_name} agent...\")\n",
        "    try:\n",
        "        prompt = agent_prompt_func(paper_content, cited_papers)\n",
        "        response = llama_generate(prompt, max_new_tokens=512)\n",
        "        print(f\"  {agent_name} agent completed\")\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"  Error in {agent_name} agent: {e}\")\n",
        "        return f\"ERROR in {agent_name} agent: {str(e)}\"\n",
        "\n",
        "print(\"Loading CSV file...\")\n",
        "try:\n",
        "    df = pd.read_csv(\"df_neruips_21_22_final.csv\")\n",
        "    print(f\"Successfully loaded CSV file with shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV file not found. Please check the file path.\")\n",
        "    exit(1)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CSV file: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "# Check if required columns exist\n",
        "required_columns = ['df_Abstract', 'df_Introduction', 'df_Related_Work', 'df_Methodology',\n",
        "                   'df_Dataset', 'df_Conclusion', 'df_Experiment_and_Results',\n",
        "                   'LLM_extracted_future_work', 'relevance_8_cited_in', 'relevance_8_cited_by']\n",
        "\n",
        "missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "if missing_columns:\n",
        "    print(f\"Warning: Missing columns: {missing_columns}\")\n",
        "    print(f\"Available columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Create combined content column\n",
        "df['combined'] = df.apply(\n",
        "    lambda row: (\n",
        "        f\"Abstract: {row.get('df_Abstract', '')}\\n\"\n",
        "        f\"Introduction: {row.get('df_Introduction', '')}\\n\"\n",
        "        f\"Related_Work: {row.get('df_Related_Work', '')}\\n\"\n",
        "        f\"Methodology: {row.get('df_Methodology', '')}\\n\"\n",
        "        f\"Dataset: {row.get('df_Dataset', '')}\\n\"\n",
        "        f\"Conclusion: {row.get('df_Conclusion', '')}\\n\"\n",
        "        f\"Experiment_and_Results: {row.get('df_Experiment_and_Results', '')}\\n\"\n",
        "        f\"LLM_extracted_future_work: {row.get('LLM_extracted_future_work', '')}\\n\"\n",
        "        f\"Extra1: {row.get('df_col_2', '')}\\n\"\n",
        "        f\"Extra2: {row.get('df_col_3', '')}\\n\"\n",
        "        f\"Extra3: {row.get('df_col_4', '')}\\n\"\n",
        "        f\"Extra4: {row.get('df_col_5', '')}\\n\"\n",
        "        f\"Extra5: {row.get('df_col_6', '')}\\n\"\n",
        "        f\"Extra6: {row.get('df_col_7', '')}\\n\"\n",
        "        f\"Extra7: {row.get('df_col_8', '')}\"\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(\"Processing all samples with three agents...\")"
      ],
      "metadata": {
        "id": "hgvSJWzVCwbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Settings"
      ],
      "metadata": {
        "id": "oWG11BkgOIRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rank_bm25\n",
        "!pip3 -q install llama-index-llms-langchain\n",
        "!pip3 -q install langchain_community\n",
        "!pip3 -q install llama_index\n",
        "!pip3 -q install sentence_transformers\n",
        "!pip3 -q install langchain\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import io\n",
        "import time\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "XJOk6kMkT0dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API key\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# LangChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.schema import Document as LCDocument\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.document_loaders import CSVLoader, DirectoryLoader, TextLoader, DataFrameLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "# LlamaIndex\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.schema import Document as LIDoc\n",
        "from llama_index.core.service_context import ServiceContext\n",
        "from llama_index.core.service_context_elements.llm_predictor import LLMPredictor\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "# Sentence Transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Tokenizer\n",
        "import tiktoken\n",
        "\n",
        "# -------------------------------\n",
        "# Token budget helpers\n",
        "# -------------------------------\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "\n",
        "def truncate_for_context(query: str, passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    q_tokens = enc.encode(query, disallowed_special=())\n",
        "    budget = max_tokens - len(q_tokens)\n",
        "    kept, used = [], 0\n",
        "    for p in passages:\n",
        "        p_toks = enc.encode(p, disallowed_special=())\n",
        "        if used + len(p_toks) > budget:\n",
        "            if budget - used > 0:\n",
        "                kept.append(enc.decode(p_toks[:(budget - used)]))\n",
        "            break\n",
        "        kept.append(p)\n",
        "        used += len(p_toks)\n",
        "    return kept\n",
        "\n",
        "def count_tokens(text: str, model: str = MODEL_NAME) -> int:\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "    return len(enc.encode(text, disallowed_special=()))\n",
        "\n",
        "def ensure_passages_within_budget(\n",
        "    query: str,\n",
        "    passages: list[str],\n",
        "    max_tokens: int = MAX_CONTEXT_TOKENS,\n",
        "    model: str = MODEL_NAME,\n",
        ") -> list[str]:\n",
        "    total = count_tokens(query + \"\\n\\n\".join(passages), model=model)\n",
        "    if total <= max_tokens:\n",
        "        return passages\n",
        "    print(f\"Truncating context ({total} tokens)…\")\n",
        "    return truncate_for_context(query, passages, max_tokens=max_tokens, model=model)\n",
        "\n",
        "# -------------------------------\n",
        "# Retriever setup\n",
        "# -------------------------------\n",
        "hf_emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def make_retriever_for_docs(docs, k=3):\n",
        "    # FAISS\n",
        "    faiss_store = FAISS.from_documents(docs, hf_emb)\n",
        "    faiss_r = faiss_store.as_retriever(search_kwargs={\"k\": k})\n",
        "\n",
        "    # BM25\n",
        "    bm25_r = BM25Retriever.from_documents(docs)\n",
        "    bm25_r.k = k\n",
        "\n",
        "    # Ensemble retriever\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# QA pipeline\n",
        "# -------------------------------\n",
        "chat_llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def generate_limitations(question, retriever):\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=chat_llm,\n",
        "        chain_type=\"stuff\",   # simplest: stuff docs into prompt\n",
        "        retriever=retriever,\n",
        "        return_source_documents=False\n",
        "    )\n",
        "    return qa.run(question).strip()\n"
      ],
      "metadata": {
        "id": "VehwjfzhTRM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os, sys, logging\n",
        "\n",
        "# ─── API Key ─────────────────────────────────────────────\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "\n",
        "# ─── Logging ─────────────────────────────────────────────\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "# ─── Model + Embeddings ──────────────────────────────────\n",
        "MODEL_NAME         = \"gpt-4o-mini\"\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "\n",
        "chat_llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# SentenceTransformer (manual embeddings if needed)\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# LangChain wrapper for embeddings\n",
        "hf_emb = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# ─── Helper to extract text from docs ─────────────────────\n",
        "def get_doc_text(d):\n",
        "    if hasattr(d, \"text\"):\n",
        "        return d.text\n",
        "    if hasattr(d, \"get_content\"):\n",
        "        return d.get_content()\n",
        "    if hasattr(d, \"content\"):\n",
        "        return d.content\n",
        "    raise AttributeError(f\"No text attr on {type(d)}\")\n",
        "\n",
        "# ─── Input sections and reference suffixes ────────────────\n",
        "main_cols = [\n",
        "    \"df_Abstract\",\n",
        "    \"df_Introduction\",\n",
        "    \"df_Related_Work\",\n",
        "    \"df_Methodology\",\n",
        "    \"df_Dataset\",\n",
        "    \"df_Conclusion\",\n",
        "    \"df_Experiment_and_Results\"\n",
        "]\n",
        "\n",
        "ref_suffixes = [\n",
        "    \"Introduction\",\n",
        "    \"Related_Work\",\n",
        "    \"Methodology\",\n",
        "    \"Dataset\",\n",
        "    \"Conclusion\",\n",
        "    \"Experiment_and_Results\",\n",
        "    \"Limitation\",\n",
        "    \"Extra\"\n",
        "]\n",
        "\n",
        "# ─── System Prompt ───────────────────────────────────────\n",
        "system_prompt = \"\"\"You are a helpful, respectful, and honest assistant\n",
        "for generating limitations or shortcomings of a research paper.\n",
        "Generate limitations or shortcomings for the following passages\n",
        "from the scientific paper.\n",
        "\"\"\"\n",
        "\n",
        "# ─── Unified helper for LLM calls ─────────────────────────\n",
        "def run_critic(prompt: str, *, system_prompt: str | None = None) -> str:\n",
        "    \"\"\"\n",
        "    Wraps ChatOpenAI to (optionally) send a system prompt + user prompt,\n",
        "    and returns the assistant's reply as a stripped string.\n",
        "    \"\"\"\n",
        "    messages: list[SystemMessage | HumanMessage] = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append(SystemMessage(content=system_prompt))\n",
        "\n",
        "    messages.append(HumanMessage(content=prompt))\n",
        "\n",
        "    response = chat_llm.invoke(messages)\n",
        "    return response.content.strip()\n"
      ],
      "metadata": {
        "id": "B_5h-JDRTRHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Rel_Prompt = '''You are a Relevance Evaluation Agent, an expert in assessing the relevance of retrieved text chunks from a vector\n",
        "database against an input query for the task of generating limitations of scientific articles. Your task is to evaluate the relevance\n",
        "of 10 retrieved text chunks against an input query, which consists of a scientific paper (including sections: Abstract, Introduction,\n",
        "Methodology, Related Work, Experiment and Results, Limitations, and Future Work) and its rewritten version. For each chunk, assign a\n",
        "relevance score from 1 (least relevant) to 10 (most relevant) based on semantic and contextual alignment with the input query, and\n",
        "provide a brief justification for the score.\n",
        "\n",
        "Input:\n",
        "\n",
        "Input Query: [The full text of the original scientific paper and its rewritten version]\n",
        "Retrieved Text Chunks: A list of 10 text chunks, each with a unique identifier and content, formatted as:\n",
        "\n",
        "Chunk 1: [chunk_id_1]: [retrieved_text_1]\n",
        "Chunk 2: [chunk_id_2]: [retrieved_text_2]\n",
        "Chunk 3: [chunk_id_3]: [retrieved_text_3]\n",
        "Chunk 4: [chunk_id_4]: [retrieved_text_4]\n",
        "Chunk 5: [chunk_id_5]: [retrieved_text_5]\n",
        "Chunk 6: [chunk_id_6]: [retrieved_text_6]\n",
        "Chunk 7: [chunk_id_7]: [retrieved_text_7]\n",
        "Chunk 8: [chunk_id_8]: [retrieved_text_8]\n",
        "Chunk 9: [chunk_id_9]: [retrieved_text_9]\n",
        "Chunk 10: [chunk_id_10]: [retrieved_text_10]\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Evaluate Relevance: For each of the 10 retrieved text chunks, assess its relevance to the input query based on semantic and contextual\n",
        "alignment with the original and rewritten scientific paper. Consider how closely the chunk matches key concepts, arguments, or details\n",
        "in the query.\n",
        "\n",
        "Assign Relevance Score:\n",
        "High Scores (8–10): The chunk has strong semantic and contextual alignment with the input query, closely matching key concepts or details.\n",
        "Prioritize chunks containing limitations (e.g., study constraints, challenges) or methodological summaries (e.g., study design, methods),\n",
        "boosting their score by 1–2 points if they align well with the query.\n",
        "\n",
        "Medium Scores (4–7): The chunk has moderate semantic and contextual alignment, containing relevant but less central content (e.g., results,\n",
        "general context, or partial methodological details).\n",
        "\n",
        "Low Scores (1–3): The chunk has minimal or no semantic and contextual alignment, such as unrelated content, generic statements, or\n",
        "off-topic information.\n",
        "\n",
        "Prioritize Limitations and Methodology: Chunks explicitly discussing limitations (e.g., sample size, data constraints, scope issues) or\n",
        "methodological summaries (e.g., study design, experimental setup) are highly relevant. Boost their score by 1–2 points if they align\n",
        "well with the input query, compared to other relevant content.\n",
        "\n",
        "Provide Justification: For each chunk, include a brief justification explaining the assigned score, referencing the chunk’s semantic and\n",
        "contextual alignment with the input query and noting whether it contains limitations or methodological summaries.\n",
        "\n",
        "Do Not Modify Text: Evaluate each chunk as provided, without modifying or paraphrasing the retrieved text.\n",
        "\n",
        "Handle Irrelevant Chunks: If a chunk is unrelated to the input query or lacks meaningful content, assign a score of 1 with an appropriate\n",
        "justification.\n",
        "\n",
        "Workflow:\n",
        "Plan: Review the input query (original and rewritten paper) and the 10 retrieved text chunks to understand their content and context.\n",
        "\n",
        "Reasoning:\n",
        "Step 1: For each chunk, identify its main topic or content (e.g., limitations, methodology, results, background).\n",
        "Step 2: Compare the chunk’s content to the input query, assessing semantic and contextual alignment with the paper’s sections\n",
        "(e.g., Limitations, Methodology).\n",
        "Step 3: Assign a relevance score (1–10) based on alignment, prioritizing limitations and methodological summaries.\n",
        "Step 4: Write a brief justification for the score, explaining the chunk’s relevance and any priority given to limitations or methodology.\n",
        "Step 5: Verify the score and justification are accurate and consistent with the chunk’s content and the input query.\n",
        "\n",
        "Analyze: Use text analysis tools to confirm semantic alignment (e.g., keyword matching for “limitation,” “constraint,” “methodology,” “sample size”) and assess relevance to the input query.\n",
        "Reflect: Ensure scores and justifications are fair, consistent, and reflect the chunk’s alignment with the query, re-evaluating any ambiguous cases.\n",
        "Continue: Iterate until all 10 chunks are evaluated with scores and justifications.\n",
        "\n",
        "Tool Use:\n",
        "Use text analysis tools to identify limitation-related or methodology-related keywords (e.g., “limited,” “constraint,” “sample size,” “methodology”) and assess semantic similarity between chunks and the input query.\n",
        "Use semantic similarity checks to confirm alignment between the chunk and the query’s key concepts.\n",
        "\n",
        "Chain of Thoughts: Document the reasoning process internally for each chunk. For example:\n",
        "“This chunk mentions a small sample size, a limitation, and aligns closely with the query’s focus, so it receives a high score (9).”\n",
        "“This chunk discusses results without addressing limitations or methodology, so it receives a medium score (6).”\n",
        "“This chunk is generic and unrelated to the query’s specific content, so it receives a low score (1).”\n",
        "\n",
        "Output Format: The output must be in strict JSON format, containing an array of 10 objects, one for each retrieved text chunk, with the\n",
        "following structure for each object:\n",
        "\"Chunk_number\": [Chunk number, e.g., \"Chunk 1\", \"Chunk 2\", ..., \"Chunk 10\"]\n",
        "\"relevance_score\": [Integer from 1 to 10]\n",
        "\"justification\": [Brief explanation of the score, referencing the chunk’s semantic and contextual alignment with the query and any emphasis on limitations or methodological summaries]\n",
        "\n",
        "Example: Input: Input Query: [Full text of the original scientific paper and its rewritten version] Retrieved Text Chunks:\n",
        "\n",
        "Chunk 1: chunk_001: The study was limited by a small sample size, which may affect generalizability.\n",
        "Chunk 2: chunk_002: The experiment used a randomized controlled trial design to test the algorithm.\n",
        "Chunk 3: chunk_003: The experiment achieved a 20% improvement in processing speed.\n",
        "...\n",
        "Chunk 10: chunk_010: Data processing is a key challenge in modern research.\n",
        "\n",
        "Output: [ { \"Chunk_number\": \"Chunk 1\", \"relevance_score\": 9, \"justification\": \"The chunk has strong semantic and contextual alignment with\n",
        "the input query, explicitly discussing a limitation (small sample size), which is a high-priority element for limitation generation.\" },\n",
        "{ \"Chunk_number\": \"Chunk 2\", \"relevance_score\": 8, \"justification\": \"The chunk aligns well with the input query by describing the\n",
        "methodological approach, a high-priority element, though it is slightly less central than limitations-related content.\" },\n",
        "{ \"Chunk_number\": \"Chunk 3\", \"relevance_score\": 6, \"justification\": \"The chunk has moderate semantic and contextual alignment,\n",
        "discussing experimental results, but lacks focus on limitations or methodology, resulting in a mid-range score.\" },\n",
        "...\n",
        "{ \"Chunk_number\": \"Chunk 10\", \"relevance_score\": 3, \"justification\": \"The chunk provides generic background information with minimal\n",
        "semantic and contextual alignment to the input query’s specific concepts or arguments.\" } ] '''"
      ],
      "metadata": {
        "id": "yNJzO_PjUQYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### measure relevance score with each chunk with input"
      ],
      "metadata": {
        "id": "kDuCghOKUcMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "MAX_CONTEXT_TOKENS = 127_000\n",
        "hf_emb = HuggingFaceEmbeddings()\n",
        "enc = tiktoken.encoding_for_model(MODEL_NAME)\n",
        "\n",
        "# Initialize column\n",
        "df1[\"cited_by_top_20_raw\"] = None\n",
        "df1[\"cited_by_top_20_texts\"] = None\n",
        "df1[\"cited_by_top_20_meta\"] = None\n",
        "df1[\"retrieved_text_llm_asses\"] = None\n",
        "\n",
        "# Iterate through df1\n",
        "for i, row in df1.iloc[:325].iterrows():\n",
        "    print(\"index\",i)\n",
        "    cited_list = row.get(\"cited_by_full_text\", [])\n",
        "    all_docs = []\n",
        "\n",
        "    for j, cited_dict in enumerate(cited_list):\n",
        "        abstract = cited_dict.get(\"abstractText\", \"\")\n",
        "        sections = cited_dict.get(\"sections\", [])\n",
        "        row_num = cited_dict.get(\"row_number\", \"\")\n",
        "        file_name = cited_dict.get(\"file_name\", \"\")\n",
        "\n",
        "        if isinstance(abstract, str) and abstract.strip():\n",
        "            all_docs.append(Document(\n",
        "                page_content=\"Abstract: \" + abstract.strip(),\n",
        "                metadata={\"row_number\": row_num, \"file_name\": file_name, \"position\": j}\n",
        "            ))\n",
        "\n",
        "        for sec in sections:\n",
        "            if isinstance(sec, dict):\n",
        "                heading = sec.get(\"heading\", \"\").strip()\n",
        "                text = sec.get(\"text\", \"\").strip()\n",
        "                if text:\n",
        "                    combined = f\"{heading}: {text}\" if heading else text\n",
        "                    all_docs.append(Document(\n",
        "                        page_content=combined,\n",
        "                        metadata={\"row_number\": row_num, \"file_name\": file_name, \"position\": j}\n",
        "                    ))\n",
        "\n",
        "    if not all_docs:\n",
        "        continue\n",
        "\n",
        "    # Split into chunks\n",
        "    splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
        "    chunked_docs = splitter.split_documents(all_docs)\n",
        "    print(f\"Total number of chunks for row {i}: {len(chunked_docs)}\")\n",
        "\n",
        "    # Create retrievers\n",
        "    faiss_store = FAISS.from_documents(chunked_docs, hf_emb)\n",
        "    faiss_r = faiss_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "    bm25_r = BM25Retriever.from_documents(chunked_docs)\n",
        "    bm25_r.k = 20\n",
        "\n",
        "    # Ensemble Retriever\n",
        "    ensemble = EnsembleRetriever(\n",
        "        retrievers=[faiss_r, bm25_r],\n",
        "        weights=[0.5, 0.5]\n",
        "    )\n",
        "\n",
        "    # Use row-specific query\n",
        "    example_query = row.get(\"response_string_neurips\", \"\")\n",
        "    input_query = (\n",
        "        f\"Scientific paper:\\n{row['response_string_neurips']}\\n\\n\"\n",
        "        f\"Rewritten version of scientific paper:\\n{row['Input_Query_rewrite']}\"\n",
        "    )\n",
        "    if not input_query.strip():\n",
        "        continue\n",
        "\n",
        "    # Retrieve top 20 and store\n",
        "    top20 = ensemble.get_relevant_documents(input_query)\n",
        "    top20 = top20[:20]\n",
        "\n",
        "    # Store top20 as list of strings in the column\n",
        "    df1.at[i, \"cited_by_top_20_raw\"] = top20  # stores full Document objects (content + metadata)\n",
        "    df1.at[i, \"cited_by_top_20_texts\"] = [doc.page_content for doc in top20]  # just the texts\n",
        "    df1.at[i, \"cited_by_top_20_meta\"] = [doc.metadata for doc in top20]  # just the metadata\n",
        "\n",
        "    # ---------- 6) Batch LLM Scoring ---------------\n",
        "    all_llm_scores = []\n",
        "\n",
        "    # Rel_Prompt = \"You are a helpful assistant tasked with evaluating text relevance.\"  # or load externally\n",
        "\n",
        "    prefix = (\n",
        "        f\"{Rel_Prompt}\\n\\n\"\n",
        "        f\"Input Query:\\n{example_query}\\n\\n\"\n",
        "        \"Here are up to 10 retrieved text chunks:\\n\"\n",
        "    )\n",
        "    prefix_len = count_tokens(prefix, model=MODEL_NAME)\n",
        "\n",
        "    question = (\n",
        "        \"\\\\nOn a scale of 1–10, how relevant is each chunk to the above Input Query? \"\n",
        "        \"Respond with JSON array with Chunk Number, Score, and Justification for each chunk.\"\n",
        "    )\n",
        "    question_len = count_tokens(question, model=MODEL_NAME)\n",
        "\n",
        "    for batch_start in (0, 10):\n",
        "        batch_docs = top20[batch_start:batch_start+10]\n",
        "        if not batch_docs:\n",
        "            continue  # skip empty batches\n",
        "        batch_texts = [d.page_content for d in batch_docs]\n",
        "\n",
        "        # truncate to token budget\n",
        "        available = MAX_CONTEXT_TOKENS - prefix_len - question_len\n",
        "        kept, used = [], 0\n",
        "        for p in batch_texts:\n",
        "            toks = enc.encode(p, disallowed_special=())\n",
        "            if used + len(toks) > available:\n",
        "                break\n",
        "            kept.append(p)\n",
        "            used += len(toks)\n",
        "\n",
        "        chunks_list = \"\\n\\n\".join(\n",
        "            f\"Chunk {batch_start+idx+1}: {text}\" for idx, text in enumerate(kept)\n",
        "        )\n",
        "        prompt = prefix + chunks_list + question\n",
        "\n",
        "        # Call the LLM to assess\n",
        "        raw = run_critic(prompt)\n",
        "        all_llm_scores.append(raw)  # you can also parse JSON if needed\n",
        "\n",
        "    # Save LLM assessments\n",
        "    df1.at[i, \"retrieved_text_llm_asses\"] = all_llm_scores\n",
        "\n",
        "    # Optional debug print\n",
        "    if i == 0 or i == 20:\n",
        "        print(f\"all_llm_scores for row {i}:\", all_llm_scores)\n",
        "\n",
        "df1.to_csv(\"df.csv\",index=False)"
      ],
      "metadata": {
        "id": "1gjZrLOvUQP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Placeholder: simulate loading df\n",
        "# df = pd.read_csv(\"your_path.csv\")\n",
        "\n",
        "# Prepare a new column to store high-score chunks\n",
        "df1[\"top_chunks_texts\"] = None\n",
        "\n",
        "def extract_top_chunks(row):\n",
        "    try:\n",
        "        raw = row[\"retrieved_text_llm_asses\"]\n",
        "        if isinstance(raw, str):\n",
        "            # Convert string to list\n",
        "            raw_list = ast.literal_eval(raw)\n",
        "        elif isinstance(raw, list):\n",
        "            raw_list = raw\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        all_chunks = []\n",
        "        for entry in raw_list:\n",
        "            # Extract the JSON block\n",
        "            match = re.search(r\"\\[.*\\]\", entry, re.DOTALL)\n",
        "            if not match:\n",
        "                continue\n",
        "            try:\n",
        "                chunk_json = json.loads(match.group())\n",
        "                for item in chunk_json:\n",
        "                    score = item.get(\"relevance_score\", 0)\n",
        "                    chunk_idx = int(item.get(\"Chunk_number\", \"Chunk 0\").split()[-1]) - 1\n",
        "                    if score >= 7:\n",
        "                        # Safely get the chunk from cited_by_top_20_texts\n",
        "                        text_list = row.get(\"cited_by_top_20_texts\", [])\n",
        "                        if isinstance(text_list, list) and 0 <= chunk_idx < len(text_list):\n",
        "                            all_chunks.append(text_list[chunk_idx])\n",
        "            except Exception as e:\n",
        "                continue\n",
        "        return all_chunks if all_chunks else None\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Apply to all rows\n",
        "df1[\"top_chunks_texts\"] = df1.apply(extract_top_chunks, axis=1)\n"
      ],
      "metadata": {
        "id": "PTuHWunMUoxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9H0Hw0VcO34R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Only apply ast.literal_eval to non-null strings\n",
        "df1['top_chunks_texts'] = df1['top_chunks_texts'].apply(\n",
        "    lambda x: ast.literal_eval(x) if pd.notnull(x) else np.nan\n",
        ")"
      ],
      "metadata": {
        "id": "PjmHZI7FRucp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['retrieved_text_llm_asses'] = (\n",
        "    df['retrieved_text_llm_asses']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")\n",
        "df['top20_docs'] = (\n",
        "    df['top20_docs']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "bfSit6KsRuX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['top20_docs']\n",
        "    asses = row['retrieved_text_llm_asses']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_in'] = df.apply(pick_high_relevance, axis=1)\n"
      ],
      "metadata": {
        "id": "fCFeOmeOR6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import pandas as pd\n",
        "\n",
        "df['cited_by_top_20_texts'] = (\n",
        "    df['cited_by_top_20_texts']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "Vr51ANeAR5_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['retrieved_text_llm_asses_cited_by'] = (\n",
        "    df['retrieved_text_llm_asses_cited_by']\n",
        "      .fillna('[]')               # turn NaN → \"[]\"\n",
        "      .apply(ast.literal_eval)    # now safe to eval\n",
        ")"
      ],
      "metadata": {
        "id": "1M2x0GI5R55Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_chunk_dicts(cell):\n",
        "    \"\"\"\n",
        "    cell is expected to be a list of strings, each string containing a\n",
        "    ```json ... ``` block holding a JSON array of chunk‐dicts.\n",
        "    This returns a flat list of all dicts.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for s in cell or []:\n",
        "        # 1) remove the ```json fences\n",
        "        s_clean = re.sub(r'^```json\\s*', '', s.strip())\n",
        "        s_clean = re.sub(r'```$',      '', s_clean.strip())\n",
        "\n",
        "        # 2) parse the JSON\n",
        "        try:\n",
        "            data = json.loads(s_clean)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        # 3) if it’s a list of dicts, extend; otherwise skip\n",
        "        if isinstance(data, list):\n",
        "            out.extend(d for d in data if isinstance(d, dict))\n",
        "    return out\n",
        "\n",
        "# apply to your DataFrame\n",
        "df['retrieved_text_llm_asses_cited_by_upd'] = df['retrieved_text_llm_asses_cited_by'].apply(extract_chunk_dicts)\n"
      ],
      "metadata": {
        "id": "WAjb1kQhRuSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take chunk where relevance score 8 or more\n",
        "\n",
        "def pick_high_relevance(row, threshold=8):\n",
        "    docs  = row['cited_by_top_20_texts']\n",
        "    asses = row['retrieved_text_llm_asses_cited_by_upd']\n",
        "    # find all indices where relevance_score ≥ threshold\n",
        "    idxs = [i for i, d in enumerate(asses)\n",
        "            if isinstance(d, dict) and d.get('relevance_score', 0) >= threshold]\n",
        "    # pull the same‐indexed items from top20_docs (guarding against bad indices)\n",
        "    return [docs[i] for i in idxs if i < len(docs)]\n",
        "\n",
        "# create a new column with the selected docs\n",
        "df['relevance_8_cited_by'] = df.apply(pick_high_relevance, axis=1)\n"
      ],
      "metadata": {
        "id": "ddTfqLlBSC8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Citation_agent = '''You are an expert scientific research assistant tasked with inferring potential limitations for an unspecified\n",
        "current scientific article based solely on its cited papers.\n",
        "You are given information from multiple cited papers, which are assumed to be referenced by the current article.\n",
        "Your goal is to analyze these cited works and identify possible limitations that the current paper may have, by\n",
        "comparing its presumed scope, methods, or results against the cited literature.\n",
        "Because the input paper itself is not provided, you must reason from the cited papers alone, identifying what\n",
        "gaps, stronger methods, broader coverage, or alternative results the cited works might expose in the hypothetical\n",
        "current paper that cites them.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Generate a list of scientifically grounded limitations that the current article might have, assuming it builds upon or is informed by the provided cited papers.\n",
        "\n",
        "Each limitation should:\n",
        "\n",
        "Be concise\n",
        "\n",
        "Reference the relevant cited paper(s) by title\n",
        "\n",
        "Clearly explain how the cited paper exposes a potential limitation\n",
        "\n",
        "Be plausible and insightful based on common scientific reasoning\n",
        "\n",
        "Workflow:\n",
        "Plan:\n",
        "Identify key insights, strengths, and scopes of the cited papers that could set a high bar or reveal blind spots\n",
        "in a hypothetical citing article.\n",
        "\n",
        "Reasoning: Let's think step by step to infer limitations:\n",
        "Review each cited paper to extract its methodology, findings, and scope.\n",
        "Ask: If a paper cited this work but did not adopt or address its insights, what limitation might arise?\n",
        "Identify where the cited paper offers better methodology, broader scope, or contradicting findings.\n",
        "Formulate each limitation as a plausible shortcoming of a hypothetical article that builds on—but possibly\n",
        "underutilizes—these cited works.\n",
        "\n",
        "Justify each limitation based on specific attributes of the cited paper (e.g., \"more comprehensive dataset\",\n",
        "\"stronger evaluation metric\", etc.)\n",
        "\n",
        "Analyze:\n",
        "Develop a set of inferred limitations, each tied to specific cited paper(s) and grounded in logical comparison.\n",
        "\n",
        "Reflect:\n",
        "Ensure coverage of all relevant cited papers and validate that each limitation is scientifically plausible in\n",
        "context.\n",
        "\n",
        "Output Format:\n",
        "Bullet points listing each limitation.\n",
        "For each: Description, explanation, and reference to the cited paper(s) in the format Paper Title.\n",
        "\n",
        "Tool Use (if applicable):\n",
        "\n",
        "Use citation lookup tools or document content to extract accurate summaries.\n",
        "Do not assume details about the input paper—focus only on drawing limitations based on differences, omissions,\n",
        "or underuse of the cited works.\n",
        "\n",
        "Chain of Thoughts:\n",
        "During the Reasoning step, document the thought process explicitly. For example:\n",
        "\"I selected [Paper X] because it uses a more robust method than the current article.\"\n",
        "\"The current article's simpler method may limit accuracy compared to [Paper X].\"\n",
        "\"I reviewed all cited papers to ensure no relevant gaps were missed.\"\n",
        "This narrative ensures transparency and justifies each identified limitation.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Cited Papers Information:\n",
        "{cited_papers}\n",
        "\n",
        "Please identify limitations that would be relevant for researchers who might cite this paper in future work.\n",
        "Consider what limitations future authors might mention when discussing this paper's contribution to the field,\n",
        "based on the cited papers context.\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "GjfSnwMTTHoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate limitations using cited in + cited by papers"
      ],
      "metadata": {
        "id": "QBrBGIJkS3Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the column names to concatenate\n",
        "cols_to_concat = [\n",
        "    \"neurips_Abstract\",\n",
        "    \"neurips_Introduction\",\n",
        "    \"neurips_Related_Work\",\n",
        "    \"neurips_Methodology\",\n",
        "    \"neurips_Dataset\",\n",
        "    \"neurips_Conclusion\",\n",
        "    \"neurips_Experiment_and_Results\",\n",
        "    \"neurips_Extra\"\n",
        "]\n",
        "\n",
        "# Create a new column 'response_string_neurips' with labeled concatenation\n",
        "def concat_with_labels(row):\n",
        "    parts = []\n",
        "    for col in cols_to_concat:\n",
        "        if isinstance(row.get(col), str) and row[col].strip():\n",
        "            label = col.replace(\"neurips_\", \"\").replace(\"_\", \" \")\n",
        "            parts.append(f\"{label}: {row[col].strip()}\")\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "df[\"response_string\"] = df.apply(concat_with_labels, axis=1)"
      ],
      "metadata": {
        "id": "PkDsmexeUH9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "# Tokenization setup\n",
        "encoding   = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
        "max_tokens = 128000\n",
        "\n",
        "def truncate_to_max_tokens(text: str, max_length: int) -> str:\n",
        "    tokens = encoding.encode(text)\n",
        "    return encoding.decode(tokens[:max_length]) if len(tokens) > max_length else text\n",
        "\n",
        "# Make sure the output column exists\n",
        "df['citation_agent_in_by_8'] = ''\n",
        "\n",
        "# Process each row\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(f\"Processing row {i}...\")\n",
        "    row = df.iloc[i]\n",
        "\n",
        "    # 1) Collect all items from relevance_8_cited_in\n",
        "    cited_in_list = row.get('relevance_8_cited_in', []) or []\n",
        "    cited_in_texts = []\n",
        "    for itm in cited_in_list:\n",
        "        # if it's a dict with 'text' key, grab that, otherwise str(itm)\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_in_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_in_texts.append(str(itm))\n",
        "\n",
        "    # 2) Collect all items from relevance_8_cited_by\n",
        "    cited_by_list = row.get('relevance_8_cited_by', []) or []\n",
        "    cited_by_texts = []\n",
        "    for itm in cited_by_list:\n",
        "        if isinstance(itm, dict) and 'text' in itm:\n",
        "            cited_by_texts.append(itm['text'])\n",
        "        else:\n",
        "            cited_by_texts.append(str(itm))\n",
        "\n",
        "    # 3) Build the combined prompt section\n",
        "    cited_in_block = \"\\n\".join(cited_in_texts)\n",
        "    cited_by_block = \"\\n\".join(cited_by_texts)\n",
        "\n",
        "    combined_cited_input = (\n",
        "        \"Referenced papers:\\n\" + cited_in_block +\n",
        "        \"\\n\\nPapers who cited this paper:\\n\" + cited_by_block\n",
        "    )\n",
        "\n",
        "    input_paper = df['response_string'][i]\n",
        "    prompt = Citation_agent + (\n",
        "        \"You are an assistant tasked to generate limitations or shortcomings \"\n",
        "        \"in a scientific article. Below is the input paper:\\n\"\n",
        "        f\"{input_paper}\\n\\n\"\n",
        "        \" Below is the relevant text from both the papers \"\n",
        "        \"that this article cites and those that cite it.\\n\\n\"\n",
        "        f\"{combined_cited_input}\\n\\n\"\n",
        "        \"Please generate limitations based on this information.\"\n",
        "    )\n",
        "\n",
        "    # 5) Truncate and call LLM\n",
        "    truncated = truncate_to_max_tokens(prompt, max_tokens)\n",
        "    try:\n",
        "        llm_summary = azure_run_critic(truncated)\n",
        "    except Exception as e:\n",
        "        print(f\"Error at row {i}: {e}\")\n",
        "        llm_summary = \"ERROR\"\n",
        "\n",
        "    df.at[i, \"citation_agent_in_by_8\"] = llm_summary\n"
      ],
      "metadata": {
        "id": "HayadLbXSYQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOI7G1AgBVdo"
      },
      "outputs": [],
      "source": [
        "# Initialize new columns for each agent\n",
        "df['limitations_extractor'] = \"\"\n",
        "df['limitations_analyzer'] = \"\"\n",
        "df['limitations_reviewer'] = \"\"\n",
        "df['limitations_merged_final'] = \"\"\n",
        "\n",
        "# Process each row with all agents\n",
        "for i in range(len(df)): # len(df)\n",
        "    print(\"i is\", i)\n",
        "    global_current_row = i + 1  # Update global counter\n",
        "    global_df = df  # Update global DataFrame reference\n",
        "\n",
        "    print(f\"\\n=== Processing row {i+1}/{len(df)} ===\")\n",
        "    row = df.iloc[i]\n",
        "    paper_content = row['combined']\n",
        "\n",
        "    # Get cited papers information for all agents\n",
        "    cited_in = row.get('relevance_8_cited_in', '')\n",
        "    cited_by = row.get('relevance_8_cited_by', '')\n",
        "    cited_papers = f\"Papers cited by this article:\\n{cited_in}\\n\\nPapers that cited this article:\\n{cited_by}\"\n",
        "    # Run all three agents\n",
        "    extractor_output = run_agent(\"Extractor\", paper_content, cited_papers, get_extractor_prompt)\n",
        "    analyzer_output = run_agent(\"Analyzer\", paper_content, cited_papers, get_analyzer_prompt)\n",
        "    reviewer_output = run_agent(\"Reviewer\", paper_content, cited_papers, get_reviewer_prompt)\n",
        "\n",
        "    # Store individual agent outputs\n",
        "    df.at[i, 'limitations_extractor'] = extractor_output\n",
        "    df.at[i, 'limitations_analyzer'] = analyzer_output\n",
        "    df.at[i, 'limitations_reviewer'] = reviewer_output\n",
        "\n",
        "    # Merge all agent outputs\n",
        "    try:\n",
        "        merger_prompt = get_merger_prompt(extractor_output, analyzer_output, reviewer_output)\n",
        "        merged_output = llama_generate(merger_prompt, max_new_tokens=512)\n",
        "        df.at[i, 'limitations_merged_final'] = merged_output.strip()\n",
        "    except Exception as e:\n",
        "        df.at[i, 'limitations_merged_final'] = f\"ERROR in Master Coordinator agent: {str(e)}\"\n",
        "\n",
        "    print(f\"  Row {i+1} completed\")\n",
        "\n",
        "    # Save progress every 5 rows to prevent data loss\n",
        "    if i % 5 == 0:\n",
        "        output_file = \"df_neurips_limitations_3_agents.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"  ✅ Checkpoint saved at row {i+1}\")\n",
        "\n",
        "# Save results\n",
        "output_file = \"df_neurips_limitations_3_agents.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"\\nResults saved to: {output_file}\")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "cfeKV06TDX2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth coverage"
      ],
      "metadata": {
        "id": "NQ5ApMYAGoCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making lists of list 'master_agent' text\n",
        "import re\n",
        "\n",
        "# make sure the output column exists\n",
        "df['limitations_merged_final_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"limitations_merged_final\"]\n",
        "    # skip if missing or not a string\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'limitations_merged_final_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'limitations_merged_final_list'] = lim_list\n"
      ],
      "metadata": {
        "id": "i-YZE2sB8ld2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu5_JG1h8Zg6"
      },
      "outputs": [],
      "source": [
        "# making lists of list 'ground truth' text\n",
        "\n",
        "import re\n",
        "\n",
        "# ensure the output column exists\n",
        "df['Lim_and_OR_ground_truth_list'] = None\n",
        "\n",
        "for row_idx in range(len(df)):\n",
        "    raw = df.at[row_idx, \"Lim_and_OR_ground_truth_final\"]\n",
        "    # skip non-strings\n",
        "    if not isinstance(raw, str):\n",
        "        df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = []\n",
        "        continue\n",
        "\n",
        "    # split on double-newline before a numbered item\n",
        "    parts = re.split(r'\\n\\n(?=\\d+\\.)', raw.strip())\n",
        "\n",
        "    lim_list = []\n",
        "    for part in parts:\n",
        "        m = re.match(r'(\\d+)\\.\\s*(.*)', part, flags=re.S)\n",
        "        if not m:\n",
        "            continue\n",
        "        num  = int(m.group(1))\n",
        "        text = m.group(2).strip()\n",
        "        lim_list.append([num, text])\n",
        "\n",
        "    df.at[row_idx, 'Lim_and_OR_ground_truth_list'] = lim_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making combinations from 'ground truth' and llm generated text'\n",
        "\n",
        "df['combined'] = [[] for _ in range(len(df))]\n",
        "\n",
        "# Generate combinations for each row\n",
        "for i in range(len(df)):\n",
        "    combined_list = []\n",
        "    list1 = df[\"Lim_and_OR_ground_truth_list\"][i]\n",
        "    list2 = df[\"limitations_merged_final_list\"][i]\n",
        "\n",
        "    # Generate all possible combinations\n",
        "    for item1 in list1:\n",
        "        for item2 in list2:\n",
        "            combined_list.append((item1, item2))\n",
        "\n",
        "    # Store the first 100 combinations (or all if fewer)\n",
        "    df.at[i, 'combined'] = combined_list  # Truncate if needed"
      ],
      "metadata": {
        "id": "Q4BL139r8w3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "import time\n",
        "import pandas as pd\n",
        "# from openai import AzureOpenAI, RateLimitError\n",
        "\n",
        "import os\n",
        "import time\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set up OpenAI API\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define the OpenAI streaming function for GPT-4o-mini\n",
        "def run_critic_openai(prompt: str):\n",
        "    summary_text = \"\"\n",
        "    stream = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        stream=True,\n",
        "        temperature=0\n",
        "    )\n",
        "    for chunk in stream:\n",
        "        summary_text += chunk.choices[0].delta.content or \"\"\n",
        "    return summary_text.strip()\n",
        "\n",
        "# Now your batch‐processing loop:\n",
        "all_generated_summary = []\n",
        "start_time = time.time()\n",
        "\n",
        "import json\n",
        "\n",
        "llm_results = []\n",
        "df['LLM_eval'] = ''\n",
        "for idx in range(len(df)): # len(df)\n",
        "    print(\"idx is\",idx)\n",
        "    pairs = df.at[idx, 'combined']   # assume this is List[Tuple[list, list]]\n",
        "    if not isinstance(pairs, list) or not pairs:\n",
        "        llm_results.append(None)\n",
        "        continue\n",
        "\n",
        "    # build the named-pairs block in one go\n",
        "    formatted = \"\\n\".join(\n",
        "        f\"Pair {i+1}:\\n  List1: {first}\\n  List2: {second}\"\n",
        "        for i, (first, second) in enumerate(pairs)\n",
        "    )\n",
        "\n",
        "    prompt = (\n",
        "        \"For each of the following pairs, answer “Yes” if List1 contains a topic or limitation\\n\"\n",
        "        \"from List2, or List2 contains a topic or limitation from from List1; otherwise answer “No”.\\n\"\n",
        "        \"Respond *only* with a JSON object mapping each Pair name to “Yes” or “No”.\\n\\n\"\n",
        "        \"Pairs:\\n\"\n",
        "        f\"{formatted}\"\n",
        "    )\n",
        "\n",
        "    # single call per row\n",
        "    resp_text = run_critic_openai(prompt)\n",
        "    llm_results.append(resp_text)\n",
        "\n",
        "    df.at[idx, 'LLM_eval'] = resp_text\n"
      ],
      "metadata": {
        "id": "vSA2C5Q58wxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# extract all 'Yes', 'No'\n",
        "pattern = r'\"Pair\\s*\\d+\"\\s*:\\s*\"(Yes|No)\"'\n",
        "\n",
        "all_matches = []\n",
        "for idx in range(len(df)):\n",
        "    raw = df.at[idx, 'LLM_eval']\n",
        "    if not isinstance(raw, str):\n",
        "        all_matches.append([])\n",
        "        continue\n",
        "    matches = re.findall(pattern, raw)\n",
        "    all_matches.append(matches)\n"
      ],
      "metadata": {
        "id": "-QPgh-ML8wrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for idx, tuples in df['combined'].items():\n",
        "    if not isinstance(tuples, list):\n",
        "        continue\n",
        "    # get the matching list for this row\n",
        "    matches = all_matches[idx] if idx < len(all_matches) else []\n",
        "\n",
        "    for j, (list1, list2) in enumerate(tuples):\n",
        "        # grab the j-th match or None if out of range\n",
        "        is_match = matches[j] if j < len(matches) else None\n",
        "\n",
        "        rows.append({\n",
        "            'source_row': idx,\n",
        "            'List1':      list1,\n",
        "            'List2':      list2,\n",
        "            'is_match':   is_match\n",
        "        })\n",
        "\n",
        "result_df = pd.DataFrame(rows)\n",
        "\n",
        "result_df.rename(\n",
        "    columns={\n",
        "        'List1': 'Ground_Truth',\n",
        "        'List2': 'LLM_generated'\n",
        "    },\n",
        "    inplace=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "coYkymR69RH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_leading_number(x):\n",
        "    \"\"\"\n",
        "    If x is a list, grab its first element; then:\n",
        "    • If it’s an int, return it.\n",
        "    • If it’s a string starting with digits (with or without a dot), return those digits.\n",
        "    Otherwise return None.\n",
        "    \"\"\"\n",
        "    # step 1: if it’s a list, pull out the first item\n",
        "    val = x[0] if isinstance(x, list) and x else x\n",
        "\n",
        "    # step 2: if it’s already an int, just return it\n",
        "    if isinstance(val, int):\n",
        "        return val\n",
        "\n",
        "    # step 3: if it’s a string, regex for leading digits\n",
        "    if isinstance(val, str):\n",
        "        # match “123.” or just “123”\n",
        "        m = re.match(r'^\\s*(\\d+)(?:\\.)?', val)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "\n",
        "    return None\n",
        "\n",
        "# extract into new columns\n",
        "result_df['gt_number']        = result_df['Ground_Truth'].apply(extract_leading_number)\n",
        "result_df['llm_gen_number']   = result_df['LLM_generated'].apply(extract_leading_number)\n"
      ],
      "metadata": {
        "id": "Qzxu5do09RCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth coverage\n",
        "\n",
        "# Initialize variables\n",
        "current_section = None\n",
        "section_has_yes = False\n",
        "match = 0\n",
        "\n",
        "# Iterate through the DataFrame\n",
        "for index, row in result_df.iterrows():\n",
        "    # Check if we are still in the same section\n",
        "    if row['gt_number'] == current_section:\n",
        "        # Check if there is a 'Yes' in 'is_match'\n",
        "        if row['is_match'] == 'Yes':\n",
        "            section_has_yes = True\n",
        "    else:\n",
        "        # We've reached a new section, check if the last section had a 'Yes'\n",
        "        if section_has_yes:\n",
        "            match += 1\n",
        "        # Reset for new section\n",
        "        current_section = row['gt_number']\n",
        "        section_has_yes = (row['is_match'] == 'Yes')\n",
        "\n",
        "# Check the last section after exiting the loop\n",
        "if section_has_yes:\n",
        "    match += 1\n",
        "print(match)\n",
        "\n",
        "\n",
        "# total number of unique ground truth\n",
        "\n",
        "# Calculate consecutive blocks where 'ground_truth' is the same\n",
        "unique_blocks = result_df['Ground_Truth'].ne(result_df['Ground_Truth'].shift()).cumsum()\n",
        "\n",
        "# Group by these blocks and count each group\n",
        "ck = result_df.groupby(unique_blocks)['gt_number'].agg(['count'])\n",
        "\n",
        "# Output the results\n",
        "print(\"Number of unique consecutive 'ground_truth' texts and their counts:\")\n",
        "print(ck)\n"
      ],
      "metadata": {
        "id": "sTscp5vO9Q7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Measuring Quality bewtween matched pairs (NLP based metrics)"
      ],
      "metadata": {
        "id": "jCYma4Kk-9fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ground Truth: ground_truth || LLM_Generated limitation: llm_generated"
      ],
      "metadata": {
        "id": "3oeLB_NN_lpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# say you want to rename 'oldA'→'newA' and 'oldB'→'newB'\n",
        "df.rename(columns={\n",
        "    'Ground_Truth': 'ground_truth',\n",
        "    'LLM_generated': 'llm_generated',\n",
        "    # 'Is_same': 'is_match',\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "iBF9CAs3Tn-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the column 'is_match' is 'no'\n",
        "df_filtered = df[df['is_match'].str.lower() != 'no']"
      ],
      "metadata": {
        "id": "3kCqlOU0Tn-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df_filtered.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "WQOUxnfuTn-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTScore (all)\n",
        "!pip3 -q install bert-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85d89e97-82cc-4988-8a2f-0aa4714969ab",
        "id": "oH3IU2CYTn-P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTScor for whole texts"
      ],
      "metadata": {
        "id": "hF9cjY0TTn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "# Initialize the BERT scorer\n",
        "scorer = BERTScorer(model_type='roberta-large', lang=\"en\")\n",
        "\n",
        "# Function to calculate BERTScore for each row using one loop\n",
        "def calculate_bertscore(row):\n",
        "    # Calculate BERT Scores directly for the ground_truth and llm_generated of the row\n",
        "    _, _, F1 = scorer.score([row['ground_truth']], [row['llm_generated']])\n",
        "    return F1.mean().item()  # Return the mean F1 score\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n",
        "df_filtered['bert_score'] = df_filtered.apply(calculate_bertscore, axis=1)\n"
      ],
      "metadata": {
        "id": "fWW1VgFcENca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of the 'bert_score' column in df_highest_score\n",
        "average_bert_score = df_filtered['bert_score'].mean()\n",
        "\n",
        "# Display the average\n",
        "average_bert_score\n"
      ],
      "metadata": {
        "id": "DS-k2i6hEPgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install rouge_score"
      ],
      "metadata": {
        "id": "emuvZfNmERVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to calculate similarity metrics for each row\n",
        "def calculate_metrics(row):\n",
        "    metrics = {}\n",
        "\n",
        "    # ROUGE scores\n",
        "    rouge_scores = rouge_scorer.score(row['ground_truth'], row['llm_generated'])\n",
        "    metrics['rouge1'] = rouge_scores['rouge1'].fmeasure\n",
        "    metrics['rouge2'] = rouge_scores['rouge2'].fmeasure\n",
        "    metrics['rougeL'] = rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "    # Cosine Similarity\n",
        "    vectorizer = CountVectorizer().fit_transform([row['ground_truth'], row['llm_generated']])\n",
        "    vectors = vectorizer.toarray()\n",
        "    metrics['cosine_similarity'] = cosine_similarity(vectors)[0, 1]\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set1 = set(row['ground_truth'].split())\n",
        "    set2 = set(row['llm_generated'].split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    metrics['jaccard_similarity'] = intersection / union if union > 0 else 0\n",
        "\n",
        "    # BLEU Score\n",
        "    metrics['bleu_score'] = sentence_bleu([row['ground_truth'].split()], row['llm_generated'].split())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Apply the function to each row in the DataFrame and store results in new columns\n",
        "metric_results = df_filtered.apply(calculate_metrics, axis=1)\n",
        "\n",
        "# Expand the dictionary into separate columns\n",
        "metric_results_df = pd.DataFrame(metric_results.tolist())\n",
        "df_filtered = pd.concat([df_filtered, metric_results_df], axis=1)\n"
      ],
      "metadata": {
        "id": "_y2fbOzaA1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average of each metric\n",
        "average_metrics = {\n",
        "    'Average ROUGE-1': df_filtered['rouge1'].mean(),\n",
        "    'Average ROUGE-2': df_filtered['rouge2'].mean(),\n",
        "    'Average ROUGE-L': df_filtered['rougeL'].mean(),\n",
        "    'Average Cosine Similarity': df_filtered['cosine_similarity'].mean(),\n",
        "    'Average Jaccard Similarity': df_filtered['jaccard_similarity'].mean(),\n",
        "    'Average BLEU Score': df_filtered['bleu_score'].mean()\n",
        "}\n",
        "\n",
        "# Print the average metrics\n",
        "average_metrics\n"
      ],
      "metadata": {
        "id": "x_JC28oCA3-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic similarity"
      ],
      "metadata": {
        "id": "qY1r5p35Tn-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install keybert\n",
        "\n",
        "from keybert import KeyBERT\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Ensure all entries are strings (even if NaN)\n",
        "df_filtered['ground_truth'] = df_filtered['ground_truth'].fillna(\"\").astype(str)\n",
        "df_filtered['llm_generated'] = df_filtered['llm_generated'].fillna(\"\").astype(str)\n",
        "\n",
        "# Function to extract keywords using KeyBERT\n",
        "def extract_keywords(text):\n",
        "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    return [kw[0] for kw in keywords]  # Extract just the keywords\n",
        "\n",
        "# Apply KeyBERT to 'ground_truth' and 'LLM_generated' columns\n",
        "df_filtered['ground_truth_words'] = df_filtered['ground_truth'].apply(extract_keywords)\n",
        "df_filtered['LLM_generated_words'] = df_filtered['llm_generated'].apply(extract_keywords)\n"
      ],
      "metadata": {
        "id": "D_un-kmDBCDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Jaccard Similarity\n",
        "def jaccard_similarity(row):\n",
        "    set1 = set(row['ground_truth_words'])\n",
        "    set2 = set(row['LLM_generated_words'])\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Apply Jaccard Similarity to each row\n",
        "df_filtered['jaccard_similarity_topic'] = df_filtered.apply(jaccard_similarity, axis=1)\n",
        "df_filtered['jaccard_similarity_topic'].mean()"
      ],
      "metadata": {
        "id": "FZ9I8AWzTn-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to compute Cosine Similarity with empty check\n",
        "def cosine_sim(row):\n",
        "    gt = ' '.join(row['ground_truth_words'])\n",
        "    llm = ' '.join(row['LLM_generated_words'])\n",
        "\n",
        "    # If either is empty, return 0 similarity\n",
        "    if not gt.strip() or not llm.strip():\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        vectorizer = CountVectorizer().fit_transform([gt, llm])\n",
        "        vectors = vectorizer.toarray()\n",
        "        return cosine_similarity(vectors)[0, 1]\n",
        "    except ValueError:\n",
        "        return 0.0  # fallback if vocabulary is still empty\n",
        "\n",
        "df_filtered['cosine_similarity_topic'] = df_filtered.apply(cosine_sim, axis=1)\n",
        "mean_sim = df_filtered['cosine_similarity_topic'].mean()\n",
        "print(mean_sim)\n"
      ],
      "metadata": {
        "id": "AC4rW-7VA-Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to extract text between double asterisks\n",
        "def extract_text_between_asterisks(text):\n",
        "    matches = re.findall(r'\\*\\*(.*?)\\*\\*', text)\n",
        "    return matches\n",
        "\n",
        "# Apply the function to both columns and store results in new columns\n",
        "df_filtered['ground_truth_extracted'] = df_filtered['ground_truth'].apply(extract_text_between_asterisks)\n",
        "df_filtered['llm_generated_extracted'] = df_filtered['llm_generated'].apply(extract_text_between_asterisks)\n"
      ],
      "metadata": {
        "id": "cq4Ekh7-Tn-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}